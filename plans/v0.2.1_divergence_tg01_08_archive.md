# GIMBAL v0.2.1: Test Groups 1-8 Archive

> **ðŸ—‚ï¸ Archive Notice:** This document contains Test Groups 1-8, completed under the pre-Issue framework. Superseded by `v0.2.1_divergence_plan_2.md`, which uses stable Issue IDs (I1, I2, I3) and Test Groups 9+ (TG09, TG10, TG11a, TG11b).

> **ðŸ“‹ Before starting:** Read [`IMPORTANT_FILE_NAMING_CONVENTIONS.md`](../IMPORTANT_FILE_NAMING_CONVENTIONS.md) in the repository root for file organization requirements.

## Purpose

This testing plan systematically identifies the causes of NUTS divergences in the v0.2.1 GIMBAL model by empirically isolating contributions from different model components.

**What we're testing:**
- Base camera + kinematic model (without HMM)
- Directional HMM contribution
- Number of HMM states (S=1, 2, 3)
- Likelihood component scale balance
- Hierarchical funnel geometries (root variance, bone-length variance)
- Runtime scaling of collapsed HMM implementation

**Goal:** Understand which components cause divergences before making structural changes to the model.

---

## Standard Test Configuration

All tests use identical synthetic data and sampling parameters unless otherwise specified:

| Parameter | Value | Notes |
|-----------|-------|-------|
| `T` | 100 | Number of time steps |
| `C` | 3 | Number of cameras |
| `S` | 3 | Number of HMM states (varies in state-count tests) |
| `kappa` | 10.0 | Directional concentration |
| `obs_noise_std` | 0.5 | Observation noise |
| `occlusion_rate` | 0.02 | Missing data rate |
| `random_seed` | 42 | Reproducibility |
| `draws` | 20 | Post-tuning samples (enough to detect 100% divergence) |
| `tune` | 100 | Tuning samples (reasonable adaptation without excess) |
| `chains` | 1 | Single chain for initial diagnostics |

---

## Test Groups

### Test Group 1: Baseline without HMM

**Configuration:** Full v0.2.1 model with `use_directional_hmm=False`

**Metrics to record:**
- Divergence count (out of total draws)
- ESS (Effective Sample Size) for: `x_all`, `U`, `obs_sigma`
- R-hat convergence diagnostic
- Reconstruction error: RMSE between posterior mean `x_all` and ground truth `x_true`
- Runtime (seconds)

**Purpose:** Determines whether divergences originate in the camera/kinematic model alone, independent of the HMM.

**Interpretation:**
- High divergences â†’ Base model has geometry issues
- Low divergences â†’ HMM is likely the problem

---

### Test Group 2: Baseline with HMM

**Configuration:** Same as Test Group 1 but with `use_directional_hmm=True` and `hmm_num_states=3`

**Metrics to record:**
- All metrics from Test Group 1
- Mean absolute value of `hmm_loglik`
- Mean absolute value of camera log-likelihood (`log_obs_t.sum()`)
- Likelihood ratio: `|hmm_loglik| / |camera_loglik|`

**Purpose:** Quantifies the added effect of the directional HMM on divergences.

**Interpretation:**
- Compare divergence rates to Test Group 1
- If divergences increase substantially â†’ HMM is problematic
- Check likelihood ratio for scale imbalance

---

### Test Group 3: Tuning and sampling parameters sensitivity

**Configuration:** Test whether divergences are due to insufficient tuning or sample count

**Variations to test:**
- Baseline: 100 tune, 20 draws (standard for all other tests)
- More tuning: 500 tune, 20 draws
- Much more tuning: 1000 tune, 20 draws
- More sampling: 100 tune, 100 draws

**Run on:** Test Group 1 configuration (baseline without HMM)

**Metrics to record:**
- Divergence count and rate for each configuration
- Whether divergences occur in tuning phase vs sampling phase
- ESS for key parameters
- Acceptance rate during tuning vs sampling

**Purpose:** Determines whether divergences are a fundamental model issue or can be resolved with more tuning.

**Interpretation:**
- If divergences drop substantially with more tuning â†’ Model just needs better adaptation
- If divergences persist regardless of tuning â†’ Fundamental posterior geometry issue
- If divergences only in tuning phase â†’ Can safely ignore (tuning samples are discarded)
- If divergences only in sampling phase â†’ Model is poorly adapted despite tuning

**Decision point:** If this test shows divergences are tuning-related, we can increase tuning steps for all subsequent tests and the problem may be resolved. If divergences persist with extensive tuning, we have a fundamental model geometry issue that requires reparameterization.

---

### Test Group 4: State count comparison

**Configuration:** Run models with `S = 1`, `S = 2`, and `S = 3` states

**Important:** For `S=1`, verify that the specialized single-state code path is triggered (avoids HMM forward algorithm).

**Metrics to record:**
- Divergence count for each state count
- ESS for key parameters
- R-hat
- Reconstruction error

**Purpose:** Determines whether multi-state HMM dynamics worsen posterior geometry.

**Interpretation:**
- If divergences decrease with fewer states â†’ Multi-state symmetry/label-switching is an issue
- If divergences similar across state counts â†’ State count is not the problem

---

### Test Group 5: Likelihood scale balance

**Configuration:** Analyze all models from Test Groups 1-2 and 4

**Metrics to compute:**
- Mean absolute value of `hmm_loglik`
- Mean absolute value of `log_obs_t.sum()` (camera likelihood)
- Ratio: `|hmm_loglik| / |camera_loglik|`

**Imbalance criteria:** Flag as "imbalanced" if ratio > 3 or ratio < 1/3

**Purpose:** Checks whether mis-scaled likelihood components dominate the posterior geometry, causing divergences.

**Interpretation:**
- Balanced ratio (0.3 to 3.0) â†’ Both components contribute appropriately
- Imbalanced ratio â†’ Dominant component may drive pathological geometry

---

### Test Group 6: Divergence localization

**Configuration:** For each test in Groups 1-2 and 4, generate diagnostic visualizations

**Diagnostics to produce:**
- Parallel coordinate plot (shows parameter values at divergent vs non-divergent samples)
- Pair plot with divergences highlighted (shows which parameter combinations are problematic)

**Output location:** `results/diagnostics/v0_2_1_divergence/plots/<testname>/`

**Purpose:** Identify which specific parameters participate in divergent transitions.

**Interpretation:**
- Clustered divergences in specific parameter regions â†’ Funnel geometry
- Divergences scattered across parameter space â†’ HMM exploration issues
- High correlation with specific parameters â†’ Targeted reparameterization needed

---

### Test Group 7: Root temporal variance sensitivity

**Configuration:** Repeat Test Groups 1 and 2 with different root variance prior scales

**Prior variations:**
- Default: `eta2_root_sigma = 0.1`
- Relaxed: `eta2_root_sigma = 5.0`
- Very relaxed: `eta2_root_sigma = 10.0`

**Metrics to record:**
- Change in divergence count relative to default
- ESS changes
- Reconstruction error changes

**Purpose:** Evaluates whether the root position random walk creates a hierarchical funnel geometry.

**Interpretation:**
- Divergences decrease with relaxed prior â†’ Funnel geometry confirmed
- Divergences unchanged â†’ Root variance is not the issue

---

### Test Group 8: Bone-length variance sensitivity

**Configuration:** Repeat Test Group 1 with different bone-length variance prior scales

**Prior variations:**
- Default: `sigma2_sigma = 0.1`
- Relaxed: `sigma2_sigma = 1.0`
- Very relaxed: `sigma2_sigma = 5.0`
- Fixed: Set `sigma2` to initialization value (removes variance entirely)

**Metrics to record:**
- Change in divergence count
- ESS for bone-length parameters
- Reconstruction quality

**Purpose:** Evaluates whether bone-length variance contributes to divergences through hierarchical funnel.

**Interpretation:**
- Divergences decrease when `sigma2` fixed â†’ Hierarchical structure is problematic
- Divergences unchanged â†’ Bone-length variance is not the issue

---

### Test Group 9: Runtime scaling verification

**Configuration:** Run HMM-on and HMM-off models with varying problem sizes

**Time step variations:** `T = 30`, `T = 60`, `T = 100`
**State count variations:** `S = 1`, `S = 2`, `S = 3`

**Metrics to record:**
- Runtime (seconds) for each configuration
- Divergence count
- Verify expected scaling behavior

**Purpose:** Verify that the collapsed HMM forward algorithm scales correctly and doesn't have gradient computation shortcuts that could cause divergences.

**Expected scaling:**
- Runtime should be roughly linear in `T`
- Runtime should scale with `S` (more states = more computation)

**Interpretation:**
- Unexpected scaling patterns â†’ Implementation issue in collapsed HMM
- Consistent scaling with correct gradients â†’ Implementation is sound

---

## File Organization and Naming Conventions

**Directory structure:** Each test plan gets its own diagnostics directory

```
tests/diagnostics/v0_2_1_divergence/
â”œâ”€â”€ test_group_1_baseline_no_hmm.py          # Test implementation
â”œâ”€â”€ test_group_2_baseline_with_hmm.py        # Test implementation
â”œâ”€â”€ test_group_3_state_count.py              # Test implementation
â”œâ”€â”€ ...
â”œâ”€â”€ results_group_1_baseline_no_hmm.json     # Raw output data
â”œâ”€â”€ results_group_2_baseline_with_hmm.json   # Raw output data
â”œâ”€â”€ report_group_1_baseline_no_hmm.md        # Analysis and interpretation
â”œâ”€â”€ report_group_2_baseline_with_hmm.md      # Analysis and interpretation
â””â”€â”€ report_summary.md                        # Overall findings across all tests
```

**Naming conventions:**

| File Type | Naming Pattern | Example | Purpose |
|-----------|---------------|---------|---------|
| Test scripts | `test_group_N_<description>.py` | `test_group_1_baseline_no_hmm.py` | Executable Python test |
| Result data | `results_group_N_<description>.json` | `results_group_1_baseline_no_hmm.json` | Raw numerical output |
| Individual reports | `report_group_N_<description>.md` | `report_group_1_baseline_no_hmm.md` | Analysis of one test |
| Summary report | `report_summary.md` | `report_summary.md` | Synthesizes all findings |
| Diagnostic plots | `plots/group_N_<description>/` | `plots/group_1_baseline_no_hmm/` | Visualizations |

**Why this structure:**
- `test_*` files are immediately identifiable as executable code
- `results_*` files contain raw data for reproducibility
- `report_*` files provide human-readable interpretation
- Group numbers link related files together
- Descriptive names make purpose clear at a glance

---

## Reporting Requirements

For each test group, create three files:

### 1. Test Script: `test_group_N_<description>.py`

**Must include:**
- Docstring explaining test purpose
- Configuration parameters clearly defined at top
- Functions that run sampling and collect metrics
- Saves output to corresponding `results_group_N_<description>.json`

**Template structure:**
```python
"""
Test Group N: <Description>

Purpose: <What this test reveals>
Configuration: <Key parameters that differ from standard>
"""

def test_group_N_<description>():
    # Configuration
    config = {
        'T': 100,
        'C': 3,
        # ... other parameters
    }
    
    # Run test
    trace, metrics = run_test(config)
    
    # Save results
    save_results(metrics, 'results_group_N_<description>.json')
    
    return metrics
```

### 2. Results File: `results_group_N_<description>.json`

**Must include:**
```json
{
  "test_group": 1,
  "description": "Baseline without HMM",
  "configuration": {
    "T": 100,
    "C": 3,
    "use_directional_hmm": false
  },
  "metrics": {
    "divergence_count": 45,
    "total_draws": 200,
    "divergence_rate": 0.225,
    "ess": {
      "x_all": 87.3,
      "U": 103.2,
      "obs_sigma": 156.4
    },
    "r_hat": {
      "x_all": 1.02,
      "U": 1.01
    },
    "runtime_seconds": 142.5,
    "reconstruction_rmse": 0.45
  },
  "timestamp": "2025-12-10T14:30:00",
  "environment": {
    "pymc_version": "5.x.x",
    "pytensor_version": "2.x.x"
  }
}
```

### 3. Report File: `report_group_N_<description>.md`

**Required sections:**

#### Test header
- Test group number and name
- Configuration parameters that differ from standard
- Date and runtime environment

#### Results summary
- Model configuration table
- Key metrics (divergence rate, ESS, R-hat, runtime, reconstruction error)
- Likelihood magnitudes and ratios (if applicable)

#### Interpretation
- What this test reveals about divergence sources
- Comparison to related tests (reference by group number)
- Statistical significance of findings

#### Diagnostic artifacts
- Links to plots: `plots/group_N_<description>/`
- Links to trace files or additional data
- Any warnings or error messages

#### Recommendations
- Next steps based on these findings
- Which tests should be run next
- Suggested model modifications (if any)

### 4. Summary Report: `report_summary.md`

**Synthesizes all test groups:**
- Table comparing divergence rates across all tests
- Key findings from each test group
- Overall interpretation: What component(s) cause divergences?
- Recommended actions based on complete evidence
- Priority order for model improvements

---

## Test Execution Workflow

1. **Run baseline tests first** (Groups 1-2) to establish the fundamental divergence pattern
2. **Analyze localization** (Group 5) to identify which parameters are involved
3. **Run targeted tests** (Groups 3, 6, 7) based on localization findings
4. **Verify implementation** (Groups 4, 8) if divergences are unexpectedly severe
5. **Document findings** and propose model improvements based on empirical results

**Note:** Some tests may be skipped if earlier results make them unnecessary. For example, if Test Group 1 shows that the base model has severe divergences independent of the HMM, then state-count tests (Group 3) become lower priority.
