# v0.2.1 Detailed Specification — Review Comments (Round 1)

**Reviewer:** GitHub Copilot (Claude Sonnet 4.5)  
**Date:** November 30, 2025  
**Spec Version:** Draft v0.2.1 detailed specification  
**Status:** Ready for implementation with clarifications

---

## Executive Summary

The v0.2.1 spec is **well-structured and ready for implementation** with minor clarifications needed. The three-module architecture (cleaning → statistics → priors) is sound, and the integration with existing v0.1 components is clear. Below are detailed comments organized by section.

**Overall Assessment:** ✅ **Approved with recommendations**

**Key Strengths:**
- Clear separation of concerns across three modules
- Proper preservation of statistical integrity (no interpolated data in stats)
- Well-defined data flow and interface contracts
- Explicit decision documentation (2.1-2.7)

**Key Recommendations:**
1. Add intermediate validation steps in pipeline
2. Clarify edge cases (all joints outliers, zero empirical kappa)
3. Specify behavior when `use_for_stats_mask` is all False
4. Define integration point in existing `add_directional_hmm_prior()`

---

## Section-by-Section Review

### Section 1: Goals & Scope

**Status:** ✅ Clear

**Comments:**
- Scope is appropriately limited for v0.2.1
- Out-of-scope items are correctly deferred
- "Minimal changes to synthetic demo" might be understated — the demo will need substantial updates to showcase the full pipeline

**Recommendations:**
1. Add explicit success criterion: "Priors from cleaned data should be more stable than priors from raw synthetic data"
2. Consider adding: "Cleaning should reject < 5% of synthetic data frames as invalid"

---

### Section 2: Design Decisions

**Status:** ✅ Excellent documentation

**Comments:**
- Decision 2.2 (no interpolated data for statistics) is **critical** and well-documented
- Decision 2.4 (no weakening of κ) needs justification: Why no `alpha` factor?
  - Current v0.1 uses `kappa_scale=5.0` in `add_directional_hmm_prior()`
  - Is this equivalent to an implicit weakening?
- Decision 2.6 (single-sequence) conflicts slightly with synthetic data generation which could support batches

**Recommendations:**

1. **Clarify relationship between v0.2.1 priors and v0.1 `kappa_scale`:**
   ```python
   # Current v0.1 behavior:
   kappa_prior = pm.HalfNormal("kappa", sigma=kappa_scale)  # Default: 5.0
   
   # v0.2.1 proposal:
   mu_prior_kappa = 5 * kappa_emp
   kappa_prior = Gamma(mode=kappa_emp, sd=kappa_emp)
   
   # Question: How do these interact?
   # Are we replacing kappa_scale with kappa_emp? Or combining?
   ```

2. **Specify integration strategy:**
   - Option A: `prior_config` replaces default priors entirely
   - Option B: `prior_config` modifies/scales default priors
   - **Recommendation:** Choose Option A for clarity

3. **Add decision 2.8:** "Priors from `prior_config` override default behavior (no mixing)"

---

### Section 3: Module Specifications

**Status:** ✅ Clear structure

**Comment:** The three-module separation is excellent. Each module has a single responsibility.

---

### Section 4: Module `data_cleaning.py`

**Status:** ✅ Mostly clear, needs edge case handling

#### 4.1 Purpose
**Comment:** ✅ Clear

#### 4.2 Input/Output
**Comments:**

1. **Input shape:** `keypoints_2d: (T, J, 2)`
   - **Issue:** Current codebase uses `(C, T, K, 2)` for multi-camera data
   - **Question:** Is cleaning per-camera or pooled?
   - **Recommendation:** Specify:
     ```python
     # Option A: Clean per-camera independently
     keypoints_2d: (C, T, K, 2)  # Input
     keypoints_clean: (C, T, K, 2)  # Output
     use_for_stats_mask: (C, T, K)  # Output
     
     # Option B: Clean after triangulation (3D)
     keypoints_2d: (T, K, 2)  # Single-camera or post-triangulation
     ```
   - **Suggested:** Option B is simpler for v0.2.1 (cleaning happens on triangulated 3D)

2. **Output `use_for_stats_mask`:**
   - Shape `(T, J)` is correct if cleaning 2D
   - Shape should be `(T, K)` to match joint convention (J vs K)
   - **Action:** Use consistent naming (`K` for joints throughout)

3. **Missing output:** Consider adding `outlier_summary` dict:
   ```python
   {
     "n_jump_outliers": int,
     "n_bone_outliers": int,
     "n_interpolated": int,
     "n_invalid_frames": int,
   }
   ```

#### 4.3 Cleaning Strategy

**Comments:**

1. **Step 1 - Jump outliers:**
   - ✅ Median + MAD is robust
   - **Question:** What if a joint has constant position (MAD=0)?
   - **Recommendation:** Add: "If MAD < 1e-6, skip outlier detection for that joint"

2. **Step 2 - Bone length:**
   - ✅ Good sanity check
   - **Question:** "Mark both joints" — what if one joint is already marked? Avoid double-counting?
   - **Recommendation:** Use set union for outlier tracking

3. **Step 3 - Convert to NaN:**
   - ✅ Standard approach

4. **Step 4 - Interpolation:**
   - ✅ Linear interpolation is appropriate
   - **Missing:** What happens at boundaries? (t=0 or t=T-1)
   - **Recommendation:** Specify: "Gaps at boundaries remain NaN (no extrapolation)"

5. **Step 5 - Valid frame mask:**
   - ✅ `max_bad_joint_fraction` threshold is sensible
   - **Recommendation:** Add default: "Default `max_bad_joint_fraction=0.3`"

6. **Step 6 - Statistics mask:**
   - ✅ **Critical step**, well-defined
   - **Missing:** Confirm this is a **3-way flag**: original-and-good, outlier, or interpolated
   - **Recommendation:** Document: "`use_for_stats_mask[t,k]` is True only if original value was observed and not flagged as outlier"

#### 4.4 CleaningConfig

**Missing from spec:** Need to define `CleaningConfig` dataclass:

```python
@dataclass
class CleaningConfig:
    """Configuration for 2D keypoint cleaning."""
    jump_z_thresh: float = 3.0  # Z-score threshold for jump detection
    bone_z_thresh: float = 3.0  # Z-score threshold for bone-length
    max_gap: int = 5            # Maximum gap to interpolate
    max_bad_joint_fraction: float = 0.3  # Fraction of bad joints to mark frame invalid
```

**Action Required:** Add this to spec section 4.

---

### Section 5: Module `direction_statistics.py`

**Status:** ✅ Clear, minor edge cases

#### 5.1-5.2 Purpose & Inputs
**Comment:** ✅ Clear

#### 5.3 Outputs

**Comments:**

1. **Dictionary structure:** ✅ Good
2. **Missing fields:** Consider adding:
   ```python
   {
     "mu_emp": np.ndarray,
     "kappa_emp": float,
     "n_samples": int,
     "R_bar": float,        # NEW: Resultant length (for debugging)
     "is_valid": bool,       # NEW: False if n_samples < min_threshold
   }
   ```

3. **Edge case:** What if `n_samples == 0` for a joint?
   - **Recommendation:** Set `is_valid=False`, use fallback priors in `prior_building.py`

#### 5.4 Procedure

**5.4.1 Joint directions:**
- ✅ Standard formula
- **Question:** What if `x_t,j == x_t,parent(j)` (zero-length bone)?
- **Recommendation:** Skip that frame (don't add to stats)

**5.4.2 Empirical mean direction:**
- ✅ Correct formula
- **Edge case:** What if all `u_t,j` cancel out (circular distribution)?
  - This gives `|bar_u_j| ≈ 0`
  - κ approximation will give `kappa_emp ≈ 0`
  - **Recommendation:** Document: "If `|bar_u_j| < 0.01`, set `is_valid=False` for that joint"

**5.4.3 Empirical concentration κ:**
- ✅ Formula is correct 3D approximation
- **Source:** This appears to be from Sra (2011) or similar vMF moment matching
- **Recommendation:** Add citation in docstring:
  ```python
  """
  Compute empirical concentration using 3D vMF moment matching.
  
  Uses piecewise approximation from:
  Sra, S. (2012). A short note on parameter approximation for von Mises-Fisher distributions.
  Computational Statistics, 27(1), 177-190.
  """
  ```

**Edge case handling:**
- ✅ "For very small R̄, set kappa_emp = 0" is good
- **Recommendation:** Specify threshold: "If R̄ < 0.01, set kappa_emp = 0"

---

### Section 6: Module `prior_building.py`

**Status:** ⚠️ Needs integration clarification

#### 6.1-6.2 Purpose & Inputs
**Comment:** ✅ Clear

#### 6.3 Outputs

**Comments:**

1. **Structure is good** but spec shows:
   ```python
   priors = {
     "joint_directions": {
       joint_name: {
         "mu": {...},
         "kappa": {...}
       }
     }
   }
   ```

2. **Missing:** How does this integrate with current `add_directional_hmm_prior()`?
   
   **Current signature:**
   ```python
   def add_directional_hmm_prior(
       U, log_obs_t, S,
       *,
       prior_config: dict | None = None,  # <-- Reserved for v0.2.1
       ...
   ):
   ```

3. **Integration options:**

   **Option A: prior_config contains priors dict directly**
   ```python
   priors = build_priors_from_statistics(emp_stats, joint_names)
   
   add_directional_hmm_prior(
       U, log_obs_t, S=3,
       prior_config=priors,  # <-- Pass entire priors dict
   )
   ```

   **Option B: prior_config is a flag + source**
   ```python
   add_directional_hmm_prior(
       U, log_obs_t, S=3,
       prior_config={
           "use_data_driven_priors": True,
           "empirical_stats": emp_stats,
       }
   )
   # Function internally calls build_priors_from_statistics()
   ```

   **Recommendation:** Option A is cleaner (separation of concerns)

#### 6.4 Prior Definitions

**6.4.1 Direction prior (μ):**

**Current spec:**
```python
mu_prior = vMF(mu=mu_j_emp, kappa=5*kappa_j_emp)
```

**Issue:** This is a **prior on the latent direction `mu_j`**, not the observed `U_t,j`

**Current v0.1 code:**
```python
# In add_directional_hmm_prior():
mu_raw = pm.Normal("mu_raw", mu=0.0, sigma=1.0, shape=(S, K, 3))
norm_mu = pt.sqrt((mu_raw**2).sum(axis=-1, keepdims=True) + 1e-8)
mu = pm.Deterministic("mu", mu_raw / norm_mu)
```

This samples `mu` **uniformly on the sphere** (Normal(0,1) → normalize).

**v0.2.1 needs to change this to:**
```python
# With data-driven priors:
if prior_config is not None:
    # For each state s and joint k:
    # Sample mu[s,k,:] from vMF(mu_emp[k], kappa_emp[k])
    # This concentrates mu around empirical direction
```

**Critical question:** How to implement vMF sampling in PyMC?

**Option A: Use `pymc_distributions.VonMisesFisher`**
- Pro: Already exists in codebase
- Con: Spec says "experimental/legacy" (see `gimbal/pymc_distributions.py`)
- Con: May have numerical issues (why v0.1 avoided it)

**Option B: Rejection sampling from Normal**
```python
# Rejection sampling on sphere near mu_emp
mu_samples = []
for s in range(S):
    for k in range(K):
        # Sample from Normal centered at mu_emp
        mu_prop = pm.Normal(..., mu=mu_emp[k], sigma=1/sqrt(kappa_emp[k]))
        mu_norm = mu_prop / |mu_prop|
        # This approximates vMF for large kappa
```

**Option C: Projected Normal (simpler, approximate)**
```python
# Sample in tangent space to sphere at mu_emp
# Equivalently: Normal(mu_emp, sigma=1/sqrt(kappa)) then normalize
mu_raw = pm.Normal("mu_raw", mu=mu_emp[None, :, :], 
                   sigma=1.0/pt.sqrt(kappa_emp[None, :, None]))
mu = mu_raw / pt.sqrt((mu_raw**2).sum(axis=-1, keepdims=True) + 1e-8)
```

**Recommendation:**
- **For v0.2.1:** Use Option C (projected Normal) — simplest, avoids vMF issues
- Document that this is an approximation
- Add to spec: "Direction prior implemented as projected Normal (approximate vMF)"

**Action Required:** Clarify in spec how vMF prior is actually implemented in PyMC

---

**6.4.2 Concentration prior (κ):**

**Current spec:**
```python
kappa_prior = Gamma(mode=kappa_emp, sd=kappa_emp)
```

**Implementation check:**
```python
# Spec says model does this conversion:
ra = (mode + sqrt(mode**2 + 4*sd**2)) / (2 * sd**2)
sh = 1 + mode * ra
```

**Verification:**
```python
# Gamma(shape=sh, rate=ra) has:
# mode = (sh-1)/ra  [for sh > 1]
# variance = sh/ra^2
# sd = sqrt(sh)/ra

# If mode=10, sd=10:
ra = (10 + sqrt(100 + 400)) / 200 = (10 + 22.36) / 200 = 0.1618
sh = 1 + 10*0.1618 = 2.618

# Check:
mode_check = (2.618 - 1) / 0.1618 = 10.0 ✓
var_check = 2.618 / 0.1618^2 = 100.0 ✓
sd_check = sqrt(100) = 10.0 ✓
```

✅ **Formula is correct**

**Recommendation:** Add this verification to a test

---

**Clamping:**
```python
mode = max(kappa_emp, kappa_min)
```

**Questions:**
1. What is `kappa_min`? Suggested default: `0.1`
2. What if `kappa_emp == 0` (uniform distribution)?
   - With `kappa_min=0.1`: Prior becomes Gamma(mode=0.1, sd=0.1)
   - This is very diffuse (good fallback)

**Recommendation:** 
- Add to spec: "Default `kappa_min = 0.1`"
- Document in code: "Clamping prevents degenerate Gamma when data is uniformly distributed"

---

### Section 7: Priors Dictionary (Final Form)

**Status:** ✅ Clear structure

**Comments:**

1. **Nested dict structure** is sensible
2. **Missing:** How does `add_directional_hmm_prior()` parse this?

**Integration pseudocode needed:**
```python
def add_directional_hmm_prior(U, log_obs_t, S, *, prior_config=None, ...):
    K = U.shape[1]
    
    if prior_config is None:
        # v0.1 default behavior: uniform on sphere
        mu_raw = pm.Normal("mu_raw", 0, 1, shape=(S, K, 3))
        mu = mu_raw / |mu_raw|
        kappa = pm.HalfNormal("kappa", sigma=kappa_scale)
        
    else:
        # v0.2.1: Use data-driven priors
        joint_dir_priors = prior_config["joint_directions"]
        
        # For each joint k, get empirical stats
        mu_emp_all = []  # (K, 3)
        kappa_emp_all = []  # (K,)
        for k, joint_name in enumerate(joint_names):
            priors_k = joint_dir_priors[joint_name]
            mu_emp_all.append(priors_k["mu"]["mu"])
            kappa_emp_all.append(priors_k["mu"]["kappa"] / 5.0)  # Undo scaling
        
        # Sample mu[s,k,:] centered at mu_emp[k]
        # ... (implement projected Normal as discussed above)
        
        # Sample kappa[s,k] from Gamma
        mode = priors_k["kappa"]["mode"]
        sd = priors_k["kappa"]["sd"]
        ra = (mode + sqrt(mode**2 + 4*sd**2)) / (2*sd**2)
        sh = 1 + mode*ra
        kappa = pm.Gamma("kappa", alpha=sh, beta=ra, shape=(S, K))
```

**Action Required:** Add integration pseudocode to spec Section 8

---

### Section 8: Integration Pipeline

**Status:** ⚠️ Needs detail

**Comments:**

1. **Step 2 says:** "clean 2D keypoints"
   - But Section 5 needs **3D positions** for direction statistics
   - **Question:** Where does triangulation happen?

2. **Suggested pipeline refinement:**
   ```python
   # 1. Generate synthetic 3D + 2D
   data = generate_demo_sequence(...)
   
   # 2. Triangulate 2D → 3D (using DLT or similar)
   x_3d_init = triangulate_multi_view(data.y_observed, data.camera_proj)
   
   # 3. Clean 3D positions (jump + bone-length outliers)
   x_3d_clean, valid_mask, use_for_stats = clean_keypoints_3d(
       x_3d_init, skeleton.parents, config
   )
   
   # 4. Compute directions from cleaned 3D
   emp_stats = compute_direction_statistics(
       x_3d_clean, skeleton.parents, use_for_stats
   )
   
   # 5. Build priors dict
   priors = build_priors_from_statistics(emp_stats, skeleton.joint_names)
   
   # 6. Build PyMC model with priors
   with pm.Model() as model:
       model_result = build_camera_observation_model(
           y_observed=data.y_observed,
           camera_proj=data.camera_proj,
           parents=skeleton.parents,
           init_result=init_result,
       )
       
       add_directional_hmm_prior(
           U=model_result["U"],
           log_obs_t=model_result["log_obs_t"],
           S=3,
           prior_config=priors,  # <-- v0.2.1 data-driven priors
       )
   
   # 7. Sample
   idata = pm.sample(...)
   ```

**Action Required:** Update Section 8 with detailed pipeline including triangulation step

---

### Section 9: Tests

**Status:** ✅ Good coverage

**Additional test recommendations:**

1. **Edge case tests:**
   ```python
   def test_all_joints_outliers():
       """What if entire frame is outliers?"""
       # Should mark frame as invalid
   
   def test_zero_bone_length():
       """What if bone length is zero (collapsed joint)?"""
       # Should skip that frame for statistics
   
   def test_uniform_directions():
       """What if empirical directions are uniform (kappa_emp ≈ 0)?"""
       # Should use fallback prior (kappa_min)
   
   def test_insufficient_data():
       """What if only 2 valid samples for a joint?"""
       # Should mark joint as invalid, use fallback
   ```

2. **Integration test enhancement:**
   ```python
   def test_priors_reduce_posterior_variance():
       """Verify priors actually help inference."""
       # Sample with v0.1 default priors
       # Sample with v0.2.1 data-driven priors
       # Assert: ESS higher with data-driven priors
   ```

---

### Section 10: Acceptance Criteria

**Status:** ✅ Good

**Additions:**

1. ✅ "Priors from v0.2.1 produce higher ESS than v0.1 defaults on synthetic data"
2. ✅ "Cleaned data removes < 10% of frames on clean synthetic input"
3. ✅ "Empirical kappa values are within 20% of true kappa for synthetic data"
4. ✅ "Prior predictive samples from data-driven priors are centered near empirical means"

---

### Section 11: Review Checklist

**Status:** ✅ Helpful

**Additions to checklist:**

- [ ] Triangulation step documented in pipeline
- [ ] vMF approximation method specified (projected Normal recommended)
- [ ] `CleaningConfig` dataclass defined
- [ ] Edge cases handled (zero samples, uniform distribution, etc.)
- [ ] Integration with `add_directional_hmm_prior()` specified
- [ ] Gamma(mode, sd) → (shape, rate) conversion tested
- [ ] Minimum sample size threshold defined (suggest: 10 frames)
- [ ] Joint names passed consistently to `prior_building.py`

---

## Missing Specifications

### 1. Function Signatures

**Need to add to spec:**

```python
# gimbal/data_cleaning.py

@dataclass
class CleaningConfig:
    jump_z_thresh: float = 3.0
    bone_z_thresh: float = 3.0
    max_gap: int = 5
    max_bad_joint_fraction: float = 0.3

def clean_keypoints_3d(
    positions_3d: np.ndarray,  # (T, K, 3)
    parents: np.ndarray,       # (K,)
    config: CleaningConfig,
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Clean 3D keypoint trajectories.
    
    Returns
    -------
    positions_clean : (T, K, 3)
        Cleaned positions with interpolation
    valid_frame_mask : (T,)
        Boolean mask for valid frames
    use_for_stats_mask : (T, K)
        Boolean mask for statistics (only original non-outlier data)
    """
    ...
```

```python
# gimbal/direction_statistics.py

def compute_direction_statistics(
    positions_3d: np.ndarray,     # (T, K, 3)
    parents: np.ndarray,          # (K,)
    use_for_stats_mask: np.ndarray,  # (T, K)
    min_samples: int = 10,
) -> dict[str, dict]:
    """
    Compute empirical direction statistics per joint.
    
    Returns
    -------
    stats : dict
        {
            joint_idx: {
                "mu_emp": np.ndarray,  # (3,)
                "kappa_emp": float,
                "n_samples": int,
                "R_bar": float,
                "is_valid": bool,
            }
        }
    """
    ...
```

```python
# gimbal/prior_building.py

def build_priors_from_statistics(
    emp_stats: dict,
    joint_names: list[str],
    kappa_min: float = 0.1,
    kappa_scale: float = 5.0,
) -> dict:
    """
    Build priors dict from empirical statistics.
    
    Returns
    -------
    priors : dict
        {
            "joint_directions": {
                joint_name: {
                    "mu": {"mu": np.ndarray, "kappa": float},
                    "kappa": {"mode": float, "sd": float},
                },
                ...
            }
        }
    """
    ...
```

### 2. Triangulation Function

**Missing from spec but needed:**

```python
# gimbal/fit_params.py (or new module)

def triangulate_multi_view(
    keypoints_2d: np.ndarray,  # (C, T, K, 2)
    camera_proj: np.ndarray,   # (C, 3, 4)
    min_cameras: int = 2,
) -> np.ndarray:
    """
    Triangulate 3D positions from multi-view 2D observations.
    
    Uses DLT (Direct Linear Transform) for each joint at each timestep.
    NaN in 2D observations are excluded from triangulation.
    
    Returns
    -------
    positions_3d : (T, K, 3)
        Triangulated 3D positions
    """
    ...
```

**Action Required:** Add triangulation spec to Section 8 or create new Section 4.5

---

## Implementation Order Recommendation

Based on dependencies:

1. **Phase 1: Data cleaning** (standalone)
   - Implement `CleaningConfig`
   - Implement `clean_keypoints_3d()`
   - Write tests for cleaning

2. **Phase 2: Direction statistics** (depends on cleaning)
   - Implement `compute_direction_statistics()`
   - Write tests for statistics computation
   - Test with synthetic data

3. **Phase 3: Prior building** (depends on statistics)
   - Implement `build_priors_from_statistics()`
   - Test Gamma(mode, sd) conversion
   - Test prior dict structure

4. **Phase 4: Model integration** (depends on all above)
   - Modify `add_directional_hmm_prior()` to accept `prior_config`
   - Implement projected Normal for direction priors
   - Test full pipeline

5. **Phase 5: Demo update** (depends on integration)
   - Update synthetic demo notebook
   - Add visualization of priors vs data
   - Document pipeline

---

## Critical Questions for Author

Please clarify the following before implementation:

### Q1: 2D vs 3D Cleaning
**Current spec ambiguity:** Section 4 talks about 2D cleaning, Section 5 needs 3D positions.

**Question:** Should cleaning operate on:
- (A) 2D keypoints before triangulation, or
- (B) 3D positions after triangulation?

**Recommendation:** Option B (clean 3D) is simpler and more principled.

### Q2: vMF Implementation
**Question:** How should vMF prior for `mu` be implemented in PyMC?

**Options:**
- (A) Use existing `VonMisesFisher` from `pymc_distributions.py`
- (B) Approximate with projected Normal
- (C) Rejection sampling

**Recommendation:** Option B (projected Normal) for v0.2.1 simplicity.

### Q3: Prior Override vs Modification
**Question:** When `prior_config` is provided, should it:
- (A) Completely replace default priors, or
- (B) Modify/scale default priors?

**Recommendation:** Option A (complete replacement) for clarity.

### Q4: Joint Name Mapping
**Question:** The prior dict uses `joint_names` as keys, but model uses indices.

**Need to clarify:** How are joint names mapped to indices in `add_directional_hmm_prior()`?

**Suggestion:** Add `joint_names` parameter to `add_directional_hmm_prior()`:
```python
def add_directional_hmm_prior(
    U, log_obs_t, S,
    *,
    joint_names: list[str] | None = None,  # NEW
    prior_config: dict | None = None,
    ...
):
```

### Q5: Minimum Sample Size
**Question:** What is minimum `n_samples` to compute valid statistics?

**Recommendation:** Add to spec:
- Minimum 10 samples per joint
- If fewer, mark `is_valid=False` and use fallback uniform prior

---

## Documentation Improvements Needed

1. **Add figure:** Pipeline flowchart showing data flow through 3 modules
2. **Add table:** Comparison of v0.1 vs v0.2.1 prior sources
3. **Add example:** Show concrete prior dict values for synthetic data
4. **Add docstring standards:** Specify numpy docstring format for all functions

---

## Test Coverage Recommendations

### Unit Tests (per module)

**`data_cleaning.py`:**
- ✅ Jump outlier detection
- ✅ Bone length outliers
- ✅ Interpolation (short gaps)
- ✅ Long gaps remain NaN
- ✅ Valid frame mask
- ✅ Statistics mask correctness
- ➕ **ADD:** All joints outliers
- ➕ **ADD:** Zero-length bones
- ➕ **ADD:** Boundary interpolation

**`direction_statistics.py`:**
- ✅ Perfect alignment → high kappa
- ✅ Uniform distribution → low kappa
- ✅ Mask exclusion
- ➕ **ADD:** Zero samples for joint
- ➕ **ADD:** Circular distribution (cancel out)
- ➕ **ADD:** Very few samples (< 10)

**`prior_building.py`:**
- ✅ Gamma conversion formula
- ✅ Kappa clamping
- ➕ **ADD:** Invalid joint handling
- ➕ **ADD:** Prior dict structure validation

### Integration Tests

1. ✅ Full pipeline on synthetic data
2. ✅ Model builds without errors
3. ✅ Prior predictive sampling
4. ➕ **ADD:** ESS improvement vs v0.1
5. ➕ **ADD:** Posterior recovery of known parameters

---

## Summary and Recommendations

### Ready for Implementation: ✅

The specification is **fundamentally sound** and ready for implementation after addressing the clarifications above.

### Must Address Before Coding:

1. **Clarify 2D vs 3D cleaning** (Q1 above) → Recommend 3D
2. **Specify vMF implementation** (Q2 above) → Recommend projected Normal
3. **Define prior override behavior** (Q3 above) → Recommend complete replacement
4. **Add joint name mapping** (Q4 above) → Add parameter to function
5. **Add triangulation function spec** → New section or extend Section 8

### Nice to Have:

1. Function signature specifications (Section "Missing Specifications" above)
2. Edge case handling documentation
3. Pipeline flowchart
4. Example prior dict values

### Estimated Implementation Time:

- **Phase 1 (Cleaning):** 2-3 days
- **Phase 2 (Statistics):** 2-3 days
- **Phase 3 (Prior building):** 1-2 days
- **Phase 4 (Integration):** 2-3 days
- **Phase 5 (Demo):** 1-2 days
- **Testing & Documentation:** 2-3 days

**Total:** ~10-15 days for full v0.2.1 implementation

---

## Final Verdict

**Status:** ✅ **APPROVED FOR IMPLEMENTATION** with clarifications

**Next Steps:**
1. Author addresses critical questions (Q1-Q5)
2. Author adds missing function signatures
3. Author specifies triangulation step
4. Implementation proceeds in recommended order

**Confidence Level:** High — the architecture is solid, only details need clarification.

---

**End of Review**
