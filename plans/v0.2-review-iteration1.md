# v0.2 Overview Review — Iteration 1

**Date:** November 29, 2024  
**Reviewer:** AI Assistant  
**Document Reviewed:** `v0.2-overview.md` + `v0.2-planning-chatgpt.md`  
**Context:** Post-v0.1 completion (tagged), pre-Phase 0.2.1 implementation  
**Status:** Recommendations for refinement before detailed spec generation

---

## Executive Summary

The v0.2 roadmap is **well-structured, pragmatic, and appropriately scoped**. The 8-phase progression from simple priors (0.2.1-0.2.2) → sampler validation (0.2.3) → diagnostics (0.2.4) → real data (0.2.5-0.2.6) → sophisticated priors (0.2.7-0.2.8) follows sound engineering principles and builds on the solid v0.1 foundation.

The ChatGPT planning conversation reveals important additional context:
- Strong emphasis on **downsampled HMM** (6 Hz → 60 Hz) as a primary algorithmic improvement
- Clear preference for **PCA-based dimensionality reduction** as "low-hanging fruit"
- Focus on **modular API design** for reusability
- Explicit prioritization framework (must-have / nice-to-have / maybe-someday)

**Recommended Action:** Address priority concerns (see §7) before generating Phase 0.2.1 detailed specification.

---

## 1. Phase-by-Phase Assessment with ChatGPT Context

### Phase 0.2.1 — Coarse Anatomical Priors & Basic Cleaning

**Status:** ✅ Good starting point, aligns well with planning conversation

**Strengths:**
- Correctly positioned before sampler testing
- Pragmatic "coarse" constraint philosophy matches ChatGPT discussion emphasis on avoiding over-engineering

**Clarifications Needed:**

1. **Relationship to `v0.2-biomechanical-priors-notes.md`**:
   - That document discusses temporal smoothness and parent-relative canonical directions
   - ChatGPT conversation suggests treating temporal dynamics separately from static anatomical constraints
   - **Recommendation**: Phase 0.2.1 should focus on **static anatomical constraints only**:
     - "Legs point generally downward" (gravity-aligned)
     - "Arms within anterior hemisphere"
     - "Torso/head near vertical"
   - Defer temporal smoothness to Phase 0.2.4 or later (after diagnostics exist)

2. **Sanity Filter Specification**:
   - "Impossible distances, gross outliers" needs concrete definition
   - Candidates mentioned in ChatGPT conversation:
     - Inter-joint distances exceeding physical limits
     - Bone length violations (e.g., >3σ from mean)
     - Field-of-view violations (keypoints behind camera)
   - **Recommendation**: Document 3-5 specific filters with thresholds in detailed spec

3. **Integration Mechanism**:
   - ChatGPT conversation emphasizes modular API design
   - How do priors integrate into Stage 3?
   - **Recommendation**: Extend `add_directional_hmm_prior()` signature:
     ```python
     def add_directional_hmm_prior(
         U, log_obs_t, S,
         mu_prior_mean=None,      # NEW: (K,3) or (S,K,3) anatomical priors
         mu_prior_kappa=None,     # NEW: concentration for priors
         # ... existing parameters
     ):
     ```
   - Default `None` preserves v0.1 backward compatibility

**From ChatGPT Conversation — Additional Context:**
- Ranked as "Middle tier: moderate effort, solid payoff" (item F: "Simple anatomical / gravity-aligned priors on mu")
- Seen as **stabilization** rather than performance improvement
- Should use **weak priors** to avoid over-constraining

---

### Phase 0.2.2 — k-Means / Clustering Empirical-Bayes Priors

**Status:** ✅ Solid approach, implementation details needed

**Strengths:**
- Empirical Bayes is appropriate for data-driven priors
- ChatGPT conversation explicitly endorses this: ranked as "Top tier: very low effort, high payoff" (item D: "Empirical-Bayes / k-means initialization")

**Implementation Details Needed:**

1. **Feature Space for Clustering**:
   - ChatGPT conversation distinguishes:
     - **Option A**: Cluster on `U[t,k,:]` (directional data) → flattened (T, 3K)
     - **Option B**: Cluster on PCA-reduced pose coordinates (anticipating Phase 0.2.7)
   - **Recommendation**: Start with **Option A** (directions) for Phase 0.2.2
     - Simpler, directly matches v0.1.3 directional model
     - Reserve Option B for Phase 0.2.7 integration

2. **Transition Estimation Challenge**:
   - Frame-level clustering yields noisy state assignments: `[0,0,1,0,1,1,1,2,...]`
   - ChatGPT conversation suggests smoothing strategies:
     - **Option A**: HMM smoothing (Viterbi on raw cluster assignments)
     - **Option B**: Mode filter (5-frame window majority vote)
     - **Option C**: Minimum dwell time constraint (≥N frames per state)
   - **Recommendation**: Use **Viterbi smoothing** as primary method
     - Run k-means → get soft assignments
     - Run simple HMM with uniform priors to get smoothed sequence
     - Estimate transitions from smoothed sequence

3. **Concentration (κ) Estimation**:
   - "Rough κ scales" needs concrete algorithm
   - **Recommendation**: Per-cluster per-joint concentration:
     ```python
     # For each cluster s, joint k:
     R̄ = ||Σᵢ uᵢ|| / n  # Mean resultant length
     κ_approx = (3R̄ - R̄³) / (1 - R̄²)  # 3D vMF approximation
     ```

4. **Number of Clusters**:
   - ChatGPT conversation discusses state-number selection strategies
   - For Phase 0.2.2 (prior generation), need pragmatic choice
   - **Recommendation**: 
     - Grid search S ∈ [2,3,4,5] with silhouette score
     - Use best S for prior generation
     - Phase 0.2.8 will do formal model comparison

**From ChatGPT Conversation — Key Insight:**
- This is seen as **initialization/warm-start** strategy, not just prior generation
- Could "dramatically shorten NUTS warmup and reduce label-switching weirdness"
- Should produce reusable priors (saved to `.npz` file)

---

### Phase 0.2.3 — Sampler Decision Spike

**Status:** ✅ Good timing and scope

**Strengths:**
- Positioned after priors stabilize model (correct dependency)
- Focused comparison (not exhaustive survey)

**Refinements from ChatGPT Context:**

1. **Candidate Sampler**:
   - Overview says "JAX HMC" (vague)
   - ChatGPT conversation suggests **Blackjax** specifically:
     - JAX-based (GPU-ready)
     - Actively maintained
     - Has NUTS, HMC, and other variants
     - More directly comparable to nutpie
   - **Recommendation**: Compare nutpie vs Blackjax

2. **Evaluation Metrics Expansion**:
   - Current: R-hat, ESS, runtime per effective sample, robustness
   - ChatGPT conversation adds:
     - **Memory footprint** (important for scaling to T=500+)
     - **ESS per second** (more interpretable than raw ESS)
     - **Divergence rate** (percentage, not just count)
     - **Gradient evaluation count** per sample
   - **Recommendation**: Include all of above in comparison table

3. **Test Complexity Specification**:
   - "Moderate increases in S or T" is vague
   - **Recommendation**: Test matrix from ChatGPT conversation:
     - Baseline: S=3, T=50 (current synthetic demo)
     - Scale S: S=5, T=50
     - Scale T: S=3, T=200
     - Scale both: S=5, T=100
   - Document which scenarios cause convergence issues

4. **Decision Criteria**:
   - **Recommendation**: Define explicit rules:
     - **Keep nutpie if**: ESS/sec within 2× of alternative AND converges on all tests
     - **Switch if**: Alternative >3× faster OR nutpie fails convergence on scaled problems

**Note from ChatGPT:** Ranked as "Middle tier" — useful but not critical path for v0.2 success

---

### Phase 0.2.4 — Minimal Synthetic Diagnostics

**Status:** ⚠️ Essential but underspecified for HMMs

**Strengths:**
- Positioned perfectly (after sampler choice, before real data)
- ChatGPT conversation emphasizes **two-tier diagnostic system**:
  - Human-facing (plots, tables, "traffic light" summary)
  - Machine-facing (scalar metrics for AI-driven optimization)

**Critical Additions:**

1. **Label Switching Detection & Handling**:
   - HMMs suffer from label switching across chains
   - ChatGPT conversation explicitly discusses this
   - Current diagnostic list doesn't address it
   - **Recommendation**: Add label switching tools:
     - **Detection**: Cross-chain state alignment metric
     - **Correction**: Hungarian algorithm for relabeling (already mentioned in v0.1.3 completion report)
     - **Automation**: `relabel_chains()` function for posterior processing

2. **Numeric Diagnostics — Extended Suite**:
   - Current: R-hat, ESS, divergences
   - **Add from ChatGPT conversation**:
     - ESS per second (for sampler comparison)
     - Bulk ESS vs tail ESS (PyMC/ArviZ distinction)
     - MCSE (Monte Carlo Standard Error) for key parameters
     - Label switching score (new for HMMs)
     - Average R-hat over parameter subset (machine-facing metric)

3. **Visual Diagnostics — Extended Suite**:
   - Current: state occupancy, transition heatmap, canonical directions
   - **Add from ChatGPT conversation**:
     - Trace plots for key parameters (mu[s,k,:], kappa[s,k], transition probabilities)
     - Prior vs posterior overlays (verify learning with weak priors)
     - State sequence alignment plot across chains (visual label switching check)
     - Autocorrelation plots (verify mixing)

4. **Two-Tier API Design**:
   ```python
   # Human-facing (from notebook):
   from gimbal.diagnostics import summarize_fit
   summary = summarize_fit(model, idata, data_context)
   # Produces plots + "traffic light" summary
   
   # Machine-facing (for optimization):
   metrics = extract_optimization_metrics(idata)
   # Returns: {"avg_rhat": 1.003, "median_ess_per_sec": 42.1, ...}
   ```

5. **Packaging Recommendation**:
   - Create `gimbal/diagnostics.py` module with:
     - `run_hmm_diagnostics(idata, model_config)` → `DiagnosticReport`
     - `plot_hmm_diagnostics(report, save_path=None)` → figure
     - `relabel_chains(idata, method="hungarian")` → relabeled `InferenceData`

---

### Phase 0.2.5 — First Public Dataset Loader + Baseline Real-Data Fit

**Status:** ⚠️ Dataset choice needs reconsideration

**Strengths:**
- Correct milestone (end-to-end real data)
- Single dataset focus (not multi-dataset benchmarking)

**Major Concern — Dataset Choice:**

Current suggestion: **Human3.6M**

**Challenges with Human3.6M** (not explicitly in overview, but worth noting):
- Multiple skeleton formats (17-joint vs 32-joint)
- Complex camera calibration (intrinsics + extrinsics per subject)
- Large file sizes (gigabytes per subject)
- Licensing/access complexity

**Alternative Recommendation** (from broader context):

**AIST++ Dance Dataset**:
- Clean, well-documented
- Simple skeleton (17 joints, COCO-like)
- Public, easy download
- Multiple camera views with calibration
- Good motion variety (dance moves)
- **Advantage**: More accessible starting point

**Other Candidates** (for future consideration):
- **3DPW**: Outdoor/in-the-wild scenarios (challenging)
- **CMU Panoptic Studio**: Excellent multi-camera (>100 cameras), if multi-view is priority

**Recommendation**: 
- Start with **AIST++** for Phase 0.2.5 (easier success)
- Defer Human3.6M to future multi-dataset work or Phase 0.2.6
- Document choice explicitly in detailed spec

**Skeleton Mapping Design:**

ChatGPT conversation suggests modular structure:
```python
gimbal/datasets/
    __init__.py
    base.py              # Abstract Dataset class
    skeleton_mapper.py   # SkeletonMapper class
    aist_plus.py         # AIST++ loader
    human36m.py          # Human3.6M loader (future)
```

**SkeletonMapper interface**:
```python
class SkeletonMapper:
    """Maps dataset-specific skeleton to GIMBAL skeleton."""
    def __init__(self, source_joints, target_joints, parent_map):
        pass
    
    def map_keypoints(self, y_dataset):
        # y_dataset: (C, T, K_dataset, 2)
        # returns: (C, T, K_gimbal, 2)
        pass
    
    def map_groundtruth(self, x_dataset):
        # x_dataset: (T, K_dataset, 3)
        # returns: (T, K_gimbal, 3)
        pass
```

**Missing Data Handling**:
- Real datasets have NaNs in keypoint observations
- v0.1.2 has `valid_obs` mask handling
- **Action**: Verify this works correctly with multi-frame missing data
- **Test**: Create synthetic sequence with 30% missing data, verify inference

---

### Phase 0.2.6 — Real-Data Diagnostics & Data-Driven Anatomical Priors

**Status:** ✅ Good extension of 0.2.4

**Strengths:**
- Logical progression from synthetic to real diagnostics
- Refining priors based on data is correct approach

**Metric Clarifications:**

1. **MPJPE Variants**:
   - "MPJPE" is ambiguous. Specify:
     - **PA-MPJPE** (Procrustes-aligned): Removes global rotation/translation/scale
     - **MPJPE** (absolute): Includes global alignment errors
   - **Recommendation**: Report both, emphasize PA-MPJPE for pose quality

2. **Bone Length Error**:
   - Current: vague
   - **Recommendation**: Use **Mean Absolute Percentage Error (MAPE)**:
     ```
     MAPE = mean(|bone_pred - bone_gt| / bone_gt) × 100%
     ```
   - Normalizes across different body sizes

3. **Velocity Error**:
   - Finite difference approximation: `v[t] = (x[t+1] - x[t-1]) / (2Δt)`
   - Use L2 norm per joint, average over time and joints
   - **Recommendation**: Report velocity RMSE (interpretable in m/s or cm/s)

**Missing Data Diagnostics** (not in overview):
- Real datasets have missing observations
- **Add**: Reconstruction quality stratified by observation density:
  - High-confidence keypoints (observed in ≥3 cameras)
  - Medium-confidence (observed in 1-2 cameras)
  - Interpolated (not observed, inferred from bone constraints)
- Metric: MPJPE separately for each stratum

---

### Phase 0.2.7 — Coarse PCA + Low-D HMM + Transition Upsampling

**Status:** ⚠️ Most complex phase, needs careful scoping

**Strengths:**
- Novel approach to learning dynamics at coarse scale
- Matrix root / continuous-time conversion is elegant
- **Aligns with ChatGPT top priority**: "Top tier: very low effort, high payoff"

**Major Design Decisions from ChatGPT Conversation:**

#### 1. Downsampling Strategy (User's Preferred Approach)

ChatGPT conversation reveals **explicit user preference** for modular decomposition:

**Four Modules:**
1. **Downsampling**: 60 Hz → 6 Hz data preparation
2. **Timestep-independent HMM fitting**: Fit at 6 Hz with dwell-time-aware priors
3. **Extending HMM fits**: 6 Hz transition matrix → 60 Hz transition matrix
4. **Fitting MMs (non-hidden)**: Fit emissions given deterministic state probabilities at 60 Hz

**Key Insight from User**:
> "I think extending a 6 Hz transition matrix to 60 Hz will be easy and we can then use it to turn a 6 Hz state probability matrix into a 60 Hz probability matrix. We can use to set a dynamic prior on the emissions and that will work well."

**Transition Matrix Rescaling:**
- User prefers: Fit at 6 Hz, extend to 60 Hz, use as prior (not just interpolation)
- Two approaches discussed:
  - **Matrix root**: Find A_high such that (A_high)^10 ≈ A_low
  - **Continuous-time generator**: Q = logm(A_low)/Δt_low, then A_high = expm(Q * Δt_high)
- **Recommendation**: Implement both, start with matrix root (simpler)

**Numerical Stability for Matrix Root**:
```python
def matrix_root(A, r):
    """Compute A^(1/r) via eigendecomposition."""
    eigvals, eigvecs = np.linalg.eig(A)
    
    # Safeguards:
    eigvals = np.clip(eigvals, 1e-3, 0.999)  # Away from 0 and 1
    eigvals_root = eigvals ** (1/r)
    
    A_root = eigvecs @ np.diag(eigvals_root) @ np.linalg.inv(eigvecs)
    A_root = A_root / A_root.sum(axis=1, keepdims=True)  # Re-stochasticity
    
    return A_root
```

**Downsampling Rate:**
- Overview suggests: 60 Hz → 6 Hz (10× reduction)
- ChatGPT conversation cautions: 10× may be aggressive
  - 6 Hz ≈ 167 ms per frame
  - Fast motions (arm swings, jumps) may alias
- **Recommendation**: Start with **3-5× downsampling** (60→20 Hz or 60→12 Hz)
  - Preserves sub-second dynamics
  - Test sensitivity: try [3×, 5×, 10×] and compare

#### 2. PCA Feature Space Specification

ChatGPT conversation has detailed discussion:

**User Understanding (Phase 7.2 discussion)**:
1. Given data stream (t,k,3), apply PCA → full decomposition
2. PCs are (3*k, 1), scores/loadings are (t, 3*k)
3. Keep m PCs with variance threshold
4. Fit HMM on (t, m) projection

**Ambiguity**: What goes into PCA?

**Options Discussed**:
- **Option A**: PCA on positions `x_all` (T×K×3) → flattened (T, 3K)
  - Pro: Full pose representation
  - Con: Not rotation-invariant, root position dominates variance
  
- **Option B**: PCA on directions `U` (T×K×3) → flattened (T, 3K)
  - Pro: More rotation-invariant, matches v0.1.3 directional model
  - Con: Loses global position information
  
- **Option C**: PCA on root-relative positions (T×K×3, subtract root)
  - Pro: Translation-invariant
  - Con: Still rotation-dependent
  
- **Option D**: PCA on concatenated [positions, velocities] (T×6K)
  - Pro: Captures both pose and dynamics
  - Con: Mixes position and velocity scales

**Recommendation** (from ChatGPT conversation): 
- **Start with Option B** (directions `U`) for Phase 0.2.7
- Rationale: Consistency with v0.1.3, more rotation-invariant
- **Validate**: Check if ~80% variance achievable with D ≈ 10-20
- **Fallback**: If insufficient, try Option A with root-centering
- **Document explicitly** in detailed spec

**Dimensionality Choice**:
- User asks: "How much variance to capture to make reconstruction legitimate?"
- ChatGPT response: Depends on use case
  - For segmentation/state discovery: 90-95% variance OK
  - For precise 3D pose reconstruction: 98-99% needed
- **Recommendation**: Phase 0.2.7 is for **segmentation/state discovery**, not reconstruction
  - Target: 85-90% variance with m=10-20 dimensions
  - PCA+HMM is "low-dimensional lens", not replacement for full GIMBAL pipeline

#### 3. Missing Data and PCA

**Critical Issue from ChatGPT Conversation**:
> "The whole point of the exercise is that we have nans and need to interpolate."

**Two Problems**:
1. **Fitting PCA with NaNs**: Can't use standard PCA on incomplete data
2. **Projecting incomplete frames**: How to get z_t when x_t has missing values?

**Solutions Discussed**:

**For Fitting**:
- **Probabilistic PCA (PPCA) with missing data**: EM algorithm handles NaNs naturally
- **Matrix completion methods**: Low-rank approximation with missing entries
- **Recommendation**: Use PPCA with missing data (conceptually clean)

**For Projection**:
- Given PCA loadings W_m and mean μ, project incomplete frame x_t
- **Masked least squares**:
  ```python
  # For frame t with mask M_t (observed dimensions):
  z_t = (W_{M_t}ᵀ W_{M_t})^{-1} W_{M_t}ᵀ (x_{t,M_t} - μ_{M_t})
  ```
- Small per-frame regression problem, solvable even with missing data

**PCA Inside vs Outside Bayesian Model**:
- User suggests: "Could make PCA part of larger model (inside Bayesian analysis)"
- ChatGPT response: Complicates graph, identifiability tangles, doesn't solve reconstruction
- **Recommendation**: Keep PCA **outside** as preprocessing
  - Use PPCA for missing-data-aware fitting
  - Masked projection for incomplete frames
  - Simpler, more modular

#### 4. High-Level API Design (from ChatGPT Conversation)

User emphasizes modular design:

```python
def hierarchical_hmm_downsample_pipeline(
    U_high, log_obs_high,
    S, factor, dt_high,
    prior_config, emission_model_config
):
    # 1. Downsampling
    ds = downsample_stream(U_high, log_obs_high, factor=factor)
    
    # 2. Fit coarse HMM at 6 Hz
    dt_low = factor * dt_high
    hmm_low = fit_hmm_at_rate(logp_emit_low, S=S, dt_low=dt_low, ...)
    
    # 3. Extend to 60 Hz: transitions + state priors
    priors_high = extend_hmm_to_high_rate(hmm_low, ds, dt_high)
    
    # 4. Fit emissions (mixture model) at full rate
    emission_fit = fit_emissions_with_state_probs(
        data_high=(U_high, log_obs_high),
        state_probs_high=priors_high.state_probs_high,
        ...
    )
    
    return {
        "downsample": ds,
        "hmm_low": hmm_low,
        "priors_high": priors_high,
        "emission_fit": emission_fit
    }
```

**Benefits of Modular Design**:
- Each piece testable in isolation
- Can swap implementations (simple vs fancy rescaling)
- Respects conceptual picture: state dynamics on slow timescale, emissions on fast

---

### Phase 0.2.8 — PCA-Informed Priors for Full Model & State-Number Selection

**Status:** ✅ Good culmination, model selection details needed

**Strengths:**
- Closes the loop (low-D HMM → full model priors)
- State-number selection is critical and correctly positioned

**Model Selection Specification:**

Current: "Compare across S using real-data geometry/temporal metrics, HMM diagnostics, model comparison measures"

**"Model comparison measures" needs specification:**

**Options from ChatGPT Conversation**:

1. **Penalized Likelihood** (fast, standard):
   - BIC: `-2 log(L) + k log(n)`
   - AIC: `-2 log(L) + 2k`
   - Pro: Fast to compute
   - Con: Doesn't account for posterior uncertainty

2. **Bayesian Model Comparison** (more robust):
   - **WAIC** (Widely Applicable Information Criterion): `az.waic(idata)`
   - **LOO-CV** (Leave-One-Out Cross-Validation): `az.loo(idata)`
   - Pro: Accounts for posterior uncertainty, better for hierarchical models
   - Con: Requires full posterior samples

3. **Predictive Performance** (most interpretable):
   - Hold-out validation on withheld frames
   - Compute MPJPE on predicted poses
   - Pro: Direct measure of generalization
   - Con: Requires splitting dataset

**Recommendation**: 
- **Primary**: Use **WAIC** (PyMC built-in, standard for Bayesian models)
- **Secondary**: Report BIC for comparison
- **Validation**: If resources allow, hold-out 10% of frames for predictive check

**State-Number Selection Strategies** (from ChatGPT conversation):

Three approaches discussed:

1. **Overcomplete S + post-hoc grouping**:
   - Run with S larger than needed
   - Cluster states after sampling based on similarity
   - Merge some for interpretation
   - Pro: Single fit, captures sub-states
   - Con: Heavier label switching
   - **Use case**: Exploratory analysis

2. **Model comparison over S** (PRIMARY):
   - Fit models for S ∈ {2,3,4,5,6}
   - Compare using LOO/WAIC
   - Pro: Standard, defensible
   - Con: Multiple fits required
   - **Use case**: Formal model selection

3. **Dirichlet Process / HDP-HMM**:
   - Nonparametric state-number model
   - Pro: Elegant, automatic state discovery
   - Con: Major structural change, complex inference
   - **Classification**: "Maybe someday" (research side-quest)

**Recommendation**: 
- Phase 0.2.8 uses **approach #2** (model comparison) as primary
- Optional **approach #1** for exploratory runs
- Defer **approach #3** to future research

**Low-D to Full-Pose Mapping**:

Current: "Map low-D state means → full-pose canonical joint directions"

**Non-trivial implementation issues**:

1. **PCA inverse transform**:
   ```python
   x_full = x_lowD @ V_pca.T + mean_pose
   ```

2. **Normalization to unit directions** (if PCA was on directions):
   ```python
   mu[s,k,:] = x_full[s,k,:] / ||x_full[s,k,:]||
   ```
   - Need safeguard for near-zero norms

3. **Gimbal lock / degenerate poses**:
   - Some PCA states may have near-zero direction vectors
   - **Recommendation**: Minimum norm threshold (e.g., 0.1), flag degenerate states

4. **Consistency check**:
   - PCA state means give global poses
   - v0.1.3 uses global directions
   - **Verify**: Mapping is consistent with v0.1.3 directional model

**Recommendation**: Document mapping carefully with:
- Pseudo-code for transform
- Edge case handling (zero norms, NaNs)
- Validation test: round-trip full-pose → low-D → full-pose ≈ identity

---

## 2. Cross-Cutting Concerns

### Testing Strategy

**Current State**: v0.1 has comprehensive tests (`tests/test_hmm_v0_1_*.py`)

**v0.2 Recommendation**: Each phase produces test file:

```
tests/
  test_v0_2_1_coarse_priors.py       # Anatomical constraint integration
  test_v0_2_2_clustering_priors.py   # Empirical-Bayes prior generation
  test_v0_2_4_diagnostics.py         # Diagnostic functions
  test_v0_2_5_dataset_loader.py      # AIST++ loader, skeleton mapping
  test_v0_2_6_real_metrics.py        # MPJPE, bone error, velocity
  test_v0_2_7_pca_hmm.py             # PCA+HMM module, downsampling
  test_v0_2_8_prior_mapping.py       # Low-D → full-pose mapping
```

**Shared Test Data**:
```
tests/fixtures/
  synthetic_sequence_S3_T50.npz      # Small synthetic sequence
  aist_sample_5sec.npz               # 5-second real data excerpt
  test_skeleton_maps.json            # Sample skeleton mappings
```

### Notebook Organization

**Current State**: Notebooks accumulating in `notebook/` directory

**Concern** (from ChatGPT conversation): v0.2 will add many notebooks (1-2 per phase). Without organization, becomes unwieldy.

**Recommendation**: Restructure before Phase 0.2.1:

```
notebook/
  v0.1/                              # Archive v0.1 demos
    demo_v0_1_complete.ipynb
    demo_pymc_*.ipynb
  v0.2/
    0.2.1_anatomical_priors/
      demo_coarse_priors.ipynb
      validate_sanity_filters.ipynb
    0.2.2_clustering_priors/
      demo_kmeans_priors.ipynb
    0.2.3_sampler_comparison/
      compare_nutpie_blackjax.ipynb
    0.2.4_diagnostics/
      demo_synthetic_diagnostics.ipynb
    0.2.5_first_real_data/
      demo_aist_baseline_fit.ipynb
    0.2.6_real_diagnostics/
      demo_real_metrics.ipynb
      demo_refined_priors.ipynb
    0.2.7_pca_hmm/
      demo_coarse_dynamics.ipynb
      demo_downsampled_hmm.ipynb
    0.2.8_informed_priors/
      demo_state_selection.ipynb
```

**Benefits**:
- Clear phase boundaries
- Easy to find relevant work
- Archive previous versions without clutter

### Documentation Cadence

**Current Pattern** (v0.1): Excellent documentation structure:
- `v0.1.X-detailed-spec.md` (before implementation)
- `v0.1.X-completion-report.md` (after implementation)
- `v0.1.X-review*.md` (during iteration)

**Recommendation**: Maintain this pattern for v0.2

**For each phase 0.2.X**:
1. **Before implementation**: `v0.2.X-detailed-spec.md`
   - Detailed scope, API designs, test plan
   - Reference this review document
   
2. **During implementation**: `v0.2.X-implementation-notes.md` (optional)
   - Track decisions, issues, deviations from spec
   
3. **After implementation**: `v0.2.X-completion-report.md`
   - What was implemented, test results, known limitations
   - Follow v0.1 completion report template

**Consistency**: Use same format/structure across all v0.2.X documents

### Code Hygiene (from ChatGPT Conversation)

**"De-AI-ing" Requirement**:
- User explicitly requests: "de-AI-ing of the code to make it human readable and up to good human programming standards"
- **Scope**:
  - Remove weird abstractions or over-clever code
  - Refactor to clean, human style
  - Improve variable naming, comments
  - Simplify complex expressions

**Code Audit**:
- Not just notebooks, but also functions/modules
- Review for:
  - Clarity and maintainability
  - Consistent style
  - Appropriate abstractions
  - Good documentation

**Recommendation**: Include code quality review in each phase completion report

---

## 3. Deferred Ideas Assessment

**Current List**: 10 items explicitly deferred

**Assessment**: ✅ Good to document these explicitly. Prevents scope creep.

**Suggested Additions** (from ChatGPT conversation):

11. **Adaptive state splitting**: Start with small S, automatically add states if residuals large
12. **Joint-specific state models**: Some joints (fingers) may not need state-dependent priors
13. **Temporal multi-scale HMM**: Different time scales for fast (hand) vs slow (torso) joints
14. **Real-time / online inference**: Streaming data processing (vs batch)
15. **Uncertainty quantification tools**: Credible intervals on reconstructions, entropy maps
16. **Sticky HMM transitions**: Self-transition bias (discussed in ChatGPT, ranked "Top tier")
17. **Grouped/hierarchical κ**: Anatomical groupings (torso, limbs) with shared concentration

---

## 4. Prerequisites and Success Criteria

**Missing from Current Overview**: Prerequisites and completion criteria

### Recommended Prerequisites Section

Add to v0.2-overview.md:

```markdown
## Prerequisites

Before beginning Phase 0.2.1:

1. **v0.1 Complete and Tagged**
   - Git tag `v0.1` exists
   - All v0.1 tests passing (`tests/test_hmm_v0_1_*.py`)
   - `demo_v0_1_complete.ipynb` runs end-to-end without errors

2. **Environment Validated**
   - Python 3.10+
   - PyMC 5.10+
   - nutpie 0.13+
   - arviz 0.18+
   - Standard scientific stack (numpy, scipy, matplotlib, sklearn)

3. **Baseline Performance Documented**
   - Synthetic demo sampling time: ~X seconds for T=50, S=3, K=5
   - Convergence metrics: R-hat < 1.01, ESS > 400
   - Memory footprint: ~Y MB
```

### Recommended Success Criteria

Add success criteria for each phase:

```markdown
## Success Criteria by Phase

### Phase 0.2.1
- [ ] Anatomical priors reduce sampling divergences by >50% on synthetic data
- [ ] Sanity filters remove <5% of frames (not overly aggressive)
- [ ] Model still converges with priors enabled (R-hat < 1.01)
- [ ] Backward compatibility: v0.1 demos run with new code

### Phase 0.2.2
- [ ] Clustering produces interpretable state assignments (silhouette score > 0.3)
- [ ] Empirical priors improve convergence speed by >20% vs uniform priors
- [ ] Generated priors are programmatically reusable (saved to .npz file)

### Phase 0.2.3
- [ ] Decision made: Keep nutpie OR switch to alternative (rationale documented)
- [ ] Chosen sampler converges on test matrix (S≤5, T≤200)
- [ ] Tuning guidelines documented (step size, warmup, etc.)

### Phase 0.2.4
- [ ] Diagnostic module (`gimbal/diagnostics.py`) created and tested
- [ ] Detects known "broken" scenarios (label switching, poor mixing)
- [ ] Two-tier API: human-facing and machine-facing metrics
- [ ] Label switching correction implemented and validated
- [ ] Integrated into demo notebooks with <10 lines of user code

### Phase 0.2.5
- [ ] Dataset loader produces valid GIMBAL format (shape checks pass)
- [ ] Skeleton mapping documented and tested
- [ ] Baseline fit completes without errors on ≥1 real sequence
- [ ] Qualitative reconstruction is plausible (not obviously broken)

### Phase 0.2.6
- [ ] PA-MPJPE < 100mm on test sequence (depending on dataset difficulty)
- [ ] Bone length MAPE < 10%
- [ ] Velocity RMSE < threshold (dataset-dependent)
- [ ] Diagnostic plots automatically generated and interpretable
- [ ] Refined anatomical priors derived from real data

### Phase 0.2.7
- [ ] PCA captures ≥85% variance with D ≤ 20 dimensions
- [ ] Low-D HMM fits without errors for S ∈ [2,3,4,5]
- [ ] Matrix root upsampling produces valid stochastic matrices (rows sum to 1)
- [ ] Downsampled HMM + full-rate lifting pipeline working end-to-end
- [ ] Missing data handling tested and validated

### Phase 0.2.8
- [ ] State-number selection recommends specific S (or narrow range) using WAIC
- [ ] PCA-informed priors improve PA-MPJPE by ≥20% vs coarse priors (Phase 0.2.1)
- [ ] Model comparison metrics favor selected S
- [ ] Low-D → full-pose mapping validated (round-trip test)
```

---

## 5. Estimated Timeline

**Missing from Current Overview**: Time estimates

**Recommendation**: Add rough timeline to help plan resources

**Suggested Estimates** (person-weeks, assumes 1 FTE):

| Phase | Estimated Time | Rationale |
|-------|---------------|-----------|
| 0.2.1 | 1 week | Anatomical constraints relatively straightforward |
| 0.2.2 | 1 week | k-means standard, prior generation new code |
| 0.2.3 | 3 days | Sampler comparison mostly running experiments |
| 0.2.4 | 1.5 weeks | Diagnostics module + label switching + visualization |
| 0.2.5 | 2 weeks | Dataset loader + skeleton mapping is fiddly |
| 0.2.6 | 1 week | Extending diagnostics, refining priors |
| 0.2.7 | 2.5 weeks | PCA+HMM most complex, missing data handling |
| 0.2.8 | 1.5 weeks | Model comparison, documentation |
| **Total** | **~11 weeks** | ~2.75 months assuming full-time work |

**Caveats**:
- Assumes no major blockers (data access issues, sampler bugs)
- Includes time for documentation and testing
- Does not include time for iterating on review feedback
- ChatGPT conversation suggests this could extend if real data proves challenging

---

## 6. Interface Changes and Backward Compatibility

**Consideration** (from ChatGPT conversation): v0.2 will modify v0.1 code

**Recommendation**: Maintain backward compatibility where possible

**Example**: Phase 0.2.1 changes to `hmm_directional.py`:

```python
def add_directional_hmm_prior(
    U,
    log_obs_t,
    S,
    name_prefix="dir_hmm",
    share_kappa_across_joints=False,
    share_kappa_across_states=False,
    kappa_scale=5.0,
    # NEW in v0.2.1: Anatomical priors (optional, default None = no prior)
    mu_prior_mean=None,     # (K, 3) or (S, K, 3)
    mu_prior_kappa=None,    # scalar or (K,) or (S, K)
    **kwargs
):
    """
    Add directional HMM prior to PyMC model.
    
    ...existing docstring...
    
    NEW in v0.2.1:
    ----------
    mu_prior_mean : array-like, optional
        Prior mean directions for anatomical constraints.
        Shape (K, 3): Same prior for all states
        Shape (S, K, 3): State-specific priors
        If None (default), uses uniform prior on sphere (v0.1 behavior)
    
    mu_prior_kappa : float or array-like, optional
        Concentration for anatomical priors (higher = stricter).
        If None (default), no anatomical prior applied.
    """
    # Implementation ensures None defaults preserve v0.1 behavior
```

**Testing**: Each phase should verify v0.1 demos still run with new code (backward compatibility tests)

---

## 7. Priority Concerns Before Proceeding

If generating detailed specs now, these **3 issues should be resolved first**:

### Priority 1: Phase 0.2.1 vs Biomechanical Priors Relationship

**Issue**: `v0.2-biomechanical-priors-notes.md` discusses temporal smoothness and parent-relative directions. Unclear if these are part of 0.2.1 or separate.

**Resolution Needed**:
- **Option A**: Incorporate temporal smoothness into 0.2.1
- **Option B**: Defer temporal smoothness to 0.2.4 (after diagnostics)
- **Option C**: Create separate Phase 0.2.1.5 for temporal priors

**Recommendation**: **Option B** (defer to 0.2.4). Rationale:
- Phase 0.2.1 should focus on **static anatomical constraints** only
- Temporal smoothness is regularization technique best validated after diagnostics exist
- Simpler to debug/iterate if concerns separated
- ChatGPT conversation emphasizes modular, focused improvements

### Priority 2: Phase 0.2.5 Dataset Selection

**Issue**: Human3.6M suggested but has significant complexity.

**Resolution Needed**: Choose specific dataset before generating 0.2.5 detailed spec.

**Recommendation**: Switch to **AIST++** for Phase 0.2.5. Document:
- Download instructions
- Skeleton format (17-joint COCO-like)
- Camera calibration format
- File structure

Reserve Human3.6M for future multi-dataset work.

### Priority 3: Phase 0.2.7 PCA Feature Space

**Issue**: "PCA on full-pose space" is ambiguous (positions? directions? both?).

**Resolution Needed**: Specify exactly what features go into PCA before implementing.

**Recommendation**: Start with **directions `U`** (ChatGPT Option B):
- Rationale: Directly matches v0.1.3 directional model
- Benefit: More rotation-invariant than positions
- Test: Verify ≥85% variance achievable with D ≤ 20
- Document in 0.2.7 detailed spec. Include fallback if insufficient.

---

## 8. Recommended Next Steps

1. **Resolve Priority Concerns** (§7):
   - ✅ Confirm 0.2.1 scope excludes temporal smoothness
   - ✅ Confirm AIST++ for Phase 0.2.5
   - ✅ Confirm PCA feature space (U directions)

2. **Update v0.2-overview.md**:
   - Add Prerequisites section (§4)
   - Add Success Criteria per phase (§4)
   - Add Timeline estimates (§5)
   - Clarify Phase 0.2.5 dataset choice
   - Add Phase 0.2.7 downsampling details and PCA feature space specification

3. **Add Label Switching to Phase 0.2.4** scope:
   - Explicitly mention Hungarian algorithm implementation
   - Add cross-chain alignment diagnostics
   - Two-tier diagnostic API design

4. **Restructure Notebook Directory** (§2):
   - Create `notebook/v0.1/` and `notebook/v0.2/` structure
   - Archive v0.1 demos before starting 0.2.1

5. **Generate Phase 0.2.1 Detailed Spec**:
   - Use template from v0.1.X-detailed-spec.md documents
   - Include:
     - Static anatomical constraint definitions (per-joint prior directions)
     - Sanity filter specifications (with thresholds)
     - API changes to `add_directional_hmm_prior()` with backward compatibility
     - Test plan with success criteria
     - Example notebook outline

6. **Begin Phase 0.2.1 Implementation** after spec review

---

## 9. Summary Recommendations

**Keep As-Is:**
- ✅ Overall 8-phase structure
- ✅ Phase ordering and dependencies
- ✅ Deferred ideas list
- ✅ Scope boundaries (focus on priors, diagnostics, real data)

**Clarify Before Phase 0.2.1:**
- Relationship to biomechanical-priors-notes.md (defer temporal smoothness)
- Phase 0.2.1 exact scope (static anatomical only)
- Sanity filter specifications (3-5 concrete filters)

**Enhance for Robustness:**
- Add label switching handling to Phase 0.2.4 (critical for HMMs)
- Specify MPJPE variants (PA-MPJPE vs MPJPE)
- Add memory footprint to sampler comparison metrics
- Two-tier diagnostic API (human-facing + machine-facing)

**Simplify for Initial Success:**
- Use AIST++ instead of Human3.6M for Phase 0.2.5
- Start with 3-5× downsampling (not 10×) in Phase 0.2.7
- Use WAIC for model selection in Phase 0.2.8
- PCA on directions `U` (not positions or hybrid)

**Incorporate ChatGPT Conversation Insights:**
- Modular API design for downsampled HMM (4-module structure)
- Explicit user preference for transition matrix rescaling approach
- Missing data handling for PCA (PPCA with masked projection)
- Emphasis on "low-hanging fruit" algorithmic improvements
- Code quality and "de-AI-ing" requirements

**Document for Maintainability:**
- Add prerequisites and success criteria
- Add timeline estimates
- Restructure notebook/ directory before starting
- Maintain v0.1 documentation pattern
- Ensure backward compatibility with v0.1

---

## 10. Integration with ChatGPT Planning Conversation

**Key Insights from Planning Conversation Not in Original Overview:**

1. **Algorithmic Improvement Prioritization**:
   - Top tier: Downsampled HMM, PCA on pose, sticky transitions, k-means initialization
   - Middle tier: Grouped kappa, anatomical priors, VB experiments
   - Lower tier: DP-HMM, full PCA integration into Stage 3

2. **Modular Design Philosophy**:
   - User emphasizes clean, testable module interfaces
   - Four-module downsampling pipeline design
   - Skeleton mapper abstraction for datasets

3. **Practical Constraints**:
   - User wants to avoid "endless time on improving sampling"
   - Focus on structural improvements (dimensionality, time horizon)
   - Only then consider low-level sampler tweaks

4. **Code Quality Standards**:
   - "De-AI-ing" of code base
   - Human-readable, maintainable style
   - Good abstractions, clear documentation

5. **Diagnostic Requirements**:
   - Two-tier system (human vs machine-facing)
   - Label switching is critical for HMMs
   - "Traffic light" summary for quick assessment

---

**Overall Assessment**: The v0.2 roadmap is **sound and ready to proceed** after addressing the 3 priority concerns in §7 and incorporating the clarifications above. The ChatGPT planning conversation reveals important design preferences and priorities that should guide implementation. The progression is logical, the scope is appropriate, and the deferred ideas are well-chosen.

**Recommended Action**: Update overview document with prerequisites, success criteria, and clarifications, then generate Phase 0.2.1 detailed spec.
