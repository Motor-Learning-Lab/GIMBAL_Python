`# GIMBAL v0.2.1 — Divergence Debugging Plan (Living Document)

**Editor Note (Do Not Remove):**
All future edits to this document must:

1. **Preserve the entire existing content.** Do not overwrite, truncate, or "start fresh." Always append or modify specific sections in place.
2. **Respect file paths and names exactly as written.** Where a file path or filename is given, it is a specification, not a suggestion.
3. **Avoid modifying core library code for debugging.** Do **not** add temporary flags, branches, or knobs to `gimbal/pymc_model.py` or other core modules for the diagnostics described here. All structural variants for testing live in the *test layer* only.
4. **Avoid claiming that problems are solved.** Test reports must describe what the results *show* (e.g., divergence counts, step sizes), not assert that an issue is fully resolved.

This ensures continuity and prevents accidental divergence between the debugging plan and the codebase.

---

## Status & Organization (Updated 2025-12-14)

### Overview
This plan organizes divergence debugging around **stable Issue IDs** (I1, I2, I3) and **Test Group IDs** (TG09, TG10, TG11a, TG11b). Each Issue represents a conceptual hypothesis about pathological geometry; each Test Group is an empirical A/B comparison.

### Naming Convention
- **Issues:** I1, I2, I3, ... (never reused, stable conceptual buckets)
- **Test Groups:** TG09, TG10, TG11a, TG11b, ... (zero-padded, letters for subvariants)
- **Files:** `test_tg##_i#_slug.py`, `report_tg##_i#_slug.md`, `results_tg##_i#_slug.json`

### Current Status

#### Issue I1: Root Dynamics / Temporal Coupling
**Test Group:** TG09 (`test_tg09_i1_root_fixed.py`)  
**Status (2025-12-14):** ✅ **Tested, insufficient**  
- Anchoring implemented (`x0_root` + `eps_root` with 1m std prior)
- Result: **Still 100% divergences** in anchored baseline
- Fixed-root variant: **0% divergences**
- **Conclusion:** Root parameterization is correct but insufficient. Evidence points to I2 as root cause.

#### Issue I2: Camera Conditioning / Projection Geometry
**Test Group:** TG10 (`test_tg10_i2_direct_3d.py`)  
**Status (2025-12-14):** ✅ **Confirmed as primary cause**  
- Camera projection: **100% divergences** (1000/1000)
- Direct 3D likelihood: **3.2% divergences** (32/1000)
- **Divergence reduction: 31×**
- **Conclusion:** Camera projection geometry is the **dominant** pathology. Perspective projection + root RW creates extreme curvature.
- **Residual 3.2%:** Suggests skeleton hierarchy has minor contribution (test TG11 for quantification)
- **Next:** TG10A (orthographic) to isolate depth-division effects

#### Issue I3: Parameter Redundancy / Identifiability
**Test Groups:** TG11a (`test_tg11a_i3_redundancy_fixed.py`), TG11b (`test_tg11b_i3_redundancy_priors.py`)  
**Status (2025-12-14):** ✅ **Confirmed as co-equal critical bottleneck**  
- **TG11a completed:** Baseline 100% divergences (17 free RVs) → Fixed variant 0% divergences (4 free RVs)
  - **Divergence reduction: ∞×** (complete elimination by fixing directions/lengths to GT)
- **TG11b completed:** Baseline 100% divergences → Strong priors variant 0% divergences (24 free RVs)
  - **Divergence reduction: ∞×** (strong GT-based priors as effective as hard-fixing)
  - Priors: Length SD 2%, raw direction SD 0.05
- **Conclusion:** Parameter redundancy is NOT masked by I2—it is an equally critical pathology. Both hard-fixing (TG11a) and strong informative priors (TG11b) completely eliminate divergences. The multiple pathways for explaining observations (moving root vs changing directions vs adjusting lengths) create pathological geometry.
- **Architectural implication:** Must address BOTH I2 (camera) AND I3 (redundancy) simultaneously for working sampler. Strong directional priors are a viable solution path.

### Archive: Test Groups 1-8
**Status:** ✅ **Complete**  
**Document:** `v0.2.1_divergence_tg01_08_archive.md` (formerly `v0.2.1_divergence_test_plan.md`)  
**Summary:** Completed under pre-Issue framework. TG01-08 explored HMM effects, state counts, likelihood scaling, and hierarchical variance. Superseded by Issue-based framework (I1/I2/I3).

### Related Documents
- **This document:** Canonical roadmap for I1/I2/I3
- **Archive:** `v0.2.1_divergence_tg01_08_archive.md` (historical TG01-08 tests)
- **Naming guide:** `v0.2.1_divergence_naming_restructure.md` (detailed file mapping)
- **Quick reference:** `v0.2.1_divergence_naming_quick_reference.md` (cheat sheet)
- **Future deep dives:** `v0.2.1_divergence_i#_topic.md` (issue-specific analyses, as needed)

---

## Issue #1: Root Random-Walk (RW) Funnel

### Summary

The hierarchical structure:

* `sigma_root` (std of RW increments) as a free parameter
* `x_root[t] ~ GRW(sigma_root)` over ~100 frames
* Tight camera likelihoods

creates a classic hierarchical **scale–latent coupling**. If extreme, this induces a funnel geometry that NUTS cannot traverse.

### Minimal Diagnostic (A/B Test)

**A. Baseline model** (current):

* Hierarchical RW with free `sigma_root` (HalfNormal or Gamma)
* Centered parameterization

**B. Diagnostic variant:**

* Replace RW with fixed trajectory
* No `sigma_root`, no stochastic RW increments
* Use DLT-based or GT-based `x_root_init`:

```python
x_root = pm.Data("x_root", x_root_init)
```

### Expected Outcomes

* **If divergences collapse** in B → root RW hierarchy is a major contributor.
* **If divergences persist** → other structural issues dominate or contribute comparably.

### Metrics to Compare (Minimal)

* Divergence count / percentage
* Tuned step size
* Energy behavior (visual check)

### Interpretation

This test isolates whether the hierarchical part of the root motion is creating pathological geometry. If yes, it justifies exploring:

* Gamma(mode, sd) priors on `sigma_root`
* Non-centered RW parameterization
* Or both.

---

## Future Issues (Placeholders, To Be Expanded)

### Issue #2: Camera Likelihood Conditioning

This issue concerns whether the *camera projection layer itself* — rather than the skeleton hierarchy — is a major source of the pathological posterior geometry that causes NUTS to fail.

A sharp, high-dimensional camera likelihood can induce extreme curvature, especially in depth, causing NUTS to collapse its step size even when the underlying skeleton priors are reasonable.

#### Minimal Diagnostic #1 — Direct 3D Observation Test (Blunt Hammer)

Replace the camera likelihood entirely with a simple, well‑behaved 3D Gaussian observation model. This isolates the camera projection from the rest of the model.

**Original camera likelihood:**

```python
pm.Normal(
    "y_obs",
    mu=y_pred,
    sigma=obs_sigma,
    observed=y_observed,
)
```

**Diagnostic variant:**

```python
pm.Normal(
    "x_obs",
    mu=x_all,
    sigma=tau_3d,       # choose ~1–2 cm for synthetic data
    observed=x_true,    # synthetic ground‑truth 3D
)
```

Everything else in the model remains identical:

* Same priors (root RW, directions, lengths)
* Same initialization
* Same sampler configuration
* Same synthetic data

**Interpretation:**

* **If divergences drop sharply:** the camera projection layer or its conditioning is a major contributor to the geometry problems. Prior reparameterization alone will not fix the model until camera conditioning is addressed.
* **If divergences persist:** the skeleton hierarchy (Issue #1 or Issue #3) is the dominant source of the geometry issues.

This test cleanly partitions responsibility between the projection layer and the rest of the model.

#### Minimal Diagnostic #2 — Inflate obs_sigma (Fine Hammer)

If the full projection model still needs inspection, inflate `obs_sigma` by ×3–×10 and repeat the sanity test. A strong improvement in sampler behavior indicates that the reprojection likelihood is *too sharp relative to prior strength*.

---

### Issue #3: Parameter Redundancy (Root vs Directions vs Lengths)

This issue is about **multiple parameter pathways explaining the same observations**:

* Global root motion (`x_root`)
* Per-frame joint directions (`u_k` from `raw_u_k`)
* Bone lengths / segment scales

All three affect `x_all` and hence the reprojections. This creates redundant degrees of freedom and curved, partially flat manifolds in parameter space.

#### Minimal Diagnostic #1 — Hard-Fix Directions and Lengths (Blunt Hammer)

Use synthetic data where ground-truth 3D trajectories and skeleton parameters are known. Then:

* **Baseline:** full model as-is (root RW + free directions + free lengths).
* **Diagnostic variant:**

  * Fix joint directions `u_k(t)` to ground truth (or high-quality DLT estimates).
  * Fix bone lengths to their ground-truth values.

Implementation sketch:

```python
if fix_dirs_and_lengths:
    u_all = pm.Data("u_all", u_true)          # or DLT-based
    lengths = pm.Data("lengths", lengths_true)
else:
    # existing priors for raw_u_k, lengths, etc.
    ...
```

Everything else (root RW, cameras, likelihood, sampler settings) is identical.

**Interpretation:**

* **Divergences drop sharply when directions/lengths are fixed:**

  * Redundancy between root, directions, and lengths is a major contributor to geometric pathologies.
* **Little change in divergences:**

  * Redundancy is not dominant; focus returns to root RW (Issue #1) or camera conditioning (Issue #2).

#### Minimal Diagnostic #2 — Strong Data-Driven Priors (Fine Hammer)

Instead of hard-fixing, impose **strong, data-driven priors** on directions and lengths using ground truth from synthetic data:

* For each parameter θ (direction component or length), set:

  * Prior mean near ground truth.
  * Tight Gamma/Normal prior using `mode` and `sd` derived from GT variability.

This preserves uncertainty but heavily discourages large deviations.

**Interpretation:**

* If moving from weak priors → strong GT-based priors produces a large decrease in divergences (even without hard-fixing), that is further evidence that **parameter redundancy** is amplifying curvature.

Both diagnostics are meant to answer: *“Does removing or strongly constraining the extra ‘axes of explanation’ (directions/lengths) make the sampler behave more normally?”*

---

### Issue #4: Gauge Freedoms (Global Pose Weakly Anchored)

Possible symptoms:

* Global translation/rotation weakly constrained

Potential diagnostics:

* Temporary anchoring priors on `x_root[0]`, mean pose

---

### Issue #5: Directional Sphere Parametrization

Possible symptoms:

* Bad gradients when `raw_u_k` norms are tiny or large

Potential diagnostics:

* Log gradient norms of `raw_u_k`
* Toy directional model with known likelihood

---

### Issue #6: Data–Prior Mismatch (Initialization vs Prior Scale)

Possible symptoms:

* Divergences near initialization
* Step size collapse even after warmup

Potential diagnostics:

* Compare logp at init vs nearby draws
* Adjust prior ranges

---

## Concrete Test Suite for Issues #1–#3 (v0.2.1)

This section specifies the **only approved way** to implement and organize the diagnostics for Issues #1–#3 in v0.2.1.

### Numbering Policy (Mandatory)

To avoid ambiguity in implementation and reporting, v0.2.1 diagnostics must follow **one** consistent numbering system.

**Policy chosen:** Continue the existing v0.2.1 divergence-suite numbering.

* Existing divergence diagnostics are **Groups 1–8**.
* The Issue-focused diagnostics in this plan are **Groups 9–11**.
* Do **not** renumber Groups 9–11 as 1–3.
* Do **not** create a parallel numbering system.

Rationale: the new tests are extensions of the existing `tests/diagnostics/v0_2_1_divergence/` suite and must integrate cleanly into its existing `test_runner.py` and results/report conventions.

### Context: Existing Divergence Test Groups (1–8)

The repository already contains a working v0.2.1 divergence suite under `tests/diagnostics/v0_2_1_divergence/` with `test_runner.py` orchestrating the following existing groups:

* **Group 1:** Baseline (HMM OFF)
* **Group 2:** HMM Effect (HMM ON)
* **Group 3:** State Count (S=1,2,3)
* **Group 4:** Likelihood Scale
* **Group 5:** Divergence Localization
* **Group 6:** Root Variance Sensitivity
* **Group 7:** Bone Length Variance Sensitivity
* **Group 8:** Runtime Scaling

Issues #1–#3 in this document are implemented as **Groups 9–11** and should be integrated into the same `test_runner.py` structure.

### Standard Synthetic Data Configuration (Must Match) (Must Match)

Unless a test explicitly states otherwise, all new tests in Groups 9–11 must use:

* `synth_data = test_utils.get_standard_synth_data(T=100, C=3, S=3, seed=42)`
* This corresponds to the current defaults in `test_utils.get_standard_synth_data`, which uses:

  * `kappa=10.0`
  * `obs_noise_std=0.5`
  * `occlusion_rate=0.02`

All A/B comparisons within a test group must share the exact same `synth_data` object (or an exact regenerated copy with the same seed).

### Standard Sampler Configuration (Must Match)

Unless explicitly stated otherwise, all Groups 9–11 tests must use the existing standardized sampler wrapper:

* `trace = test_utils.sample_model(model, draws=200, tune=200, chains=1)`
* This uses `pm.sample(..., target_accept=0.95, return_inferencedata=True)`.

If a test changes sampler settings, it must:

* State the change explicitly in the test file header and in the markdown report,
* Apply the change to both A and B variants (unless the sampler change itself is the experimental variable).

### Global Rules for All New Diagnostics

To avoid ambiguity or accidental misuse, the following rules apply to **every** test group and file described below:

1. **File locations are mandatory, not advisory.**
   All new tests for this plan **must** live under:

   * `tests/diagnostics/v0_2_1_divergence/`

2. **Filenames are part of the specification.**
   When this document says `test_group_9_root_fixed.py` or `results_group_10_direct_3d.json`, those are the **exact** filenames that should be created.

3. **Core model code must not be modified for these diagnostics.**
   Do **not** add debug flags or branches to `gimbal/pymc_model.py` or other core modules for Issues #1–#3.
   All structural variants (root-fixed, direct-3D, fixed directions/lengths, etc.) must be implemented as **local model builders in the test files** (or, if strictly necessary, in `tests/diagnostics/v0_2_1_divergence/test_utils.py`).

4. **Each diagnostic test has a narrow responsibility.**
   Do not combine unrelated experiments into a single script. Each group below has a clear question; the corresponding test file should answer that question and nothing else.

5. **Results and reports are obligatory.**
   For every `test_group_X_*.py`, there must be a matching:

   * `results_group_X_*.json` (raw metrics, programmatically written), and
   * `report_group_X_*.md` (short human-readable summary of what the metrics show).

6. **Use existing utilities whenever possible (and document exactly which).**

   * Synthetic data must be generated via `test_utils.get_standard_synth_data(...)`.
   * Sampling must be done via `test_utils.sample_model(...)`.
   * Metric extraction should use `test_utils.extract_metrics(...)` and explicitly add any additional fields used in reports.

7. **Hold constants constant.**
   For each A/B comparison, hold the following fixed unless the test explicitly changes them:

   * Synthetic data configuration (T, C, S, kappa, obs_noise_std, occlusion_rate, seed)
   * Initialization method (DLT initialization from `gimbal.fit_params.initialize_from_observations_dlt`)
   * Sampler configuration (draws, tune, chains, target_accept)

8. **No "success" claims in test code or reports.**
   Tests should record metrics and, if desired, mark configurations as "better" or "worse" based on those metrics. They must **not** assert that an issue is fully solved.

9. **Define what counts as a "large" change (to avoid hand-waving).**
   For these diagnostics, treat the following as strong evidence of improvement:

   * Divergence rate drops by **≥ 10×** (e.g., 1.0 → ≤ 0.1), or
   * Divergences drop to **≤ 5%** with a clear increase in tuned step size.

   These are not "pass/fail" criteria; they are interpretation anchors.

These rules are intended to prevent an implementation from:

* Putting files in the wrong folder,
* Inventing arbitrary names,
* Modifying core model code for ad-hoc debugging,
* Or prematurely declaring success.

The following subsections define the test groups for Issues #1–#3.

### Issue #1: Root Random-Walk (RW) Funnel — Test Group 9

#### Implementation Checklist Header (copy verbatim into the test file)

* [ ] File path is exactly: `tests/diagnostics/v0_2_1_divergence/test_group_9_root_fixed.py`
* [ ] Results file path is exactly: `tests/diagnostics/v0_2_1_divergence/results_group_9_root_fixed.json`
* [ ] Report file path is exactly: `tests/diagnostics/v0_2_1_divergence/report_group_9_root_fixed.md`
* [ ] Uses `test_utils.get_standard_synth_data(T=100, C=3, S=3, seed=42)`
* [ ] Uses DLT initialization via `gimbal.fit_params.initialize_from_observations_dlt` (same as baseline)
* [ ] Sampler via `test_utils.sample_model(model, draws=200, tune=200, chains=1)` with identical settings in A/B
* [ ] Baseline variant uses `test_utils.build_test_model(..., use_directional_hmm=False, ...)`
* [ ] Variant replaces only the root RW block by setting `x_root = pm.Data("x_root", init_result.x_init[:, 0, :])`
* [ ] No changes to `gimbal/pymc_model.py` or other core modules
* [ ] Writes JSON metrics and a short markdown report; does not claim the issue is "solved"

**Question:** Is the hierarchical root random walk a major driver of divergences?

#### Test 9.1 — Root Fixed vs Root Hierarchical

**Files (names MUST match exactly):**

* `tests/diagnostics/v0_2_1_divergence/test_group_9_root_fixed.py`
* `tests/diagnostics/v0_2_1_divergence/results_group_9_root_fixed.json`
* `tests/diagnostics/v0_2_1_divergence/report_group_9_root_fixed.md`

**Fixed-value source (to avoid ambiguity):**

* The root-fixed variant must use **DLT-initialized root** by default (not ground truth). This keeps the test honest: it removes the root RW *uncertainty* without giving the model privileged access to ground truth.
* A GT-fixed root variant may be added later as an **optional** sensitivity check, but it is not part of Test 9.1.

**Model variants:** implemented in the test layer only.

1. **Baseline variant (hierarchical root RW):**

   * Reuse the existing helper, e.g. `build_test_model(..., use_directional_hmm=False, ...)` from `test_utils.py`, which calls the standard Stage 2 model builder.

2. **Root-fixed variant (DLT-fixed root):**

   * Implement a local builder (e.g. `build_test_model_root_fixed(...)`) inside `test_group_9_root_fixed.py`.
   * This builder should:

     * Use the same `synth_data` and DLT initialization path as the baseline.
     * Replace the root RW block with:

       ```python
       x_root = pm.Data("x_root", init_result.x_init[:, 0, :])  # (T, 3)
       ```
     * Build the remainder of the model so that:

       * Directions/lengths priors match the baseline (unless a later issue test changes them), and
       * The camera likelihood is identical to baseline.

**Runner integration:**

* Define `run_group_9_root_fixed()` in `test_group_9_root_fixed.py` and call it from `test_runner.py` as **Group 9**.

**Expected metrics:**

* Divergence counts / rates.
* Tuned step size (mean over draws): extract from `trace.sample_stats.step_size` if present.
* Tree depth saturation (optional but useful): `trace.sample_stats.tree_depth` and whether it hits `max_treedepth`.
* Energy/BFMI (optional): record E-BFMI or save an energy plot via existing plotting helpers.
* R̂/ESS are optional here because `chains=1` in the standard configuration; if chains are increased later, include them.

**Key outcome (interpretation, not a success claim):**

* If the root-fixed variant has **substantially fewer divergences** and a **larger tuned step size** than the baseline, this is strong evidence that the root RW hierarchy (Issue #1) makes a major contribution to the pathological geometry.
* If both behave similarly, it suggests that other issues (camera conditioning or redundancy) are at least as important.

**

* Large reduction in divergences and increase in step size for the root-fixed model strongly implicates the root RW hierarchy (Issue #1) as a core geometric problem.

---

### Issue #2: Camera Likelihood Conditioning — Test Group 10

#### Implementation Checklist Header (copy verbatim into the test file)

* [ ] File path is exactly: `tests/diagnostics/v0_2_1_divergence/test_group_10_direct_3d.py`
* [ ] Results file path is exactly: `tests/diagnostics/v0_2_1_divergence/results_group_10_direct_3d.json`
* [ ] Report file path is exactly: `tests/diagnostics/v0_2_1_divergence/report_group_10_direct_3d.md`
* [ ] Uses `test_utils.get_standard_synth_data(T=100, C=3, S=3, seed=42)`
* [ ] Uses the same DLT initialization path as baseline
* [ ] Sampler via `test_utils.sample_model(model, draws=200, tune=200, chains=1)` with identical settings in A/B
* [ ] Baseline variant uses `test_utils.build_test_model(..., use_directional_hmm=False, ...)`
* [ ] Variant replaces only the likelihood (camera → direct 3D) while keeping the same skeleton hierarchy
* [ ] Uses `x_true = synth_data["joint_positions"]` with shape `(T, K, 3)`
* [ ] Uses a single fixed `tau_3d = 0.02` meters (no sweeps)
* [ ] No changes to `gimbal/pymc_model.py` or other core modules
* [ ] Writes JSON metrics and a short markdown report; does not claim the issue is "solved"

**Question:** Does the camera projection layer and its likelihood scale dominate the pathological geometry?

#### Test 10.1 — Direct 3D Likelihood vs Camera Likelihood

**Files (names MUST match exactly):**

* `tests/diagnostics/v0_2_1_divergence/test_group_10_direct_3d.py`
* `tests/diagnostics/v0_2_1_divergence/results_group_10_direct_3d.json`
* `tests/diagnostics/v0_2_1_divergence/report_group_10_direct_3d.md`

**Ground-truth attribute name (avoid guessing):**

* The synthetic data returned by `get_standard_synth_data` stores ground-truth joint positions as:

  * `x_true = synth_data["joint_positions"]` with shape `(T, K, 3)`.

**tau_3d choice (avoid open-ended tuning):**

* Use a single default value:

  * `tau_3d = 0.02` meters (2 cm)
* Do not sweep `tau_3d` in this test unless it becomes necessary later. This test is intended to be a blunt hammer.

**Shape expectation:**

* `x_all` and `x_true` must both have shape `(T, K, 3)`.

**Model variants:**

1. **Camera-likelihood baseline:**

   * Use the existing Stage 2 model via `build_test_model(..., use_directional_hmm=False, ...)` from `test_utils.py`.

2. **Direct-3D variant:**

   * Use the same synthetic data and initialization as the baseline.
   * Rebuild the model so that the skeleton hierarchy (root RW, directions, lengths) is identical to the baseline.
   * Replace the camera likelihood with a direct 3D Gaussian likelihood on ground-truth positions:

     ```python
     x_true = synth_data["joint_positions"]
     tau_3d = 0.02
     pm.Normal(
         "x_obs",
         mu=x_all,
         sigma=tau_3d,
         observed=x_true,
     )
     ```

**Runner integration:**

* Define a function `run_group_10_direct_3d()` in `test_group_10_direct_3d.py` that:

  * Runs both variants with the same sampler settings,
  * Records divergence statistics and tuned step sizes,
  * Writes `results_group_10_direct_3d.json` and any auxiliary diagnostics needed for the markdown report.
* Call `run_group_10_direct_3d()` from `test_runner.py` as a new **Test Group 10** entry.

**Expected metrics:**

* Divergence rate and step size for:

  * Full camera model.
  * Direct-3D model.

**Key outcome (interpretation):**

* If the direct-3D model shows healthy sampling (few divergences, reasonable step size) while the camera model remains problematic, this strongly suggests that camera conditioning (Issue #2) is a major contributor.
* If both behave poorly, then the skeleton hierarchy alone is already problematic, and Issue #1 / Issue #3 need more attention.

#### Optional Test 10.2 — Inflated obs_sigma

* A separate script (e.g. `test_group_10_likelihood_scale.py`) may be created to test inflated `obs_sigma` values by copying the camera-likelihood model and multiplying `obs_sigma` by a factor (3×, 5×, 10×).
* This script must:

  * Live in the same diagnostics folder,
  * Use explicit filenames for results and report,
  * Refrain from altering `pymc_model` itself.

---

**

* If direct-3D sampling behaves well while camera-likelihood sampling does not, then camera conditioning (Issue #2) is a major contributor.

#### Optional Test 10.2 — Inflated obs_sigma

* Implement a simple variant where `obs_sigma` in the camera likelihood is multiplied by a factor (e.g., 3×, 5×, 10×) in a locally-copied model build.
* Compare divergence rates and step sizes.
* This test can live in the same file or a separate `test_group_10_likelihood_scale.py`, but it should *not* introduce new flags into `gimbal.pymc_model`.

---

### Issue #3: Parameter Redundancy — Test Group 11

#### Implementation Checklist Header (copy verbatim into the test file)

* [ ] File path is exactly: `tests/diagnostics/v0_2_1_divergence/test_group_11_redundancy_fixed.py` (Test 11.1) and `test_group_11_redundancy_priors.py` (Test 11.2)
* [ ] Results file paths are exactly: `results_group_11_redundancy_fixed.json` and `results_group_11_redundancy_priors.json`
* [ ] Report file paths are exactly: `report_group_11_redundancy_fixed.md` and `report_group_11_redundancy_priors.md`
* [ ] Uses `test_utils.get_standard_synth_data(T=100, C=3, S=3, seed=42)`
* [ ] Uses the same DLT initialization path as baseline
* [ ] Sampler via `test_utils.sample_model(model, draws=200, tune=200, chains=1)` with identical settings in A/B
* [ ] Baseline variant uses `test_utils.build_test_model(..., use_directional_hmm=False, ...)`
* [ ] Test 11.1 replaces directions/lengths with `pm.Data` using GT-derived `u_true` and `lengths_true`
* [ ] Test 11.2 uses explicit strong GT-based priors with a single documented default tightness (no ad-hoc tuning)
* [ ] No changes to `gimbal/pymc_model.py` or other core modules
* [ ] Writes JSON metrics and a short markdown report; does not claim the issue is "solved"

**Question:** Do redundant degrees of freedom (root motion vs directions vs bone lengths) significantly worsen posterior geometry?

#### Test 11.1 — Fix Directions and Lengths to Ground Truth

**Files (names MUST match exactly):**

* `tests/diagnostics/v0_2_1_divergence/test_group_11_redundancy_fixed.py`
* `tests/diagnostics/v0_2_1_divergence/results_group_11_redundancy_fixed.json`
* `tests/diagnostics/v0_2_1_divergence/report_group_11_redundancy_fixed.md`

**How to compute u_true (avoid ambiguity):**

* Compute directions from ground-truth joint positions and parents:

  * `x_true = synth_data["joint_positions"]` with shape `(T, K, 3)`
  * `parents = synth_data["parents"]`
* Define (for non-root joints):

  ```python
  u_true = np.zeros_like(x_true)
  for k in range(K):
      p = parents[k]
      if p < 0:
          continue
      v = x_true[:, k, :] - x_true[:, p, :]
      u_true[:, k, :] = v / (np.linalg.norm(v, axis=-1, keepdims=True) + 1e-12)
  ```
* Confirm the expected shape used by the model builder (typically `(T, K, 3)` for per-frame directions).

**Bone lengths ground truth:**

* Use `lengths_true = synth_data["bone_lengths"]` (static reference lengths from the skeleton used to generate the sequence).

**Model variant:**

* Keep the root RW and camera likelihood identical to the baseline.
* Replace stochastic directions and lengths with fixed `pm.Data` nodes:

  ```python
  u_all = pm.Data("u_all", u_true)
  lengths = pm.Data("lengths", lengths_true)
  ```
* Use these to build `x_all` deterministically from `x_root`.

**Expected behavior:**

* A substantial reduction in divergences and/or an increase in tuned step size relative to the baseline is strong evidence that redundancy in directions/lengths contributes to geometric pathologies.

#### Test 11.2 — Strong Data-Driven Priors (GT-Based)

**Files (names MUST match exactly):**

* `tests/diagnostics/v0_2_1_divergence/test_group_11_redundancy_priors.py`
* `tests/diagnostics/v0_2_1_divergence/results_group_11_redundancy_priors.json`
* `tests/diagnostics/v0_2_1_divergence/report_group_11_redundancy_priors.md`

**Model variant:**

* Start from the same baseline as other groups (root RW, directions, lengths, camera likelihood).
* Instead of fixing directions/lengths, enforce strong priors centered on ground truth.

**Lengths (explicit, reproducible):**

* Use `gimbal.prior_building.get_gamma_shape_rate(mode, sd)`.
* Choose:

  * `mode = lengths_true[k]`
  * `sd = 0.02 * lengths_true[k]` (2% relative SD) as a default "strong" prior.
* Use the resulting `(shape, rate)` in `pm.Gamma`.

**Directions (explicit default tightness):**

* The objective is to constrain directions to be within roughly **~5 degrees** of ground truth most of the time.
* Use one of the following approaches depending on what the existing direction prior machinery supports:

  1. **vMF/von Mises-like concentration:** pick a single default concentration parameter (document it in the test file), e.g. `kappa_dir = 200` as a starting point for a tight prior.
  2. **Gaussian perturbation in tangent space:** if direction priors are built via unconstrained raw vectors normalized to unit length, place a tight Normal prior on the raw vector around the GT raw vector with `sd_raw = 0.05` (and document this value).

The key requirement is not the exact distribution; it is that the test uses a **single explicit default** and documents it, rather than leaving concentration to ad-hoc tuning.

**Key comparisons:**

* Divergence rates and step sizes across three configurations:

  1. Baseline (weak priors on directions/lengths).
  2. Strong GT-based priors (Test 11.2).
  3. Fully fixed directions/lengths (Test 11.1).

**Interpretation:**

* A monotonic improvement from (1) → (2) → (3) (fewer divergences, larger step sizes, better energy diagnostics) is strong evidence that parameter redundancy (Issue #3) is amplifying the geometry problems.
* If no clear pattern emerges, then redundancy may be a secondary factor relative to Issues #1 and #2.

---

---

### Repository Structure and Cleanup Notes

* **Do not** add debug-specific flags or branches to `gimbal/pymc_model.py` for these diagnostics. All structural variants for Issues #1–#3 should be implemented as local model builders inside the new test files (or, if absolutely necessary, in `tests/diagnostics/v0_2_1_divergence/test_utils.py`).
* Keep ad-hoc scripts in `debug/` as *exploratory only*. Anything that becomes part of a systematic investigation (like these tests) should live under `tests/diagnostics/v0_2_1_divergence/` with the test/results/report pattern.
* Over time, consider:

  * Archiving older, less-informative diagnostic scripts by moving them to a `tests/diagnostics/v0_2_1_divergence/archive/` subdirectory once Issues #1–#3 are resolved.
  * Keeping the main `test_runner.py` focused on the most decision-relevant groups (baseline, HMM effect, state count, Issues #1–#3).

*(End of Document — continue extending here in future edits.)*
`