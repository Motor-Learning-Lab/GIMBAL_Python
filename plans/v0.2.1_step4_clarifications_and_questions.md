# v0.2.1 Step 4: Clarifications and Questions

This document tracks questions, decisions, and implementation progress for Step 4 (fitting toolchain validation on L00_minimal).

---

## [CURRENT] Questions and Clarifications - December 19, 2025 (Pre-Implementation)

**Date:** December 19, 2025  
**Status:** ✅ Answers Received - Implementation In Progress

### Critical Path Questions (Must Resolve Before Implementation)

---

**Q1: Phase 4.0 (Pre-Flight Cleanup) - Is This Still Required?**

The spec mandates completing 4.0.A-D (Step 3 documentation consolidation, parameter semantics, archive obsolete code, artifact layout README) before any fitting work.

**Context:**
- Step 3 just completed comprehensive hardening (see v0.2.1_step3_completion_summary.md)
- Module consolidation done (9 micro-modules → 2)
- Artifact policy documented (tests/pipeline/datasets/README.md)
- Import hygiene cleaned up

**Apparent redundancy with completed work:**
- 4.0.A: Already done - v0.2.1_step3_completion_summary.md is the authoritative reference
- 4.0.D: Already done - tests/pipeline/datasets/README.md documents artifacts

**Question:** Should we:
- **Option A:** Skip 4.0 entirely - Step 3 hardening already covered these requirements
- **Option B:** Do minimal 4.0.B (parameter semantics) and 4.0.C (obsolete code check), skip A and D
- **Option C:** Treat 4.0 as a formal review checkpoint - verify completeness and add any gaps

**Recommendation:** Option B - Quick pass for parameter semantics and obsolete code, then proceed to fitting

> Opher: We have rewritten Step 4.0. Let me know if it feels elss redundant now.

---

**Q2: L00 Scope - What Does "1-State HMM" Mean Operationally?**

The spec says L00 uses `num_states = 1` and calls this "both a valid scientific outcome and a practical way to de-risk the early pipeline."

**Clarification needed:**
1. **HMM behavior with K=1:**
   - Transition matrix is [[1.0]] (deterministic self-loop)
   - State emission is a single set of canonical directions
   - Is this mathematically equivalent to "no HMM" (fixed regime throughout)?

> Opher: Yes. Still, it may be easier to cover the degenerate case as part of the general case. This is what it is conceptually.

2. **Stage 3 invocation:**
   - Do we still call `add_directional_hmm_prior()` with `S=1`?
   - Or do we skip Stage 3 entirely for L00?
   - Or is there a `use_hmm: bool` flag that bypasses discrete state sampling?

> Opher: I think that we should write all of our code so that it handles 1 state HMMs gracefully. That way, from the point of view of the code, a 1 state HMM is just like any other HMM. This will take some work for treating this border case, but it will make interfacing with the project much easier later.

3. **Ground truth implications:**
   - L00 synthetic data was generated with `num_states=1` in config
   - Does `z_true` array contain all zeros, or is it not generated at all?

> Opher: I would say an array containing all zeros.

   - What should "state accuracy" metric (Stage J) report for K=1 case?

> Opher: I would say that it is like the state was accurately predicted in every time step.

**Question:** Please clarify the intended behavior. My understanding is:
- K=1 HMM still has Stage 3 prior but with trivial dynamics
- This tests that the directional prior works in the simplest case
- State accuracy test should verify z_pred is constant (all same state)

**Is this correct?**

> Opher: Yes.

---

**Q3: Fixed vs Estimated Parameters - Existing API Support**

The spec requires ability to fix cameras and bone lengths (4.3.2: "disable estimation by default").

**Current API check needed:**
- Does `build_camera_observation_model()` have parameters to disable camera/length estimation?
- Or do we need to add `estimate_cameras: bool`, `estimate_lengths: bool` flags?
- What's the current behavior - are cameras/lengths always estimated?

**Question:** Should I:
1. **First:** Audit current API to see what's already supported
2. **Then:** Implement minimal flags if needed
3. **Or:** Assume we need new parameters and add them?

**Recommendation:** Let me read the current API first and report what exists before adding new parameters.

> Opher: Yes. You can also go ahead and add only what's missing and document the work.

---

**Q4: Stages B and D - Cleaning on Synthetic Data?**

The spec includes optional stages for 2D and 3D cleaning:
- Stage B: 2D cleaning (outliers/missingness handling)
- Stage D: 3D smoothing/cleaning

**Context:**
- L00_minimal is "clean baseline" - no noise, outliers, or missingness (by design)
- Cleaning tools exist (gimbal.data_cleaning) but are for **real** noisy data
- Synthetic data from Step 3 is already "ground truth clean"

**Question:** For L00 specifically, should these stages:
- **Option A:** Skip entirely (document as "not applicable to clean synthetic baseline")
- **Option B:** Run as no-ops (pass-through) to test the stage exists
- **Option C:** Still apply (e.g., light smoothing) to exercise the code paths

**Recommendation:** Option A for L00, but ensure code structure supports enabling them for L01-L03 later

> Opher: Option C is best. It's important that our smoothing doesn't ruin already smooth data.

---

**Q5: Stage G - "Minimal" Model Build - What's Included?**

The spec says "Build PyMC model (minimal)" but doesn't specify which stages to include.

**Clarification needed:**
- **Stages 1+2 only:** Collapsed HMM + Camera observation (no directional prior)?
- **Stages 1+2+3:** Full three-stage pipeline with K=1 HMM?
- **Something else:** Just camera observation without HMM at all?

**Related to Q2:** If K=1 HMM is "trivial but must not break," I assume we include all three stages.

**Question:** Confirm that "minimal" means:
- All three stages (1: HMM, 2: Camera, 3: Directional)
- With `num_states=1`
- With fixed cameras and bone lengths
- This tests the full pipeline in simplest configuration

**Is this correct?**

> Opher:
> **Clarification of “Minimal” Model in Stage G**
>
> In Step 4, “minimal model” does **not** mean a reduced or partial pipeline. It means the *simplest scientifically valid instantiation of the full intended model*, used to verify that the entire modeling stack builds, samples, and produces sensible outputs without geometric, probabilistic, or software breakage.
>
> Concretely, the minimal model **includes all three conceptual stages** of the pipeline:
>
> 1. Stage 1: HMM / latent state machinery
> 2. Stage 2: Camera observation model
> 3. Stage 3: Directional / skeletal structure
>
> These stages are instantiated in their simplest non-degenerate form:
>
> * `num_states = 1` (a 1×1 transition matrix)
> * Cameras are present but **fixed** (no camera estimation)
> * Bone lengths are present but **fixed**
> * Directional structure is active (because it defines the geometry of the skeleton)
>
> The HMM is therefore *trivial but real*: it must not break, special-case, or bypass logic simply because there is only one state. A single-state explanation is a valid scientific outcome for some datasets, not a pathological corner case.
>
> The minimal model explicitly **does not** include:
>
> * Multi-state regime inference
> * Camera parameter estimation
> * Bone-length estimation
> * Advanced identifiability tricks or strong priors beyond what is already required
>
> Fixing cameras and lengths isolates core pipeline correctness from identifiability and geometry confounds. Failures at this stage should reflect bugs in model construction, wiring, or numerical stability—not deep modeling questions.
>
> In short: **“minimal” means the full three-stage model, with `num_states = 1` and all estimable geometry fixed**, used to validate end-to-end construction and sampling in the simplest scientifically meaningful case.


---

**Q6: Stage H - Sampling Strategy and Duration**

The spec says "Fit model (short run)" with divergences/ESS/R-hat reporting.

**Clarification needed:**
1. **Sampler choice:**
   - Use nutpie (current default)?
   - Or is this the "sampler decision spike" from v0.2 Phase 0.2.3?
   - Or just use whatever works for now, formal evaluation later?

> Because we've done some work with initial conditions, it seems we are limited to PyMC right now. However, I think we should try to go back to nutpie. I do think that's probably later in 0.2.3.

2. **"Short run" parameters:**
   - What's a reasonable short run? (e.g., 100 samples, 2 chains?)
   - Goal is "sampling completes" not "well-mixed posterior"
   - Is initialization required (fit_params.initialize_from_observations)?

> Opher: 200 samples with 2 chains seems like a good place to start. We are trying to determine quickly if there are any major sampling issues (divergences, strong bimodality, instability) that prevent the sampler from doing its job on a long run.

3. **Failure tolerance:**
   - If sampling diverges or fails to converge, is that a blocker?
   - Or do we just document it and proceed to Stage I/J?

> Opher: In general, it's best to proceed with stages that you can do and document the progress and results. Not getting blocked is a virtue as long as nothing irreversible happens.

**Question:** Provide guidance on:
- Sampler to use (nutpie unless you prefer otherwise)
- Target samples and chains for "short run"
- Whether initialization is required
- Acceptable failure modes (divergences OK if documented?)

---

**Q7: Stage J - Ground Truth Comparison - What to Measure?**

The spec lists:
- 3D RMSE per joint
- Direction angular error
- State accuracy (for K=1, this is trivial)

**Clarification needed:**
1. **Which parameters are identifiable in L00?**
   - Joint positions (x) - yes, from observations
   - Joint directions (U) - yes, from directional prior
   - Bone lengths - fixed (not estimated per 4.3.2)
   - Camera parameters - fixed (not estimated per 4.3.2)
   - State sequence (z) - trivial with K=1

2. **Comparison baseline:**
   - Compare to ground truth passed in explicitly (as spec says)
   - Or compare to L00 dataset's saved ground truth?
   - Or both (verify they match)?

3. **Success criteria:**
   - What RMSE threshold is "reasonable" for L00?
   - What angular error is acceptable?
   - Or is this exploratory (just document what we get)?

**Question:** 
- Confirm we compare estimated (x, U) to ground truth (x_true, u_true) from L00 dataset

> Opher: Yes.

- Should there be quantitative thresholds, or just qualitative "looks reasonable"?

> Opher: The advantage of quantitative thresholds is that you can use them to determine if there are major errors in the process of fitting. They don't need to be tight, but they should indicate if something has gone very wrong.

- If fit is poor (high RMSE), do we debug before proceeding or document and continue?

> Opher: I'm in favor of getting as far as you can non-interactively and then we will work through what has happened in interactive sessions to see how ot move forward. I think it's best if you don't stop as long as there is a way forward.

---

**Q8: 4.3.1 Inventory Table - Scope and Format**

The spec requires documenting current switches/options before adding new ones.

**Clarification needed:**
- Scope: Just `build_camera_observation_model()` and `add_directional_hmm_prior()`?
- Or include all gimbal functions used in pipeline?
- Format: The template shows columns (Option, Location, Meaning, Default, Used in Stage(s))

**Question:**
- Should inventory include:
  - PyMC model building functions only?
  - Data processing functions (triangulation, cleaning, etc.)?
  - Sampling/inference options?

> Opher: All three.

- Where should this table live?
  - In this clarifications doc?
  - As separate plans/v0.2.1_step4_api_inventory.md?
  - In the completion report?

**Recommendation:** Create plans/v0.2.1_step4_api_inventory.md with all gimbal functions used in Stages A-J

> Opher: Agreed.

---

**Q9: Deliverable Structure - Single Script or Modular?**

The spec says "A single integration test (or diagnostic script)" but also describes stages A-J as separate checkpoints.

**Clarification needed:**
- **Option A:** One monolithic script that runs all stages sequentially
- **Option B:** One runner script that calls separate stage functions (modular)
- **Option C:** Separate test scripts per stage (e.g., test_stage_c_triangulation.py)

**Related design question:**
- Should stages be functions in a module (e.g., `fit_toolchain.py`)?
- Or methods in a class (e.g., `FitPipeline.run_stage_a()`)?
- Or just a linear script with clear section markers?

**Question:** 
- Preferred structure for maintainability and reuse?
- Should this be in `tests/pipeline/` or new location?
- Naming convention: `test_v0_2_1_fit_L00.py` or `fit_toolchain_validation.py`?

**Recommendation:** Option B - One runner with modular stage functions, allows testing stages independently


> Opher: Agreed on option B. It can be in `tests/pipeline` for now. Since it is in `test/pipeline` let's call it `v0.2.1_toolchain_switches_inventory.md`

---

**Q10: Output Directory Structure**

The spec mentions "run-scoped output directory" with intermediate artifacts, plots, and fit_report.md.

**Clarification needed:**
- Location: `tests/pipeline/fit_runs/` or `results/v0_2_1_L00_fit/`?
- Naming: Timestamped runs vs single canonical "latest" run?
- Versioning: How to track multiple fitting attempts with different parameters?

**Question:**
- Should output structure mirror Step 3 datasets?
  - `tests/pipeline/fit_runs/v0.2.1_L00_fit/`
  - With subdirectories: `stage_a_load/`, `stage_c_triangulation/`, etc.?
- Or flatter structure with prefixed files: `stage_a_validation.png`, `stage_c_trajectories.png`?

**Recommendation:** 
```
tests/pipeline/fit_runs/v0.2.1_L00_fit/
├── config.json (fitting options used)
├── stage_a_load/
│   ├── validation.txt
│   └── 2d_scatter.png
├── stage_c_triangulation/
│   ├── metrics.json
│   └── trajectories_3d.png
├── ...
└── fit_report.md (summary)
```
> Opher: `tests\pipeline\fits\v0.2.1_L00_minimal\`
>
> with files:
> `config.json`
> `load_validation.txt`
> `triangulation_metrics.json`
> `fit_report.md`
>
> And a directory `figures` with files such as `load_2d_scatter.png`, `triangulation_trajectories_3d.png`, etc.

---

### Implementation Sequencing Questions

**Q11: Should We Implement Stages Linearly or Build Infrastructure First?**

Two approaches:
- **Linear:** Implement Stage A, test it, then Stage B, etc. (slow but steady)
- **Infrastructure-first:** Build all stage functions as stubs, then fill in logic (see full picture)

**Question:** Your preference for Step 4? 

**Recommendation:** Infrastructure-first - create stage functions A-J as documented stubs with "TODO" markers, then implement in order

> Opher: Linear. Build stages and test them before moving on. 

---

**Q12: Relationship to v0.2 Phase Numbering**

The spec title says "v0.2.1 Step-1" but describes Step 4. The v0.2-overview.md has phases 0.2.1-0.2.8.

**Clarification needed:**
- Is this document actually "v0.2.1 Step 4" (fixing title typo)?
- How does "Step 4" relate to v0.2 phases?
  - Phase 0.2.1 was data-driven priors (complete)
  - Phase 0.2.4 is synthetic diagnostics
  - Is this Step 4 = Phase 0.2.4?

**Question:** Confirm naming convention - should I refer to this as "Step 4" or "Phase 0.2.4" in code/docs?

> Opher: We are in v0.2.1 Step 4. The steps of v0.2.1 appear on lines 6-11 of v0.2.1_step4_fit_L00.md as well as other places.

---

## Questions Summary for User Review

**Critical (block implementation):**
1. Q1: Skip/minimize 4.0 cleanup phase? (Recommendation: Skip A and D, quick pass B and C)
2. Q2: K=1 HMM operational behavior (still use Stage 3 with S=1?)
3. Q5: "Minimal model" = all 3 stages with K=1?
4. Q6: Sampler choice and "short run" parameters
5. Q7: Ground truth comparison success criteria

**Important (affects design):**
6. Q3: Check existing API for fixed parameters support (do audit first)
7. Q9: Single script vs modular stage functions (Recommendation: modular)
8. Q10: Output directory structure (Recommendation: hierarchical with stage subdirs)

**Clarification (for completeness):**
9. Q4: Skip cleaning stages B/D for L00? (Recommendation: yes)
10. Q8: API inventory scope and location
11. Q11: Linear vs infrastructure-first implementation (Recommendation: infrastructure-first)
12. Q12: Naming - "Step 4" vs "Phase 0.2.4"?

---

## Document Status

- [x] User has reviewed questions
- [x] Critical questions answered
- [x] Implementation can proceed
- [x] Design decisions documented

---

## Implementation Progress - December 19, 2025

### Phase 1: Setup and API Inventory (In Progress)

**Current Task:** Creating API inventory and Stage 4.0.B parameter semantics
