# v0.2.1 Step 3 Plan: Synthetic Data Generation

**Status:** Draft for Copilot implementation  
**Date:** December 17, 2025  
**Scope:** This document defines the synthetic data generation infrastructure for v0.2.1, focusing on:

1. **Continuous motion generator** using second-order attractor dynamics (replaces simple kappa-based model)
2. **Config-driven data generation** with JSON schema and four canonical datasets (L00-L03)
3. **Multi-camera observation model** with identifiability checks and quality metrics
4. **Dataset artifact format** (.npz) with ground truth, observations, and metadata
5. **Visualization and metrics** for human-in-the-loop validation
6. **Integration tests** validating generated datasets against thresholds

**Note:** Items related to alignment, fitting toolchain, end-to-end runner, and camera priors are covered in separate Step documents (see v0.2.1_cleanup_plan.md for roadmap).

v0.2.1 Step 3 should be considered complete once:

* **Continuous motion generator implemented:** Second-order attractor dynamics with state-dependent parameters (mu, omega, zeta, sigma_process)
* **Config-driven generation working:** JSON schema defined, can load configs and generate datasets
* **Four canonical datasets exist:** L00 (minimal), L01 (noise), L02 (outliers), L03 (missingness) saved to disk with .npz + figures + metrics
* **Camera quality validation:** Identifiability checks implemented and passing for L00-L03
* **Integration tests passing:** pytest validates bone lengths, smoothness, 2D sanity checks with conservative thresholds
* **Human validation completed:** User (Opher) has reviewed figures and confirmed datasets are reasonable

---

## 1. Synthetic Data Generation Overview

**Goal:** Build and validate a **config-driven synthetic data generator** that produces continuous ground truth motion, explicit-camera 2D observations, and a self-contained dataset artifact. Then generate **four canonical datasets** (L00–L03) of increasing complexity and save figures that make the simulated data easy to inspect.

**Scope boundary:** Step 3 stops after data generation + visualization. No inference, no fitting, no HMM sampling diagnostics here.

**Implementation status:** Current `gimbal/synthetic_data.py` provides basic functionality. This plan specifies major enhancements:
- Replace simple directional noise with continuous second-order dynamics
- Add JSON config system
- Add camera quality validation
- Add comprehensive metrics and visualization
- Create canonical reference datasets for testing

---

### 1.1 Definition of Done

Step 3 is complete only when **both** (A) machine checks pass and (B) you (Opher) have visually verified that the figures are reasonable.

**A. Machine-verifiable acceptance (required):**

1. A single config file can generate:

   * continuous ground-truth motion
   * explicit-camera 2D observations
   * a self-contained dataset file
2. Four canonical configs (L00–L03) exist and can be generated without code modification.
3. Each dataset has:

   * a corresponding **standard figure set** saved to disk
   * a corresponding **metrics.json** saved to disk (see 3.8)
4. A pytest integration test validates the generator outputs for L00–L03 using conservative thresholds (see 3.8).

**B. Human-in-the-loop acceptance (required):**

5. Opher reviews the saved figures for L00–L03 and records a short approval note in `results/` (or the run directory) before Step 3 is declared closed.

---

### 3.2 Config Schema (Generator-Only)

**Format choice (decision):** Use **JSON** for synthetic dataset configs in v0.2.1.

Rationale: the repo has already used JSON consistently for run artifacts/config-like payloads; JSON avoids YAML parsing edge cases and keeps configs trivially serializable into `.npz` metadata.

Implement one concrete JSON schema sufficient for data generation.

Required top-level keys:

* `meta`: `{ "name", "version", "seed", "dt", "T" }`
* `dataset_spec`: (see below)
* `output`: `{ "run_name" }`

**Critical separation:** `dataset_spec` describes *data to generate*. Tooling controls (Step 4/5) live under `tool_options` in later steps and must not be required for generation.

`dataset_spec` required keys:

* `skeleton`: `{ "parents", "joint_names", "lengths", "bone_pairs" }`
* `states`: `{ "num_states", "transition_matrix" }`
* `motion`: `{ "per_state_params", "root_params", "init_pose" }`
* `cameras`: `{ "cameras": [ {"name","K","R","t","image_size"} ... ] }`
* `camera_quality`: `{ "min_cameras_per_point", "min_pairwise_ray_angle_deg", "min_fraction_points" }`
* `observation`: `{ "noise_px", "outliers", "missingness" }`

A commented template must be written to:

```
tests/pipeline/configs/v0.2.1/_template.json
```

---

### 3.3 Continuous Motion Generator (Second-Order Attractor)

Implement the second-order attractor system:

* Latent pose position `q_t` and velocity `v_t`
* State-dependent parameters `(mu_k, omega_k, zeta_k, sigma_process_k)`

Discrete-time update (dt from config):

```
a_t = -omega_k^2 * (q_t - mu_k)
      -2 * zeta_k * omega_k * v_t
      + Normal(0, sigma_process_k)

v_{t+1} = v_t + a_t * dt
q_{t+1} = q_t + v_{t+1} * dt
```

Representation constraints:

* `q_t` is a concatenation of raw 3D **bone direction vectors** (one 3-vector per bone)
* Normalize per bone before FK
* Bone lengths come directly from config

Root translation uses the same model (separate parameters under `motion.root_params`).

Determinism: all randomness must be seeded by `meta.seed`.

#### Initialization (decision)

Initialize the motion at the attractor of state 0:

* `q_0 = mu_{z0}` where `z0 = 0` (or the first state in the config)
* `v_0 = 0`
* root initialized to `root_mu_{z0}`

This reduces transient artifacts and makes early frames interpretable.

#### Process noise and state variability (make it explicit)

The phrase "small" is not sufficient. The generator must explicitly distinguish:

1. **Within-state pose dispersion** (how far `q_t` wanders around `mu_k` when staying in state k)
2. **Temporal smoothness** (how rapidly `q_t` changes frame-to-frame)

Config must therefore specify per state:

* `mu_k` (attractor)
* `sigma_pose_k` (target within-state dispersion)
* `(omega_k, zeta_k)` (smoothness / time scale)

And the generator must choose/tune `sigma_process_k` so that the realized within-state dispersion approximately matches `sigma_pose_k`.

Implementation guidance (acceptable heuristic):

* Start with a provisional `sigma_process_k`
* Simulate a short trajectory staying in state k
* Measure realized dispersion of `q_t - mu_k`
* Scale `sigma_process_k` up/down to match `sigma_pose_k` within tolerance

This avoids requiring a closed-form stationary variance derivation.

#### Parameter selection guidance (non-hardcoded)

The template config must include guidance, not strict numbers.

* Use `zeta ≈ 0.7` for smooth, non-oscillatory behavior
* Choose a time constant τ (e.g., 0.25–0.5 s) and set `omega ≈ 1/τ` as a starting point
* Set `sigma_pose_k` in interpretable units:

  * either as an angular spread (degrees) around direction vectors, or
  * as a raw-vector spread that is converted to angular spread during metrics

How to tell if motion is wrong (use saved metrics/figures):

* Too jittery: high 95th-percentile jerk; large frame-to-frame direction changes; 2D projections look noisy even before adding pixel noise
* Too static: very low speeds/accelerations; directions barely change; trajectories look nearly constant
* Rings/oscillates: visible periodic overshoot around pose; oscillatory acceleration sign changes; increased power at a narrow frequency band

The generator must save these quantities (3.8) so tuning is evidence-based.

---

### 3.4 Observation Model (2D)

Given `x_true` and explicit cameras:

1. Project to 2D using a **pinhole camera model** (distortion off by default)
2. Add Gaussian pixel noise
3. Inject outliers if configured
4. Apply missingness if configured (set to NaN)

Occlusion realism is explicitly out of scope.

#### Camera parameter specification guidance (must be included)

The config must specify per camera:

* `image_size`: `[width_px, height_px]`
* Intrinsics `K` (3×3)
* Extrinsics: `R` (3×3) and `t` (3,)

Recommended parameterization path (agent may implement helpers):

* Choose image size (e.g., 1920×1080 or 1280×720)
* Choose a nominal horizontal FOV (e.g., 60–90°)
* Compute:

  * `fx = (width/2) / tan(FOV/2)`
  * `fy` either equal to `fx` or computed from vertical FOV
  * `cx = width/2`, `cy = height/2`

Extrinsics guidance:

* Use explicit camera placements on a rough arc or ring around the subject
* Ensure cameras are not co-linear and not all in a plane that makes depth ambiguous

#### Identifiability check (must be enforced by generator)

We do **not** hard-require a fixed number of cameras in the schema. Instead, we require that the chosen camera set provides multi-view geometry that supports triangulation.

The generator must compute a camera-quality metric based on rays:

* For each time t and joint j, consider cameras with non-missing 2D observations
* Compute unit rays from each camera center to the 3D point `x_true[t,j]`
* Let θ be the pairwise angle between rays

Requirement (configurable under `dataset_spec.camera_quality`):

* At least `min_cameras_per_point` cameras available (default 3)
* Among those, the **minimum pairwise ray angle** must be ≥ `min_pairwise_ray_angle_deg` (suggest 10–20°)
* This must hold for at least `min_fraction_points` of (t,j) points (suggest ≥0.95 for L00)

If the requirement fails, the generator must either:

* raise an error, or
* resample/reposition cameras (only if explicitly enabled)

For initial datasets, keep distortion off and keep cameras in a stable, non-degenerate configuration.

---

### 3.5 Dataset Artifact Format

Write one self-contained dataset file per config (suggested: `.npz`). Required contents:

* Truth: `x_true`, `u_true`, `z_true`, `A_true` (TODO: clarify if A_true is acceleration from second-order system)
* Observations: `y_2d`
* Skeleton metadata: `parents`, `joint_names`, `lengths`
* Camera params: per-camera `K`, `R`, `t`, `image_size`
* Serialized config text + config hash

---

### 3.6 Canonical Datasets (L00–L03)

Create four configs under:

```
tests/pipeline/configs/v0.2.1/
```

We need two axes of difficulty:

* **Data quality** (noise/outliers/missingness)
* **Model complexity** (skeleton size/hierarchy, number of states, motion richness)

For v0.2.1, prioritize **data quality first** because it is faster to validate and it directly stress-tests projection/triangulation/cleaning and the inlier/outlier logic. Motion/skeleton complexity can be expanded in v0.2.2.

Accordingly:

* Keep skeleton and motion generation *fixed and minimal* across L00–L03.
* Increase observation corruption across L00–L03.

**L00_minimal.json** (baseline reference dataset)

* `num_states = 1` (1×1 transition matrix)
* Minimal skeleton/hierarchy (see 3.6.1)
* Low noise, no outliers, no missingness
* Camera-quality requirements strict (high `min_fraction_points`)

**L01_noise.json**

* Same as L00 but increased `noise_px`

**L02_outliers.json**

* Same as L00 but add outliers (missingness off)

**L03_missingness.json**

* Same as L00 but add missingness (Bernoulli)

Each config must produce a dataset file, figures, and metrics.

#### 3.6.1 Skeleton and motion minimality (must be specified)

L00–L03 must explicitly specify:

* number of joints J
* parents array length J
* bone list used for directions (`bone_pairs`)

Guidance for v0.2.1:

* Start with a small but non-trivial tree (e.g., a root with 2–3 chains) rather than a pure chain.
* Avoid extreme branching or very long chains in v0.2.1.

**Implementation note:** Use existing `DEMO_V0_1_SKELETON` from `gimbal/skeleton_config.py` (7 joints, simple tree) for consistency with existing tests. Config files must still serialize all skeleton parameters explicitly so datasets are self-contained.

---

---

### 3.7 Standard Figure Set (Must Save)

For each dataset, save a consistent set of plots under `figures/`:

1. **3D skeleton motion preview**: trajectories of selected joints (root + 2–4 extremities)
2. **3D pose snapshots**: a small grid of frames showing skeleton pose in 3D
3. **2D reprojection montage**: one camera, a few frames with keypoints overlaid
4. **Missingness/outlier summary**: fraction missing/outlier per camera/joint
5. **State timeline**: true state sequence over time (and transition matrix image)

Purpose: rapid human inspection and quick detection of pathological motion/camera setups.

---

### 3.8 Machine-Verifiable Metrics + Generator Tests

In addition to figures, each generated dataset must save `metrics.json` containing:

* **Bone-length consistency:** max/mean deviation of implied bone lengths from config lengths
* **Direction normalization health:** distribution of raw direction norms before normalization; count of near-zero norms
* **Smoothness metrics:** distributions (mean/95th pct) of per-joint:

  * speed
  * acceleration
  * jerk (finite differences)
* **State sanity:** dwell times per state; transition counts; check `num_states=1` behaves as expected for L00
* **2D sanity:** fraction of NaNs (missingness), fraction of outliers injected, pixel coordinate bounds violations

A pytest integration test must run the generator for L00–L03 and assert conservative conditions, e.g.:

* Bone length deviation near zero (numerical tolerance)
* No NaNs in truth arrays
* 2D coordinates mostly within image bounds for inliers
* Missingness/outlier fractions within a small tolerance of config targets
* Acceleration/jerk percentiles not extreme relative to dt and pose scale (agent to calibrate using L00 then hold constant)

**Note:** thresholds should be defined in *relative* terms where possible (e.g., jerk relative to typical speed), so they generalize across skeleton scales.

### 3.9 Deliverables

* Generator functions (pure functions; no hard-wired paths)
* A generator runner that:

  * loads config
  * writes dataset + figures + `metrics.json`
  * writes a short `dataset_report.md`
* Four configs L00–L03
* Pytest `test_v0_2_1_synth_generator.py` validating L00–L03
