# v0.2.1 Step 3 Plan: Synthetic Data Generation

**Status:** Implementation Complete - All Features Implemented  
**Date:** December 17-18, 2025  
**Last Updated:** December 18, 2025 7:30 PM

**Scope:** This document defines the synthetic data generation infrastructure for v0.2.1, focusing on:

1. **Continuous motion generator** using second-order attractor dynamics (replaces simple kappa-based model)
2. **Config-driven data generation** with JSON schema and four canonical datasets (L00-L03)
3. **Multi-camera observation model** with identifiability checks and quality metrics
4. **Dataset artifact format** (.npz) with ground truth, observations, and metadata
5. **Visualization and metrics** for human-in-the-loop validation
6. **Integration tests** validating generated datasets against thresholds

### Implementation Accomplishments (Dec 18, 2025)

**Core Infrastructure (✅ Complete):**
- L00_SKELETON created: 4-joint Y-structure (root → proximal → {distal_left, distal_right})
- SyntheticDataConfig refactored: NamedTuple → dataclass with from_json() method
- Second-order attractor motion generator implemented (generate_skeletal_motion_continuous in gimbal/synthetic_data.py)
- JSON config schema defined with comprehensive template
- Directory structure: tests/pipeline/datasets/v0.2.1_L00_minimal/ (config.json in dataset dir)
- **Camera placement helper (Dec 18, 6pm):** `camera_from_placement()` added to gimbal/camera_utils.py - converts intuitive placement (position, target, FOV) to camera parameters (K, R, t)

**Metrics & Visualization (✅ Complete - Dec 18, 7pm):**
- Moved to gimbal/ with focused, reusable functions
- Metrics modules: metrics_bone_length, metrics_direction, metrics_smoothness, metrics_state, metrics_observation
- Visualization modules: viz_motion_3d, viz_poses_3d, viz_reprojection_2d, viz_state_timeline
- **Camera visualization added:** 3D plots now show camera positions (purple pyramids), view directions (arrows), and labels
- Pipeline code wraps gimbal functions for backward compatibility

**Identifiability Checking (✅ Complete - Dec 18, 7:30pm):**
- New module: gimbal/identifiability.py with 3-tier system
- **Tier 1:** check_identifiability() - validation with 100-sample checking, returns pass/fail + diagnostics
- **Tier 2:** iteratively_adjust_cameras() - optimization using J_θ + J_d + J_F objective with L-BFGS-B
- **Tier 3:** auto_place_cameras() - two-ring placement (heights h/2 and 2h, radius 3w)
- All three tiers tested and working (mean min angle: 85°, 100% pass rate on L00_minimal)

**Canonical Datasets (✅ Complete - Regenerated Dec 18, 7pm):**
- L00-L03 configs created with T=1800 frames (30 sec @ 60fps)
- All use L00_SKELETON (simpler than DEMO_V0_1_SKELETON per user guidance)
- **Camera configuration fixed:** Cameras now positioned with 60° FOV at 3.5× skeleton radius (~35 units), properly framing skeleton
- K matrices: proper focal length (fx=fy=1108.5) computed from FOV, principal point at image center (640, 360)
- All 4 datasets regenerated: dataset.npz + metrics.json + 5 figures each
- 2D reprojections now in valid image bounds (108-968 pixels) - **visualization bug resolved**
- **Camera visualization:** All 3D plots now show camera positions and directions

**Quality Validation (✅ Metrics Confirm):**
- Bone length consistency: 0.000000 deviation across all datasets
- Direction normalization: 1.000000 mean norm (perfect)
- No bounds violations - projections within [0,1280] × [0,720]
- Jerk 95th percentile: ~13000-16000 (reasonable for skeleton/fps)
- 2D reprojection plots now show keypoints correctly
- **Identifiability:** 100% of points pass criteria (min angle 85°, 3 cameras)

**Second-Order Dynamics (✅ Already Implemented):**
- Motion generator uses position q_t, velocity v_t, acceleration a_t
- State-dependent parameters: ω (natural frequency), ζ (damping ratio), σ_process (noise)
- Attractor dynamics: a_t = -ω²(q_t - μ) - 2ζωv_t + noise
- All config files use second-order parameters (omega, zeta, sigma_pose)

v0.2.1 Step 3 **COMPLETE**:

* ✅ **Continuous motion generator implemented:** Second-order attractor dynamics working
* ✅ **Config-driven generation working:** JSON schema defined, loading and generating
* ✅ **Four canonical datasets exist:** L00-L03 saved with .npz + figures + metrics
* ✅ **Camera placement helper:** Implemented and used to fix camera configurations
* ✅ **Camera quality validation:** 3-tier identifiability checking implemented
* ✅ **Metrics & visualization refactored:** Moved to gimbal/ with focused functions
* ✅ **Integration tests passing:** All datasets generate successfully with valid metrics
* ⏳ **Human validation:** Awaiting user (Opher) review of regenerated figures

---

## 1. Synthetic Data Generation Overview

**Goal:** Build and validate a **config-driven synthetic data generator** that produces continuous ground truth motion, explicit-camera 2D observations, and a self-contained dataset artifact. Then generate **four canonical datasets** (L00–L03) of increasing complexity and save figures that make the simulated data easy to inspect.

**Scope boundary:** Step 3 stops after data generation + visualization. No inference, no fitting, no HMM sampling diagnostics here.

**Implementation status (Dec 18, 2025):**
- ✅ Second-order attractor implemented in generate_skeletal_motion_continuous()
- ✅ JSON config system working with from_json() in SyntheticDataConfig
- ✅ Camera quality: K matrices fixed, bounds violations resolved, identifiability checking implemented
- ✅ Comprehensive metrics and visualization: 5 figure types per dataset + camera visualization
- ✅ Canonical datasets created: L00-L03 all generated successfully
- ✅ Metrics & visualization moved to gimbal/ as focused, reusable functions

**Original plan enhancements:**
- Replace simple directional noise with continuous second-order dynamics ✅
- Add JSON config system ✅
- Add camera quality validation (identifiability checks) ✅
- Add comprehensive metrics and visualization ✅
- Create canonical reference datasets for testing ✅
- Move metrics/viz to gimbal/ for reusability ✅

---

### 1.1 Definition of Done

Step 3 is complete only when **both** (A) machine checks pass and (B) you (Opher) have visually verified that the figures are reasonable.

**A. Machine-verifiable acceptance (required):**

1. ✅ A single config file can generate continuous ground-truth motion, explicit-camera 2D observations, and self-contained dataset file
2. ✅ Four canonical configs (L00–L03) exist and can be generated without code modification
3. ✅ Each dataset has corresponding standard figure set and metrics.json saved to disk
4. ✅ All datasets pass quality checks (bone length, direction norms, bounds, identifiability)

**B. Human-in-the-loop acceptance (required):**

5. ⏳ Opher reviews the saved figures for L00–L03 and records approval note

**Current status:** Items 1-4 complete, item 5 awaiting user review.

---

### 3.2 Config Schema (Generator-Only)

**Implementation note (Dec 18):** ✅ Schema implemented and working

**Format choice (decision):** Use **JSON** for synthetic dataset configs in v0.2.1.

Rationale: the repo has already used JSON consistently for run artifacts/config-like payloads; JSON avoids YAML parsing edge cases and keeps configs trivially serializable into `.npz` metadata.

**Implemented structure:**
- Template location: `tests/pipeline/datasets/config_template.json` (158 lines with comprehensive docs)
- Config location: `tests/pipeline/datasets/v0.2.1_L00_minimal/config.json` (in dataset directory per user spec)
- All configs use comment stripping (keys starting with _ are ignored)

Implement one concrete JSON schema sufficient for data generation.

Required top-level keys:

* `meta`: `{ "name", "version", "seed", "dt", "T" }`
* `dataset_spec`: (see below)
* `output`: `{ "run_name" }`

**Critical separation:** `dataset_spec` describes *data to generate*. Tooling controls (Step 4/5) live under `tool_options` in later steps and must not be required for generation.

`dataset_spec` required keys:

* `skeleton`: `{ "parents", "joint_names", "lengths", "bone_pairs" }`
* `states`: `{ "num_states", "transition_matrix" }`
* `motion`: `{ "per_state_params", "root_params", "init_pose" }`
* `cameras`: `{ "cameras": [ {"name","K","R","t","image_size"} ... ] }`
* `camera_quality`: `{ "min_cameras_per_point", "min_pairwise_ray_angle_deg", "min_fraction_points" }`
* `observation`: `{ "noise_px", "outliers", "missingness" }`

A commented template must be written to:

```
tests/pipeline/configs/v0.2.1/_template.json
```

---

### 3.3 Continuous Motion Generator (Second-Order Attractor)

Implement the second-order attractor system:

* Latent pose position `q_t` and velocity `v_t`
* State-dependent parameters `(mu_k, omega_k, zeta_k, sigma_process_k)`

Discrete-time update (dt from config):

```
a_t = -omega_k^2 * (q_t - mu_k)
      -2 * zeta_k * omega_k * v_t
      + Normal(0, sigma_process_k)

v_{t+1} = v_t + a_t * dt
q_{t+1} = q_t + v_{t+1} * dt
```

Representation constraints:

* `q_t` is a concatenation of raw 3D **bone direction vectors** (one 3-vector per bone)
* Normalize per bone before FK
* Bone lengths come directly from config

Root translation uses the same model (separate parameters under `motion.root_params`).

Determinism: all randomness must be seeded by `meta.seed`.

#### Initialization (decision)

Initialize the motion at the attractor of state 0:

* `q_0 = mu_{z0}` where `z0 = 0` (or the first state in the config)
* `v_0 = 0`
* root initialized to `root_mu_{z0}`

This reduces transient artifacts and makes early frames interpretable.

#### Process noise and state variability (make it explicit)

The phrase "small" is not sufficient. The generator must explicitly distinguish:

1. **Within-state pose dispersion** (how far `q_t` wanders around `mu_k` when staying in state k)
2. **Temporal smoothness** (how rapidly `q_t` changes frame-to-frame)

Config must therefore specify per state:

* `mu_k` (attractor)
* `sigma_pose_k` (target within-state dispersion)
* `(omega_k, zeta_k)` (smoothness / time scale)

And the generator must choose/tune `sigma_process_k` so that the realized within-state dispersion approximately matches `sigma_pose_k`.

Implementation guidance (acceptable heuristic):

* Start with a provisional `sigma_process_k`
* Simulate a short trajectory staying in state k
* Measure realized dispersion of `q_t - mu_k`
* Scale `sigma_process_k` up/down to match `sigma_pose_k` within tolerance

This avoids requiring a closed-form stationary variance derivation.

#### Parameter selection guidance (non-hardcoded)

The template config must include guidance, not strict numbers.

* Use `zeta ≈ 0.7` for smooth, non-oscillatory behavior
* Choose a time constant τ (e.g., 0.25–0.5 s) and set `omega ≈ 1/τ` as a starting point
* Set `sigma_pose_k` in interpretable units:

  * either as an angular spread (degrees) around direction vectors, or
  * as a raw-vector spread that is converted to angular spread during metrics

How to tell if motion is wrong (use saved metrics/figures):

* Too jittery: high 95th-percentile jerk; large frame-to-frame direction changes; 2D projections look noisy even before adding pixel noise
* Too static: very low speeds/accelerations; directions barely change; trajectories look nearly constant
* Rings/oscillates: visible periodic overshoot around pose; oscillatory acceleration sign changes; increased power at a narrow frequency band

The generator must save these quantities (3.8) so tuning is evidence-based.

---

### 3.4 Observation Model (2D)

Given `x_true` and explicit cameras:

1. Project to 2D using a **pinhole camera model** (distortion off by default)
2. Add Gaussian pixel noise
3. Inject outliers if configured
4. Apply missingness if configured (set to NaN)

Occlusion realism is explicitly out of scope.

#### Camera parameter specification guidance (must be included)

The config must specify per camera:

* `image_size`: `[width_px, height_px]`
* Intrinsics `K` (3×3)
* Extrinsics: `R` (3×3) and `t` (3,)

Recommended parameterization path (agent may implement helpers):

* Choose image size (e.g., 1920×1080 or 1280×720)
* Choose a nominal horizontal FOV (e.g., 60–90°)
* Compute:

  * `fx = (width/2) / tan(FOV/2)`
  * `fy` either equal to `fx` or computed from vertical FOV
  * `cx = width/2`, `cy = height/2`

Extrinsics guidance:

* Use explicit camera placements on a rough arc or ring around the subject
* Ensure cameras are not co-linear and not all in a plane that makes depth ambiguous

#### Identifiability check (must be enforced by generator)

We do **not** hard-require a fixed number of cameras in the schema. Instead, we require that the chosen camera set provides multi-view geometry that supports triangulation.

The generator must compute a camera-quality metric based on rays:

* For each time t and joint j, consider cameras with non-missing 2D observations
* Compute unit rays from each camera center to the 3D point `x_true[t,j]`
* Let θ be the pairwise angle between rays

Requirement (configurable under `dataset_spec.camera_quality`):

* At least `min_cameras_per_point` cameras available (default 3)
* Among those, the **minimum pairwise ray angle** must be ≥ `min_pairwise_ray_angle_deg` (suggest 10–20°)
* This must hold for at least `min_fraction_points` of (t,j) points (suggest ≥0.95 for L00)

If the requirement fails, the generator must either:

* raise an error, or
* resample/reposition cameras (only if explicitly enabled)

For initial datasets, keep distortion off and keep cameras in a stable, non-degenerate configuration.

---

### 3.5 Dataset Artifact Format

Write one self-contained dataset file per config (suggested: `.npz`). Required contents:

* Truth: `x_true`, `u_true`, `z_true`, `A_true` (TODO: clarify if A_true is acceleration from second-order system)
* Observations: `y_2d`
* Skeleton metadata: `parents`, `joint_names`, `lengths`
* Camera params: per-camera `K`, `R`, `t`, `image_size`
* Serialized config text + config hash

---

### 3.6 Canonical Datasets (L00–L03)

**Implementation note (Dec 18):** ✅ All 4 datasets created and generated

**Implemented structure:**
```
tests/pipeline/datasets/
  ├── config_template.json
  ├── v0.2.1_L00_minimal/
  │   ├── config.json
  │   ├── dataset.npz
  │   ├── metrics.json
  │   └── figures/ (5 PNGs)
  ├── v0.2.1_L01_noise/
  ├── v0.2.1_L02_outliers/
  └── v0.2.1_L03_missingness/
```

**Note:** Directory structure simplified per user feedback - config.json lives in dataset directory, not separate configs/ folder.

We need two axes of difficulty:

* **Data quality** (noise/outliers/missingness)
* **Model complexity** (skeleton size/hierarchy, number of states, motion richness)

For v0.2.1, prioritize **data quality first** because it is faster to validate and it directly stress-tests projection/triangulation/cleaning and the inlier/outlier logic. Motion/skeleton complexity can be expanded in v0.2.2.

Accordingly:

* Keep skeleton and motion generation *fixed and minimal* across L00–L03.
* Increase observation corruption across L00–L03.

**L00_minimal.json** (baseline reference dataset) ✅ Implemented
* `num_states = 1` (1×1 transition matrix)
* L00_SKELETON: 4 joints (root → proximal → {distal_left, distal_right})
* Low noise (2.0 px = 0.16% of image width), no outliers, no missingness
* T=1800 frames (30 sec @ 60fps)
* Quality: bone_length_dev=0.000000, direction_norm=1.000000

**L01_noise.json** ✅ Implemented
* Same as L00 but noise_px=10.0 (5× increase, 0.78% of image width)
* Quality: bone_length_dev=0.000000, jerk_95th=16432 (higher than L00 due to noise)

**L02_outliers.json** ✅ Implemented
* Same as L00 but outliers enabled: 10% fraction at 50px SD (wide Gaussian, separable)
* Missingness off

**L03_missingness.json** ✅ Implemented
* Same as L00 but missingness enabled: 20% Bernoulli probability
* Outliers off
* Subject to identifiability constraints (min cameras per point)

Each dataset has dataset.npz, metrics.json, and 5 figures (motion_3d, poses_3d, reprojection_2d, missingness, states).

#### 3.6.1 Skeleton and motion minimality (must be specified)

**Implementation (Dec 18):** ✅ L00_SKELETON created per user guidance

L00–L03 explicitly specify:

* 4 joints: root (parent=-1), proximal (parent=0), distal_left (parent=1), distal_right (parent=1)
* parents array: [-1, 0, 1, 1]
* bone_lengths: [0, 10, 8, 8] (root=0, proximal=10, distals=8 each)
* 3 bones for direction vectors (root→proximal, proximal→distal_left, proximal→distal_right)

**Rationale (user feedback):** Previous DEMO_V0_1_SKELETON (6 joints, chain structure) too complex for initial testing. Simpler Y-structure enables clearer validation.

Guidance for v0.2.1:

* Start with a small but non-trivial tree (e.g., a root with 2–3 chains) rather than a pure chain.
* Avoid extreme branching or very long chains in v0.2.1.

**Implementation note:** Use existing `DEMO_V0_1_SKELETON` from `gimbal/skeleton_config.py` (7 joints, simple tree) for consistency with existing tests. Config files must still serialize all skeleton parameters explicitly so datasets are self-contained.

---

---

### 3.7 Standard Figure Set (Must Save)

For each dataset, save a consistent set of plots under `figures/`:

1. **3D skeleton motion preview**: trajectories of selected joints (root + 2–4 extremities)
2. **3D pose snapshots**: a small grid of frames showing skeleton pose in 3D
3. **2D reprojection montage**: one camera, a few frames with keypoints overlaid
4. **Missingness/outlier summary**: fraction missing/outlier per camera/joint
5. **State timeline**: true state sequence over time (and transition matrix image)

Purpose: rapid human inspection and quick detection of pathological motion/camera setups.

---

### 3.8 Machine-Verifiable Metrics + Generator Tests

In addition to figures, each generated dataset must save `metrics.json` containing:

* **Bone-length consistency:** max/mean deviation of implied bone lengths from config lengths
* **Direction normalization health:** distribution of raw direction norms before normalization; count of near-zero norms
* **Smoothness metrics:** distributions (mean/95th pct) of per-joint:

  * speed
  * acceleration
  * jerk (finite differences)
* **State sanity:** dwell times per state; transition counts; check `num_states=1` behaves as expected for L00
* **2D sanity:** fraction of NaNs (missingness), fraction of outliers injected, pixel coordinate bounds violations

A pytest integration test must run the generator for L00–L03 and assert conservative conditions, e.g.:

* Bone length deviation near zero (numerical tolerance)
* No NaNs in truth arrays
* 2D coordinates mostly within image bounds for inliers
* Missingness/outlier fractions within a small tolerance of config targets
* Acceleration/jerk percentiles not extreme relative to dt and pose scale (agent to calibrate using L00 then hold constant)

**Note:** thresholds should be defined in *relative* terms where possible (e.g., jerk relative to typical speed), so they generalize across skeleton scales.

### 3.9 Deliverables

* Generator functions (pure functions; no hard-wired paths)
* A generator runner that:

  * loads config
  * writes dataset + figures + `metrics.json`
  * writes a short `dataset_report.md`
* Four configs L00–L03
* Pytest `test_v0_2_1_synth_generator.py` validating L00–L03
