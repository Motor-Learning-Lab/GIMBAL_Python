# v0.2.1 Divergence Debugging Plan (PyMC)

Goal: generate a small set of reproducible diagnostics that localize which model component(s) trigger divergences, and whether the cause is (a) bad geometry/parameterization, (b) invalid data/priors, or (c) a logp/gradient implementation bug.

This plan assumes PyMC is the active backend.

---

## 1) Where outputs must go (mandatory)

Use the existing diagnostic directory:

* `tests/diagnostics/v0_2_1_divergence/`

Ensure these subdirectories exist:

* `tests/diagnostics/v0_2_1_divergence/results/`
* `tests/diagnostics/v0_2_1_divergence/plots/`
* `tests/diagnostics/v0_2_1_divergence/debug_outputs/`
* `tests/diagnostics/v0_2_1_divergence/archive/`

Naming patterns (must match `IMPORTANT_FILE_NAMING_CONVENTIONS.md`):

* test script: `test_group_N_<desc>.py`
* results JSON: `results/results_group_N_<desc>.json`
* report MD: `report_group_N_<desc>.md`
* plots dir: `plots/group_N_<desc>/`
* summary report: `report_summary.md`

Run ID policy:

* Each script accepts `--run_id` (default ISO timestamp) and writes:

  * `results_group_N_<desc>__<run_id>.json` plus a convenience copy `results_group_N_<desc>.json` (latest).
  * same for reports.

Do not mix diagnostic artifacts with pipeline fit outputs. The pipeline toolchain output dir remains:

* `tests/pipeline/fits/v0.2.1_L00_minimal/`

---

## 2) How to find the mixture observation model and log_obs_t

PowerShell from repo root:

If ripgrep is installed:

```powershell
rg -n "log_obs_t" -S
rg -n "use_mixture|logodds_inlier|inlier_prob|logaddexp" -S
rg -n "pm\.Potential\(\"y_obs\"" -S
```

Without ripgrep:

```powershell
Get-ChildItem -Recurse -Filter *.py | Select-String -Pattern "log_obs_t" | Select-Object -First 50
Get-ChildItem -Recurse -Filter *.py | Select-String -Pattern "use_mixture|logodds_inlier|inlier_prob|logaddexp" | Select-Object -First 50
Get-ChildItem -Recurse -Filter *.py | Select-String -Pattern "pm\.Potential\(\"y_obs\"" | Select-Object -First 50
```

Expected location in current code: `gimbal/pymc_model.py` in the observation likelihood section, where `use_mixture` branches, defines `pm.Deterministic("log_obs_t", ...)`, and adds `pm.Potential("y_obs", log_obs_t.sum())`.

---

## 3) Step 1B clarity: logp and gradients

This must be explicit in code and reports:

* PyMC builds the joint log-probability as a PyTensor computation graph.
* PyTensor executes the compiled logp and provides automatic differentiation.
* NUTS uses the PyTensor gradient of the joint logp.

Canonical compilation pattern (use a tiny compatibility shim if needed for your PyMC version):

```python
logp_fn = model.compile_logp()     # PyTensor-compiled

dlogp_fn = model.compile_dlogp()   # PyTensor autodiff, compiled

pt = model.initial_point()
logp0 = logp_fn(pt)
grad0 = dlogp_fn(pt)
```

If there are custom PyTensor Ops (e.g., in HMM code), gradient connectivity/validity becomes a prime suspect.

---

## 4) Shared utilities to implement once

Create `tests/diagnostics/v0_2_1_divergence/_diag_utils.py` with:

* `make_paths(group_desc, run_id)` returning run-specific + latest paths for results and report, plus plot and debug directories.
* `collect_environment()` collecting versions (python, pymc, pytensor, arviz, numpy, scipy) and platform.
* `write_json(path, payload)` stable JSON with UTF-8.
* `save_text(path, text)`.
* `safe_point_logps(model, point)` returning `model.point_logps(point)`, total logp, and NaN/inf flags.
* `compile_logp_and_grad(model)` returning compiled callables.

Every test script must:

* write results JSON even if later steps fail
* write a report MD pointing to the JSON and plots

---

## 5) Test groups (implement exactly; minimal set)

All groups should accept: `--dataset_name` (default `v0.2.1_L00_minimal`), `--max_T`, `--max_K`, `--seed`, `--run_id`, `--overwrite`.

### Group 1: build-only sanity

* Script: `test_group_1_build_only_sanity.py`
* Build the model with the failing configuration.
* Evaluate at `model.initial_point()`:

  * total logp (compiled)
  * `model.point_logps(point)` (sorted worst contributors)
* Save in JSON and include a short report table of the worst 10 terms.

Decision rule:

* Any `-inf` or NaN term is an immediate bug/domain issue.

### Group 2: gradient sanity

* Script: `test_group_2_gradient_sanity.py`
* Compile logp and dlogp; evaluate at initial point.
* Report:

  * grad L2 and Linf norms
  * NaN/inf counts
  * top 20 absolute gradient components (name + index for vectors)
* Add a tiny perturbation check: 3 perturbed points; ensure logp and gradients remain finite.

Decision rule:

* NaN/inf gradients indicate an implementation/domain problem (often mixture masking or custom Ops).

### Group 3: baseline sampling, smallest model

* Script: `test_group_3_sampling_baseline_minimal.py`
* Config: `use_mixture=False`, `use_directional_hmm=False`.
* Truncate aggressively (default `max_T=80`, `max_K=8`).
* Sample: `chains=2`, `tune=300`, `draws=100`.
* Save sampling summary: divergences, mean step size, max treedepth fraction, min ESS, max R-hat (on a small selected set).

Decision rule:

* If this diverges, the core kinematics/likelihood parameterization is already ill-conditioned.

### Group 4: mixture only

* Script: `test_group_4_sampling_mixture_only.py`
* Config: `use_mixture=True`, `use_directional_hmm=False`.
* Same truncation and sampling as Group 3.
* Additional checks written to JSON:

  * `uniform_logp` value (finite)
  * histogram summary (min/median/max) of normal logp per observed point
  * initialization and posterior range of inlier probability / logodds
  * verify invalid observations contribute exactly 0 to the potential (not `-inf`)

Decision rule:

* If Group 3 is stable but Group 4 diverges, focus on mixture math, masking, and probability parameterization.

### Group 5: directional HMM only

* Script: `test_group_5_sampling_hmm_only.py`
* Config: `use_mixture=False`, `use_directional_hmm=True`.
* Same truncation and sampling.
* Add a micro gradient check for the HMM term:

  * run a tiny synthetic (T=5, K=2) evaluation of the HMM logp and gradient for key parameters.

Decision rule:

* If Group 3 is stable but Group 5 diverges, focus on HMM Op gradients and transitions.

### Group 6: full model, truncated

* Script: `test_group_6_sampling_full_truncated.py`
* Config: mixture + HMM enabled, but still truncated.
* Same sampling.

Decision rule:

* If truncated full model works but full-size fails, the dominant issue is scaling/identifiability, not a hard bug.

### Group 7: likelihood-only (freeze latent kinematics)

* Script: `test_group_7_likelihood_only_freeze_latents.py`
* Fix `x` and `U` (and optionally bone lengths) to deterministic values (init or ground truth).
* Sample only observation/noise parameters (and mixture parameters if enabled).

Decision rule:

* If this diverges, the observation model is the culprit.
* If this is stable, divergences come from latent geometry/parameterization.

---

## 6) Required plots (write to plots subdirectories)

Each sampling group attempts:

* `trace_selected.png` (only a handful of parameters)
* `energy.png` and E-BFMI (if available)
* `step_size_treedepth.png`
* `divergence_pairs.png` (or minimal pair plot for selected vars)

Also write `artifacts_manifest.json` inside each plot directory listing produced files.

---

## 7) How to run (PowerShell)

From repo root:

```powershell
pixi run python tests/diagnostics/v0_2_1_divergence/test_group_1_build_only_sanity.py --run_id 2025-12-19T21-00-00
pixi run python tests/diagnostics/v0_2_1_divergence/test_group_3_sampling_baseline_minimal.py --max_T 80 --max_K 8 --seed 123
```

---

## 8) Report format (required sections)

Each `report_group_N_<desc>.md` must include:

1. Header: group, run_id, timestamp, key config toggles
2. Model checks: total logp at init; worst 10 point-logp contributors
3. Gradient checks: norms, NaN/inf counts, top contributors
4. Sampling summary (if sampled): divergences, step size, treedepth, min ESS, max R-hat (selected)
5. Artifacts: relative links to JSON and plots
6. Interpretation: one paragraph
7. Next action: one paragraph

`report_summary.md` should synthesize: which group is the first to fail, and what that implies.

---

## 9) Legacy artifacts note

The repo already contains older artifacts named `results_tg*.json` and `report_tg*.md`. Do not delete them.

If migrating fully to `group_N` naming, move legacy `tg*` artifacts into `tests/diagnostics/v0_2_1_divergence/archive/` and keep `report_summary.md` pointing at the current set.
