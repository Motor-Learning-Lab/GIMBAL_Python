# v0.2.1 Step 3: Clarifications and Questions

This document tracks questions, decisions, and implementation progress for Step 3 (synthetic data generation).

---

## [CURRENT] Questions and Clarifications - December 18, 2025 (Post-Implementation)

**Date:** December 18, 2025 3:00 PM  
**Status:** Awaiting User Review

### Open Questions for User

**Q1: Parameter Validation**
Do the L00-L03 parameters meet your "measurable criteria" intent?
- L00: noise_px=2.0 (0.16% of image width)
- L01: noise_px=10.0 (0.78% of image width)  
- L02: outlier_std_px=50.0, fraction=0.1
- L03: missingness_p=0.2

**Q2: Testing Strategy**
You said "don't want pytest-based failure mode" and "just document quality." What should replace current pytest threshold tests?
- Option A: Quality report scripts that print metrics (no pass/fail)
- Option B: Jupyter notebook for interactive exploration
- Option C: Keep pytest but only for obvious failures (NaN checks, shape validation)
- Option D: Something else?

**Q3: Feature Scope**
Should these be implemented now or deferred?
- Camera placement helpers: (position, target, FOV) → (K, R, t) conversion
- Identifiability checking: 3-tier system (check/adjust/auto_place)
- Move metrics/viz to gimbal/: You said they're "library code"

**Recommendation:** Defer camera helpers + identifiability to Step 4, move metrics/viz to gimbal/ now (quick)

**Q4: Figure Review**
Please review generated figures when convenient:
- Location: `tests/pipeline/datasets/v0.2.1_L00_minimal/figures/` (and L01-L03)
- 5 figures per dataset: motion_3d, poses_3d, reprojection_2d, missingness, states
- Confirm: motion looks reasonable, 2D projections valid, no obvious issues

---

## [RESOLVED] Initial Questions - December 17-18, 2025

**Date:** December 17, 2025 (questions) → December 18, 2025 (user responses)  
**Status:** All Decisions Received and Implemented

### User Decisions Summary

**Key architectural decisions from Opher:**

1. **Code Philosophy:**
   - Prioritize simplicity over backward compatibility
   - No need to keep legacy code versions (use git history if needed)
   - Clean, curated, comprehensible codebase over rapid progress
   - Update or delete affected tests as needed

2. **SyntheticDataConfig:**
   - Extend existing class rather than creating new one
   - Add from_json() constructor method
   - Validate in constructors
   - Maintain as Python wrapper around JSON config

3. **Skeleton for L00-L03:**
   - Current 7-joint skeleton too complex for initial testing
   - Create simpler: root → proximal joint → 2 distal joints
   - Total: 3 joints, 3 bone segments

4. **Dataset Parameters:**
   - T = 30 seconds per HMM state on average (T = 30 * S * fps)
   - At 60fps: T = 1800 frames per state
   - Parameter selection driven by measurable criteria:
     - Noise: Relative to image scale (fraction of image width)
     - Outliers: Explicitly defined distribution, separable in residuals
     - Missingness: Subject to identifiability constraints

5. **Camera Identifiability:**
   - Implement three-tier approach:
     - `check_identifiability()`: Validation function
     - `iteratively_adjust_cameras()`: Optimization loop with convergence checks
     - `auto_place_cameras()`: Start with intelligent initial placement
   - Sample-based checking for large datasets (don't check all frames)

6. **Testing Approach:**
   - Remove pytest-based pass/fail thresholds for now
   - Focus on documenting quality metrics
   - Flag obvious failures but leave interpretation to user
   - Defer comprehensive pytest strategy to later phase

7. **Directory Structure:**
   - Use `tests/pipeline/datasets/v0.2.1_L00_minimal/` (dataset name in directory)
   - No separate `configs/` directory (config.json in dataset directory)
   - Template: `tests/pipeline/datasets/config_template.json`

8. **Library Organization:**
   - Metrics and visualization are library code, not test utilities
   - Move to `gimbal/` (e.g., `gimbal/pipeline_utils.py`)

### Implementation Progress

**Completed (December 18, 2025 - Afternoon):**

1. ✅ **L00_SKELETON created** (4 joints: root, proximal, distal_left, distal_right)
   - Exported from gimbal.skeleton_config
   - Validated on import

2. ✅ **SyntheticDataConfig refactored** (NamedTuple → dataclass)
   - Added rich fields: per_state_params, root_params, camera_specs, etc.
   - Added `from_json()` class method
   - Added `uses_second_order_dynamics()` helper
   - Backward compatible: all existing tests continue working

3. ✅ **New directory structure implemented**
   - Pattern: `tests/pipeline/datasets/v0.2.1_L00_minimal/`
   - Config.json in dataset directory (not separate configs/)
   - Template at `tests/pipeline/datasets/config_template.json`

4. ✅ **L00-L03 configs created** with Opher's requirements:
   - T = 1800 frames (30 sec at 60fps for S=1)
   - Skeleton: L00_SKELETON (4 joints)
   - K matrices: pixel-based (fx=fy=640, cx=640, cy=360)
   - image_size: [1280, 720]
   - Noise: L00=2px, L01=10px (measurable criteria)
   - Outliers: L02=10% at 50px SD (separable distribution)
   - Missingness: L03=20% Bernoulli (subject to identifiability)

5. ✅ **All 4 datasets generated successfully**:
   - L00_minimal: Perfect baseline (bone_dev=0.000000, no bounds violations)
   - L01_noise: Higher noise (10px vs 2px)
   - L02_outliers: 10% outliers enabled
   - L03_missingness: 20% missingness enabled
   - All have perfect bone length consistency and direction normalization
   - All have ~1800 frames, 4 joints, 3 cameras

**Next Steps:**
- Clean up old directory structure (tests/pipeline/datasets/v0.2.1/ subdirectory)
- Validate metrics show expected differences (L01 noise, L02 outlier detection, L03 missingness fraction in observations)
- Move metrics/visualization to gimbal/ per Opher's feedback
- Update pytest suite to work with new directory structure
- Remove threshold-based failures, add quality reporting
- Update step3 plan document with final status

### Implementation Plan

**Current focus:** Refactoring existing implementation to align with decisions

1. ✅ Create simple 3-joint skeleton (L00_SKELETON)
2. ✅ Extend SyntheticDataConfig with second-order dynamics fields
3. ✅ Motion generator strategy (keep separate paths)
4. ✅ Update L00-L03 configs (new skeleton, longer T, updated params, new directory structure)
5. ⏳ Implement camera placement helpers
6. ⏳ Implement identifiability checking (3-tier approach)
7. ⏳ Move pipeline utilities to gimbal/
8. ⏳ Remove pytest thresholds, add quality reporting
9. ⏳ Update step3 plan document

### Open Questions

**Q1: SyntheticDataConfig Extension Strategy [RESOLVED]**

**Decision**: Hybrid dataclass approach (combines options B+C)

**Implementation**:
- Converted NamedTuple → dataclass
- Kept all legacy fields with defaults (backward compatible)
- Added optional rich fields: `per_state_params`, `root_params`, `camera_specs`, etc.
- Added `from_json()` class method for loading complex configs
- Added `uses_second_order_dynamics()` helper to distinguish modes

**Result**:
- ✓ Backward compatible: `SyntheticDataConfig()` still works
- ✓ Rich interface: `SyntheticDataConfig.from_json("config.json")` works
- ✓ All existing tests should continue passing
- ✓ No breakage: tested with simple creation and custom parameters

**Next**: Update motion generator to support both modes

**Q2: Motion Generator Integration Strategy [RESOLVED]**

**Decision**: Keep separate paths for now (pragmatic Option B)

**Rationale**:
- `generate_demo_sequence()` works, is used in many tests/examples - leave unchanged
- JSON-based system (`generate_from_config()` in tests/pipeline/) is Step 3 specific
- Opher emphasized simplicity - avoid complex dispatching during active development
- Can promote to `gimbal/` later if general-purpose use case emerges

**Implementation**:
- ✓ `generate_demo_sequence()` unchanged - all existing tests continue working
- ✓ `generate_from_config()` stays in tests/pipeline/utils/ - JSON-specific workflow
- ✓ Clear separation: simple API vs rich config-driven API
- Focus now: Fix L00-L03 configs per Opher's requirements (skeleton, directory structure, parameters)

**Q3: Parameter Selection Strategy for L00-L03 [IN PROGRESS]**

Based on Opher's guidance, parameters should be **measurable-criteria driven**, not fixed constants:

**Noise levels** (L00 vs L01):
- Choose relative to image scale: noise_px as fraction of image width
- L00 (low noise): 2px ≈ 0.0016 × 1280px ≈ 0.16% of image width
- L01 (high noise): 10px ≈ 0.0078 × 1280px ≈ 0.78% of image width
- Validate via reprojection-error statistics

**Outlier distribution** (L02):
- Use explicit distribution (wide Gaussian at outlier_std_px=50px)
- Choose magnitude so outliers clearly separable in residual distribution
- outlier_fraction=0.1 (10%) per Opher's original suggestion
- Validate: residual histogram should show bimodal distribution

**Missingness constraints** (L03):
- Set subject to identifiability: min_cameras_per_point, min_pairwise_ray_angle
- Start with miss ing_bernoulli_p=0.2, adjust downward until constraints satisfied
- Validate: check that target fraction of points meet identifiability criteria
- May need camera-aware missingness (not just Bernoulli)

**Sequence length**:
- T = 30 seconds per HMM state (on average)
- At 60 fps: T = 30 * S * 60 = 1800 * S
- L00-L02 (S=1): T=1800 frames
- L03 could use S=2 or 3 to test state transitions with missingness

**Implementation status**:
- ✓ L00 config created with T=1800, noise_px=2.0
- ⏳ Need to generate and validate via metrics/figures
- ⏳ L01-L03 configs to be created after L00 baseline established

*(Additional questions below)*

---

## [RESOLVED] Section: Initial Questions - December 17, 2025

**Status:** User responses received December 18, 2025
**Preserved for reference**

---

## Section 1: Changes Already Made to step3 Document

### 1.1 Fixed Duplicate Status Line
- **Issue:** Document had "**Status:** Draft for Copilot implementation" listed twice at the top
- **Action:** Removed duplicate (will be fixed in document update)

> Opher: ok

### 1.2 Corrected Title Mismatch
- **Issue:** Title says "v0.2.1 Step‑1 Plan" but document is about Step 3 (synthetic data)
- **Action:** Will change title to "v0.2.1 Step 3 Plan: Synthetic Data Generation"

> Opher: ok


### 1.3 Updated Scope Description
- **Issue:** Scope mentions 6 topics (cleanup, alignment, synthetic data, fitting, runner, camera priors) but document only covers synthetic data
- **Proposed Action:** Remove items 1-2 and 6 from scope, or split into separate documents
- **Recommendation:** This should be Step 3 ONLY (synthetic data), with other steps in separate documents

> Opher: We should probably have all three of the existing documents identify both all the planned steps and the associated documents so they will be easy to find.

---

## Section 2: Current Codebase vs Plan Requirements

### 2.1 Existing Motion Generator (Simple vs Second-Order Attractor)

**Current state in `gimbal/synthetic_data.py`:**
- Uses simple directional noise model: canonical direction + Gaussian noise in 3D space
- Normalized per-joint after adding noise
- State-dependent but NOT continuous dynamics (no velocity, no damping)
- Root is random walk (simple Brownian motion)

**Plan requirement (Section 3.3):**
- Second-order attractor system with position `q_t`, velocity `v_t`, acceleration `a_t`
- State-dependent parameters: `(mu_k, omega_k, zeta_k, sigma_process_k)`
- Explicit damping ratio `zeta` and natural frequency `omega`
- Requires tuning `sigma_process_k` to achieve target `sigma_pose_k`

**Questions:**
1. **Priority:** Should we REPLACE the existing simple generator or ADD the second-order generator as an option?
   - Option A: Replace entirely (breaking change, cleaner API)
   - Option B: Add new function `generate_skeletal_motion_continuous()` alongside existing
   - Recommendation: **Option A** - the plan is explicit that continuous motion is required

> Opher: As a general rule we should opt for code simplicity over back compatibility in all cases. The only times we want to keep multiple paths are when they are meaningful looking forward. In this case as well.
>
> We should check whether changes affect existing tests and if so we should reach a decision: if the test is still important, we should update it; it is not necessary, we can delete it.
>
> We should prioritize a clean, well curated, comprehensible code base over both back compatibiltiy and rapid progress in almost all situations.

2. **Backward compatibility:** Existing demos/tests use current generator. Should we:
   - Update all existing code to use new generator?
   - Keep old generator under legacy name for reference?
   - Recommendation: Update all code, keep old version as `generate_skeletal_motion_legacy()` temporarily
  
> Opher: I agree, but we don't need to keep legacy versions. If we think they might be useful, we can keep a document of versions and changes which identifies when functions that might be important have been added and removed. However, even this seems unnecessary to me. If we've gotten rid of something we decide we need, we can dig it out o fthe GitHub history.

3. **Initialization:** Plan says initialize at `mu_{z0}` with `v_0 = 0`. Current code initializes root at `[0,0,100]`. Should we:
   - Still use explicit root position for initialization?
   - Make initialization configurable in JSON?
   - Recommendation: Make root initialization part of config `motion.root_params.init_pos`

> Opher: I'm fine with explicit configuration in the JSON with default to mean HMM state 0 configuration and default to initial HMM state 0. If we do have explicit configuration, the HMM state should also be specified.

### 2.2 Configuration Format (Current vs JSON)

**Current state:**
- Uses Python `NamedTuple` (`SyntheticDataConfig`) 
- In-memory only, not serializable to disk
- Limited parameters (T, C, S, kappa, obs_noise_std, occlusion_rate, root_noise_std, random_seed)

**Plan requirement:**
- JSON configuration files
- Much richer schema with nested structure (skeleton, states, motion, cameras, observation)
- Must be serializable into .npz metadata

**Questions:**
1. **Migration path:** Should we keep `SyntheticDataConfig` as a Python wrapper around JSON config?
   - Pro: Backward compatibility, type hints, autocompletion
   - Con: Two representations to maintain
   - Recommendation: Keep `SyntheticDataConfig` but deprecate direct use, load from JSON
  
> Opher: Isn't the correct approach to load from JSON into SyntheticDataConfig? That is, I would assume that one of the constructors of the SyntheticDataConfig class in synthetic_data.py would be one that loads the appropriate data from a JSON file.

2. **Config validation:** Who validates config correctness (schema compliance)?
   - Option A: Python code with explicit checks
   - Option B: JSON Schema validation (requires jsonschema package)
   - Option C: Pydantic models (requires pydantic package)
   - Recommendation: **Option A** for now (avoid new dependencies), can upgrade later

> Opher: Agree. This should probably also be validated in constructors, no? 

### 2.3 Camera Model (Current Implementation)

**Current state in `gimbal/synthetic_data.py`:**
- `generate_camera_matrices()` uses `camera_utils.build_projection_matrix()`
- Fixed positions: front (+X), side (+Y), overhead (+Z)
- Fixed focal length (10.0)
- Properly implements look-at with up-vector
- Returns 3x4 projection matrices P = K[R|t]

**Plan requirement (Section 3.4):**
- Cameras defined in config with explicit K, R, t, image_size
- Guidance on FOV → focal length conversion
- Helper functions for camera placement recommended

**Questions:**
1. **Config vs generator:** Should camera generation be:
   - Option A: Always explicit in config (user specifies K, R, t directly)
   - Option B: Config can specify either explicit params OR placement parameters (position, target, FOV)
   - Recommendation: **Option B** - more user-friendly, with helper to convert placement → K/R/t

> Opher: Strongly agree.

2. **Image size:** Current code doesn't track image size. Plan requires it for:
   - Bounds checking (coordinates within frame)
   - FOV → focal length calculations
   - Recommendation: Add to config, default to 1280x720 for backward compatibility

> Opher: Strongly agree. 

### 2.4 Camera Quality / Identifiability Check (New Requirement)

**Current state:** No identifiability checking

**Plan requirement (Section 3.4):**
- Compute pairwise ray angles for each (t, j) point
- Require `min_cameras_per_point` with minimum `min_pairwise_ray_angle_deg`
- Must hold for `min_fraction_points` of observations

**Questions:**
1. **When to check:** Should this be:
   - Option A: Run after generation, fail if check fails
   - Option B: Run during camera placement, iteratively adjust cameras
   - Option C: Run as separate validation function, report but don't fail
   - Recommendation: **Option A** for now (fail fast), can add auto-adjustment later

> Opher: I think the most appropriate is to write a function that checks identifiability (as per specs) and another function that iteratively adjusts camera positions (possibly calling a function that distributes the cameras intelligently to start with) and relies on the check function but fails if iteration seems to get stuck or reach a failing maximum and then a final function that starts with min_cameras_per_point cameras, gives them an intelligent initial guess position, calls the iterative adjustment function and then increase then number of cameras if it fails.

2. **Performance:** For large datasets (T=1000, K=20, C=5), this is ~100K point checks
   - Should we subsample for speed?
   - Recommendation: Check every frame but add `--skip-quality-check` flag for speed

> Opher: We should not check the entire path of all the points but sample a reasonable number of time points to check. 

### 2.5 Dataset Artifact Format (Section 3.5)

**Current state:** `SyntheticMotionData` is NamedTuple, not saved to disk

**Plan requirement:**
- Save to .npz file
- Include: x_true, u_true, z_true, A_true, y_2d, skeleton metadata, camera params, config text + hash

**Questions:**
1. **What is `A_true`?** Plan mentions it but doesn't define in context
   - Is this the acceleration array from the second-order system?
   - Or is it something else (attention? alignment?)?
   - **NEEDS CLARIFICATION**

> Opher: I don't know. I think you should pick what seems like a reasonable set of parameters to save and if we're missing something we'll figure it out.

2. **Config hash:** Should we use:
   - Option A: Hash of JSON string (sensitive to formatting)
   - Option B: Hash of sorted canonical JSON (stable)
   - Option C: SHA256 of file contents
   - Recommendation: **Option B** - stable and deterministic

> Opher: Ok.

3. **Metadata storage:** Should we store config as:
   - npz['config_json'] = config_str (as string)
   - npz['config'] = json.loads(config_str) (as dict, but npz might not preserve structure)
   - Recommendation: Store as string, parse on load

Opher: Yes. That makes sense.

### 2.6 Canonical Datasets L00-L03 (Section 3.6)

**Current state:** No canonical datasets exist

**Plan requirement:**
- L00: minimal (1 state, low noise, no outliers, no missingness)
- L01: L00 + increased noise
- L02: L00 + outliers
- L03: L00 + missingness

**Questions:**
1. **Skeleton for L00-L03:** Plan says "keep skeleton fixed and minimal"
   - Should we use existing `DEMO_V0_1_SKELETON`?
   - Or create new minimal skeleton?
   - Current demo skeleton: 7 joints, simple tree structure
   - Recommendation: Use `DEMO_V0_1_SKELETON` for consistency with existing tests

> Opher: I think the current skeleton is too complex for initial testing. Let's make something simpler: a root with a proximal bone connected at a joint to two distal bones. This is 3 joints (proximal bone to root + 2 distal bones to proximal) and 3 segments.

2. **Specific parameters:** Plan gives guidance but not exact values
   - L01 noise: How much to increase? (current default is 5.0 px)
   - L02 outliers: What fraction? What magnitude?
   - L03 missingness: What Bernoulli p?
   - Recommendation: Start conservative then tune based on metrics:
     - L00: noise_px=2.0 (tight)
     - L01: noise_px=10.0 (moderate)
     - L02: noise_px=2.0, outliers=10% at 50px SD
     - L03: noise_px=2.0, missingness=20%

> **Opher:** I agree that the plan needs to specify *how* to choose parameters, not just suggest example values. For v0.2.1, parameter selection should be driven by **measurable criteria** rather than fixed constants. Noise levels should be chosen relative to image scale (e.g., as a small fraction of image width) and validated by reprojection-error statistics. Outlier severity should be defined via an explicit outlier distribution (e.g., uniform over image or wide Gaussian), with parameters chosen so that outliers are clearly separable from inliers in the residual distribution. Missingness probabilities should be set *subject to* the camera-quality / identifiability checks (minimum cameras per point and minimum ray-angle), adjusting downward until those constraints are satisfied for the target fraction of points. In all cases, the generator metrics and standard figures (reprojection error, bounds violations, ray-angle distributions, smoothness measures) should be used to confirm that each dataset stresses the intended failure mode without becoming degenerate.


1. **Sequence length T:** Plan doesn't specify
   - Current default is T=60
   - For testing, shorter is faster but less interesting
   - Recommendation: T=100 for L00-L03 (1-2 second sequences at 60 fps)

> Opher: I would go for 30 seconds of data per HMM state (on average, that is 30 seconds times the number of HMM states total). That's much longer than we've been considering but it still at least two orders of magnitude less than real data processing. This matters for identifiability, and we will lead ourselves astray if we practice on data sets that are too small.

### 2.7 Metrics and Thresholds (Section 3.8)

**Plan requirement:**
- Bone length consistency (max/mean deviation)
- Direction normalization health
- Smoothness metrics (speed, acceleration, jerk distributions)
- State sanity (dwell times, transition counts)
- 2D sanity (missingness fraction, outlier fraction, bounds violations)

**Questions:**
1. **Jerk thresholds:** Plan says "calibrate using L00 then hold constant"
   - Should calibration be manual or automated?
   - Recommendation: Run L00 once, record 95th percentile values, hardcode as thresholds with 2× safety margin

2. **Relative vs absolute:** Plan prefers relative thresholds
   - How to make jerk threshold relative to skeleton scale?
   - Recommendation: Normalize by `bone_length_mean * fps^3` (jerk has units of length/time³)

3. **Test failures:** If L00-L03 fail thresholds:
   - Option A: Test fails immediately (strict)
   - Option B: Test warns but passes (informational)
   - Option C: Test fails but prints diagnostic suggestions
   - Recommendation: **Option C** - fail but give actionable feedback

> Opher: Regarding all of these, I want to back away from a pytest-based failure mode. That is, I don't want to hardcode success and failure into these tests. I also don't want to get into pytest mechanics at this stage. When we do start building pytests, we will need to thinka bout our testing approach quite seriously.
>
> For now, what we want to do is just document the quality of the fit and be aware of obvious failures. We will leave itto me to ask more subtle questions and figure out what to prioritize in terms of fixing issues of quality of fit.

---

## Section 3: Missing Specifications

### 3.1 Figure Specifications (Section 3.7)

**Plan lists 5 figure types but lacks details:**

1. **3D skeleton motion preview:** 
   - Which joints? (plan says "root + 2-4 extremities")
   - How to identify "extremities" programmatically?
   - Recommendation: Root + all leaf nodes (joints with no children)

2. **3D pose snapshots:**
   - How many frames in grid? (plan says "small grid")
   - Which frames? (evenly spaced? state transitions? first/last?)
   - Recommendation: 3×3 grid, evenly spaced through sequence

3. **2D reprojection montage:**
   - Which camera? (plan says "one camera")
   - How many frames? (plan says "a few frames")
   - Recommendation: Camera 0, same 9 frames as 3D grid

4. **Missingness/outlier summary:**
   - What format? Bar chart? Heatmap? Table?
   - Recommendation: Two heatmaps (cameras × joints) for missingness and outliers

5. **State timeline:**
   - How to visualize transition matrix? Heatmap with probabilities?
   - Recommendation: (1) Line plot of states over time, (2) Heatmap of transition matrix with values

> Opher: I think for all of these you should make your best guesses and then we can iterate finding good pictures once we have data to show. It makes more sense to worry about this kind of pipeline detail once a pipeline exists.

### 3.2 Dataset Report Format (Section 3.9)

**Plan mentions `dataset_report.md` but doesn't specify content**

**Proposed content:**
```markdown
# Dataset: {config_name}

## Configuration
- Timesteps: {T}
- States: {S}
- Cameras: {C}
- Joints: {K}
- Noise: {noise_px} px
- Outliers: {outlier_fraction}
- Missingness: {missingness_fraction}

## Generation Summary
- Seed: {seed}
- Generated: {timestamp}
- Config hash: {hash}

## Quality Metrics
[Paste metrics.json content formatted]

## Figures
- See figures/ directory for visualizations
```

**Question:** Is this sufficient or should we add more analysis?

> Opher: Again, I'm fine with this for now. We'll be able to update it once something is working.

### 3.3 Directory Structure

**Plan doesn't specify where datasets should be saved**

**Proposed structure:**
```
tests/pipeline/
├── configs/
│   └── v0.2.1/
│       ├── _template.json (with comments)
│       ├── L00_minimal.json
│       ├── L01_noise.json
│       ├── L02_outliers.json
│       └── L03_missingness.json
├── datasets/
│   └── v0.2.1/
│       ├── L00_minimal/
│       │   ├── dataset.npz
│       │   ├── metrics.json
│       │   ├── dataset_report.md
│       │   └── figures/
│       │       ├── motion_3d.png
│       │       ├── poses_3d.png
│       │       ├── reprojection_2d.png
│       │       ├── missingness.png
│       │       └── states.png
│       ├── L01_noise/...
│       ├── L02_outliers/...
│       └── L03_missingness/...
└── test_v0_2_1_synth_generator.py
```

**Questions:**
1. Should datasets be in `tests/pipeline/datasets/` or `results/pipeline/v0.2.1/`?
   - Recommendation: `tests/pipeline/datasets/` since they're reference data for testing

2. Should figures be in subdirectory or alongside dataset.npz?
   - Recommendation: Subdirectory keeps things organized

> Opher: Lets do `tests/pipeline/datasets/v0.2.1_L00_minimal/` with `config.json`, `dataset.npz`, `metrics.json`, and `dataset_report.md` in this directory and a subdirectory `figures/` as you've drawn. That is, we don't need a separate `configs` directory and we don't need a `v0.2.1` subdirectory of `datsets`. `config_template.json` can live in the `tests/pipeline/datasets` directory.

---

## Section 4: Implementation Dependencies

### 4.1 New Python Package Requirements

**For full implementation we may need:**
- `matplotlib` or `plotly` - for figure generation (already in pixi.toml?)
- `scipy` - for finite differences (jerk calculation)
- `json` - built-in
- `hashlib` - built-in
- `pathlib` - built-in

**Action:** Verify matplotlib in pixi.toml, add if missing

> Opher: ok.

### 4.2 Testing Infrastructure

**Need to create:**
- `tests/pipeline/` directory structure
- Runner script (e.g., `generate_dataset.py`)
- Integration test file
- Metrics computation module
- Visualization module

**Question:** Should metrics/visualization be in `gimbal/` (library code) or `tests/pipeline/utils/` (test utilities)?
- Recommendation: Start in `tests/pipeline/utils/`, move to library if reused elsewhere

> Opher: I'm thinking of everything about the pipeline as part of the library.

---

## Section 5: Recommendations for User Decision

### Priority 1 (Must Decide Before Implementation):

1. **Motion generator replacement:** Replace simple generator with second-order attractor? (Recommend: YES, with legacy backup)

2. **`A_true` definition:** What is this array in the dataset? (NEEDS CLARIFICATION)

3. **Config approach:** JSON-only or Python wrapper? (Recommend: JSON primary, Python wrapper for convenience)

### Priority 2 (Can Use Recommendations):

4. **L00-L03 parameters:** Use recommended values above or specify different? (Can tune after first run)

5. **Figure details:** Use recommendations in Section 3.1? (Can refine after seeing output)

6. **Directory structure:** Use proposed structure in Section 3.3? (Seems reasonable)

### Priority 3 (Implementation Details):

7. **Camera placement helpers:** Implement placement → K/R/t conversion? (Useful but not critical for MVP)

8. **Identifiability check:** Fail-fast vs warn vs auto-adjust? (Start with fail-fast)

9. **Metrics calibration:** Manual threshold setting vs automated? (Start manual)

---

## Section 6: Proposed Implementation Order

1. **Phase 1: Core Data Generation**
   - Update `synthetic_data.py` with second-order attractor
   - Implement JSON config loading
   - Add camera quality checking
   - Test with simple manual config

2. **Phase 2: Canonical Configs**
   - Create directory structure
   - Write L00-L03 JSON configs
   - Generate datasets (no visualization yet)
   - Verify .npz format

3. **Phase 3: Metrics & Validation**
   - Implement metrics.json computation
   - Run on L00, establish baseline thresholds
   - Write pytest integration test
   - Verify tests pass

4. **Phase 4: Visualization**
   - Implement 5 figure types
   - Generate figures for L00-L03
   - Create dataset_report.md generator
   - Manual visual inspection

5. **Phase 5: Runner & Polish**
   - Create unified runner script
   - Add CLI arguments for config selection
   - Document usage in README
   - Final end-to-end test

---

## Section 7: Timeline Estimate

**With user decisions resolved:**
- Phase 1: 3-4 hours (core functionality)
- Phase 2: 1-2 hours (config files, simple generation)
- Phase 3: 2-3 hours (metrics computation, testing)
- Phase 4: 2-3 hours (visualization, requires iteration)
- Phase 5: 1 hour (integration, documentation)

**Total: ~9-13 hours of focused implementation**

(Assumes user decisions come back quickly, no major design changes needed)

---

## Next Steps

**User should review this document and provide:**
1. Answer to `A_true` question (Priority 1)
2. Confirmation or modification of motion generator replacement approach
3. Confirmation of L00-L03 parameter recommendations
4. Any corrections to proposed directory structure
5. Green light to proceed with Phases 1-5

**Agent will then:**
1. Update v0.2.1_step3_synthetic_data_generation.md with clarifications
2. Begin Phase 1 implementation
3. Iterate through phases with periodic checkpoints
4. Request user visual inspection after Phase 4 completes
