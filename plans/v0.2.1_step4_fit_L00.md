# v0.2.1 Step‑1 Plan: Alignment, Pipeline, and Camera Priors

**Status:** Draft for Copilot implementation
**Scope:** This document defines the remaining work required to complete v0.2.1, focusing on:

1. Repository cleanup and naming/organization suggestions.
2. Alignment between specifications and the current codebase.
3. **Synthetic data generation** tools + four saved datasets (increasing complexity) + figures.
4. **Fitting toolchain validation** on the simplest synthetic dataset (stage-by-stage tests + plots).
5. **End-to-end runner** that can generate → fit → evaluate → compare to ground truth, with skip/override controls.
6. A preliminary (placeholder) design for camera parameter priors.

v0.2.1 should be considered complete once:

* Spec and code are reasonably aligned (or divergences are consciously documented).
* Synthetic data generation is config-driven, produces 4 canonical datasets, and saves clear figures.
* The fitting toolchain can be stepped through on the simplest dataset with tests/plots at each stage, ending in a validated comparison to ground truth.
* A runner exists that can execute the end-to-end pipeline with controlled skips/overrides, producing a complete artifact trail.
* A placeholder design for camera priors exists and is reflected in code structure (even if not fully implemented), with a clear note that end-to-end tests must be re-run after camera priors are added.

---

## 4. Fitting Toolchain Validation on Synthetic Data (v0.2.1)

4.0 Pre‑Flight Review and Gap Closure (Required First Phase)

Purpose: Treat this phase as a formal review checkpoint, not a redo of Step 3. The goal is to explicitly verify that the hardening work already done in Step 3 is sufficient for fitting, and to close any gaps that would otherwise cause ambiguity or accidental reuse of stale artifacts.

This phase is intentionally short and bounded. If an item is already satisfied, it should be marked as such and documented; no refactoring is required unless a concrete gap is found.

Step 4 must not proceed to fitting until 4.0.A–D are checked off (possibly as "already satisfied") and a short confirmation note is written.

4.0.A Verify Step 3 Documentation Is Authoritative

Check: Confirm that there is exactly one authoritative Step 3 plan/spec and one authoritative Step 3 completion/summary document.

Actions:

Identify all Step 3–related markdown documents in plans/.

Explicitly mark (in-place) which document is authoritative.

Mark others as archived, superseded, or exploratory only if this has not already been done.

If the current state already satisfies this, add a short note stating that no action was needed.

Deliverable:

A brief confirmation note (a few bullets) appended to the Step 3 completion summary.

4.0.B Verify Parameter Semantics Needed for Fitting

Check: Confirm that all parameters produced by Step 3 and consumed by Step 4 have unambiguous meaning.

Actions:

Review generator configs, metrics.json, and any intermediate artifacts that will be inputs to fitting.

Ensure it is clear which parameters are:

inputs to the model

tuning knobs

diagnostic outputs only

Only document gaps or ambiguities that matter for fitting; do not restate the entire generator design.

Deliverable:

A short “Parameter Semantics for Step 4” subsection (½ page max) added to the Step 3 completion summary, or a note stating that no gaps were found.

4.0.C Verify No Obsolete Code Will Be Used by Fitting

Check: Ensure that Step 4 will not accidentally import or rely on obsolete Step 3 helpers.

Actions:

Identify any generator-related code that is no longer used by the current Step 3 runner/tests.

Decide only whether each should be:

left in place but clearly unused, or

archived for later reference.

No deletion is required in v0.2.1 unless something is actively misleading.

Deliverable:

A short list of files reviewed and a one-line decision for each (keep / archive / no action).

4.0.D Verify Artifact Layout Is Sufficient for Fitting

Check: Confirm that Step 3 artifacts expose everything Step 4 needs.

Actions:

Verify that dataset files, figures, and metrics.json are discoverable and documented.

Confirm that it is obvious which artifacts are inputs to fitting and which are diagnostic only.

If tests/pipeline/datasets/README.md already satisfies this, record that explicitly.

Deliverable:

A short confirmation note (or small README edit) stating that artifact layout is sufficient for Step 4.

Only after completing 4.0.A–D as a review and gap-closure step should Step 4 proceed to fitting toolchain validation.



Goal: Ensure that the full set of tools required to fit the model exist and work, by stepping through them on L00_minimal with explicit tests and plots at each stage.

Key decision: L00 uses num_states = 1 (a 1-state HMM). This is both a valid scientific outcome (some datasets are best explained by one regime) and a practical way to de-risk the early pipeline.

Scope boundary: Step 4 is allowed to use ground truth (passed in) to isolate components, but must also include at least one end-to-end fit on L00 where GT is not substituted.


---

### 4.1 Definition of Done

Step 4 is complete when:

1. All necessary tool functions exist (or are clearly stubbed with TODOs and named owners).
2. The L00 dataset can be processed through the toolchain with:

   * checkpoints at each stage
   * plots verifying output plausibility
   * explicit tests (assertions) that catch common failures
3. A final comparison to ground truth is produced for:

   * 3D motion
   * the 1-state regime (trivial, but must not break anything)
   * key fitted parameters that are identifiable in L00

---

### 4.2 Toolchain Stages (Sequential; Each Has Tests + Plots)

(Duplicate legacy stage listing removed — see canonical Stage A–J specification above.)

Run these stages in order on L00.

**Stage A — Load + basic validation**

* Test: shapes, NaN policy, camera metadata presence
* Plot: quick 2D scatter for a single frame/camera

**Stage B — (Optional) 2D cleaning**

* If used: test that missingness/outliers are handled as expected
* Plot: before/after for one camera

**Stage C — Triangulation (2D→3D)**

* Test: fraction of valid reconstructed points above threshold
* Plot: reconstructed 3D trajectories for selected joints

**Stage D — 3D cleaning / smoothing (if applicable)**

* Test: smoother reduces high-frequency noise without large bias
* Plot: before/after trajectory for one joint

**Stage E — Direction statistics**

* Test: no NaN in statistics for valid joints; reasonable concentration values
* Plot: histogram of direction norms / angular deviations

**Stage F — Data-driven priors build**

* Test: priors are finite; scales follow `kappa_scale` semantics
* Plot: prior parameter summary table rendered to markdown

**Stage G — Build PyMC model (minimal)**

* Test: model builds; initial logp finite
* Plot: none required

**Stage H — Fit model (short run)**

* Test: sampling completes; divergences reported; basic ESS/R-hat summary
* Plot: trace plots for a small set of parameters

**Stage I — Posterior predictive / fitted motion diagnostics (non-GT)**

* Test: reprojection error distribution reasonable; residuals not pathological
* Plot: reprojection error over time; observed vs predicted 2D for one camera

**Stage J — Compare to ground truth (GT passed in explicitly)**

* Metrics: 3D RMSE per joint; direction angular error; state accuracy if used
* Plot: overlay true vs estimated 3D trajectories for selected joints

---

### 4.3 Minimal Switches and Overrides for Step 4 (Concrete)

This section was previously underspecified. The goal is to define the **minimum** controls needed to execute stages A–J on L00 without redesigning the model API.

#### Step 4.3.1 Inventory (mandatory, first)

Before adding new switches, document current options already supported by `gimbal/pymc_model.py` and any helper builders.

Deliverable: a table under `plans/` titled:

```markdown
## Inventory: Options and Switches Used by Pipeline
| Option | Location | Meaning | Default | Used in Stage(s) |
```

#### Step 4.3.2 Decide: disable estimation vs override (decision)

For Step 4 on L00, default to **disabling estimation** for cameras and lengths (treat them as fixed inputs).

Rationale:

* This is a toolchain validation step, not a full uncertainty characterization step.
* Fixing these reduces geometry/identifiability confounds and keeps early failures interpretable.

Overrides remain available for isolation experiments:

* Use `overrides.camera_params` or `overrides.lengths` only when you explicitly want to test that the override pathway works.

#### Step 4.3.3 Minimal working contract (use only what is needed)

For L00, the minimal required controls are:

* `num_states = 1`
* ability to run with projection on (cameras used) and with projection bypassed (direct 3D likelihood) for debugging
* ability to *fix* cameras and lengths (disable estimation by default; overrides optional)

Proposed minimal dict interfaces (only adopt if existing code cannot already express this cleanly):

* `model_options` keys:

  * `use_projection: bool` (default True)
  * `estimate_cameras: bool` (default False for L00)
  * `estimate_lengths: bool` (default False for L00)
  * `use_hmm: bool` (default True but trivial with K=1)
  * `num_states: int` (default 1)

* `overrides` keys (optional):

  * `camera_params` (if provided, used as fixed camera params)
  * `lengths` (if provided, used as fixed lengths)
  * `A` (optional; for K=1 this is [[1]])

Rules:

* Overrides always win.
* Ground truth, when used for isolation tests, must be passed into the stage functions explicitly.
* No stage may silently read GT from disk

### 4.4 Deliverables

* A single integration test (or diagnostic script) that runs L00 through stages A–J
* A run-scoped output directory containing:

  * intermediate artifacts per stage
  * plots
  * `fit_report.md` summarizing tests/metrics
